Title,Abstract
3D Printing of Human Anatomical Models for Preoperative Surgical Planning,"Rapid Prototyping (RP) is an emerging technology, especially in a Three-Dimensional Printing (3DP) application. 3DP is used in many fields such as aeronautical, automotive, architecture, medical, and many others. 3DP can be effectively used in anatomical education for medical students who are pursuing their undergraduate degrees.It can also be used for pre-operative surgical planning by experts before surgery. Some complex organs of the human body which cannot be seen visible even after dissection of the cadaver can be printed using a 3D printer which provides haptic studies on organs and bones to students. These 3D printed parts can be used in pre-operative planning such as analysis and diagnosis formulation of affected organs. Further, it can be used in explaining the operative procedures to patients which helps them to understand and co-operate with the medical procedures. Therefore, this project aims at 3DP of complex organs and bones for anatomical studies and pre-operative planning procedures. As a first step in the project, some of the human bones were printed and analyzed for its quality."
A Bayesian statistics based investigation of binder hardening’s influence on the effective strength of particulate reinforced metal matrix composites (PRMMC),"In order to understand how hardening of the binder phase in particulate reinforced metal matrix composites (PRMMC) influences the effective strength, we present in this work a numerical framework consisting of the direct method (DM) and statistical models. Using this approach we created a large number of statistically equivalent representative volume element (SERVE) models to represent an exemplary PRMMC material WC-20 Wt.% Co and predicted its effective strengths using DM. After the global strength was calculated from each SERVE sample all derived data are interpreted by Bayesian network and diagnostic testing. By doing so the relationship between material strength and few selected characteristics have been clarified. The study shows the formulated approach as a novel means for investigating how the overall mechanical properties of random heterogeneous materials react to a certain constituent. Meanwhile, the study also demonstrates how statistical models, in particular the Bayesian network, can be used as a powerful supplement to micromechanical models for result analysis and knowledge discovery."
A big - data classification tree for decision support system in the detection of dilated cardiomyopathy using heart rate variability,"Dilated cardiomyopathy (DCM) is a heart muscle disease characterized by left ventricular (LV) or biventricular dilatation and systolic dysfunction in the absence of either pressure or volume overload or coronary artery disease sufficient to explain the dysfunction. The use of heart rate variability (HRV) analysis as well as of some machine learning algorithms, proved to be a valuable support in the diagnosis of cardiovascular disease. However, till now, only single beats or electrocardiogram segments of subjects affected by DCM were identified using machine learning techniques applied to HRV parameters. In this study, we used linear and non-linear HRV parameters and some clinical parameters (age, sex and left ventricular ejection fraction) evaluated on a large cohort of 972 subjects to early identify subjects suffered from DCM and to find which features could be selected as predictors for a correct diagnosis. By using principal component analysis and stepwise regression, we reduced the original parameters used as inputs for a series of classification and regression trees (CART). The highest accuracy of 97% and Area Under the Curve (AUC) of 95% were achieved using the ratio between low frequency and high frequency (LF/HF), sex and left ventricular ejection fraction (LVEF) parameters as inputs of the classifier."
A boundary vorticity diagnosis of the flows in a model pump-turbine in turbine mode,"In the present study, the internal flow field in a model pump-turbine operating in turbine mode was numerically studied. By recognizing that appearance of boundary vorticity flux (BVF)’s peaks might act as the precursors of flow separation in turbomachines, flow diagnosis based on the boundary vorticity dynamics was then performed. It was shown that the effect of the BVF distribution at the condition in design operating zone is to weaken the axial vorticity component ωz, as well as to enhance the radial and circumferential vorticity components ωr and ωθ. Thus, the peaks of the axial BVF component σz led to the strong weakening of ωz, which is responsible for the flow separations. Geometric optimization for the runner was then achieved by modifying the inlet angle of the blade from 14° to 16°. The optimization successfully suppressed the occurrence of the flow separations and improved the moment on the runner at design operating condition by 4%, which was achieved by the redistribution of the axial BVF component generated by pressure σpz on the pressure side. The fact that only an accurate steady simulation is required for the analysis indicates the great potential application of this method in engineering practice."
A case-based ensemble learning system for explainable breast cancer recurrence prediction,"Significant progress has been achieved in recent years in the application of artificial intelligence (AI) for medical decision support. However, many AI-based systems often only provide a final prediction to the doctor without an explanation of its underlying decision-making process. In scenarios concerning deadly diseases, such as breast cancer, a doctor adopting an auxiliary prediction is taking big risks, as a bad decision can have very harmful consequences for the patient. We propose an auxiliary decision support system that combines ensemble learning with case-based reasoning to help doctors improve the accuracy of breast cancer recurrence prediction. The system provides a case-based interpretation of its prediction, which is easier for doctors to understand, helping them assess the reliability of the system’s prediction and make their decisions accordingly. Our application and evaluation in a case study focusing on breast cancer recurrence prediction shows that the proposed system not only provides reasonably accurate predictions but is also well-received by oncologists."
A CBR framework with gradient boosting based feature selection for lung cancer subtype classification,"Molecular subtype classification represents a challenging field in lung cancer diagnosis. Although different methods have been proposed for biomarker selection, efficient discrimination between adenocarcinoma and squamous cell carcinoma in clinical practice presents several difficulties, especially when the latter is poorly differentiated. This is an area of growing importance, since certain treatments and other medical decisions are based on molecular and histological features. An urgent need exists for a system and a set of biomarkers that provide an accurate diagnosis. In this paper, a novel Case Based Reasoning framework with gradient boosting based feature selection is proposed and applied to the task of squamous cell carcinoma and adenocarcinoma discrimination, aiming to provide accurate diagnosis with a reduced set of genes. The proposed method was trained and evaluated on two independent datasets to validate its generalization capability. Furthermore, it achieved accuracy rates greater than those of traditional microarray analysis techniques, incorporating the advantages inherent to the Case Based Reasoning methodology (e.g. learning over time, adaptability, interpretability of solutions, etc.)."
A cell interaction phenomenon in a multi-cell stack under one cell suffering fuel starvation,"Cell interaction is the main factor resulting in a shorter lifespan for multi-cell stacks than for a single fuel cell stack. To explain this mechanism, we propose a cell interaction phenomenon in which one cell experiences fuel starvation. A specific voltage distribution along the straight channel direction has been observed with an innovative multipoint monitoring method. This phenomenon also can be used for fuel starvation diagnosis. This study proposes an ingenious simplified two-chamber model to analyze the current and voltage redistribution mechanism under fuel starvation. The reliability of this model has been validated in this paper. The model shows that current convergence resulted by fuel starvation in one cell can lead to a concomitant local current convergence in nearby normal cells. Based on our calculation, >85% of reaction current concentrates in the anode inlet region of the fuel-starved cell, resulting in a 70% current convergence in the two adjacent normal cells. A serious corrosion of the fuel-starved cell is observed in the post-mortem study. The faulty cell presents a 5° contact angle decrease and a 28% ECSA loss. Scanning electron microscopy and Transmission electron microscopy results show that the decline of anode outlet regions’ cathode catalyst layers are more serious. Some optimal strategies have been proposed to solve this problem."
A clinical decision-making mechanism for context-aware and patient-specific remote monitoring systems using the correlations of multiple vital signs,"Background and objectives In home-based context-aware monitoring patient's real-time data of multiple vital signs (e.g. heart rate, blood pressure) are continuously generated from wearable sensors. The changes in such vital parameters are highly correlated. They are also patient-centric and can be either recurrent or can fluctuate. The objective of this study is to develop an intelligent method for personalized monitoring and clinical decision support through early estimation of patient-specific vital sign values, and prediction of anomalies using the interrelation among multiple vital signs. Methods In this paper, multi-label classification algorithms are applied in classifier design to forecast these values and related abnormalities. We proposed a completely new approach of patient-specific vital sign prediction system using their correlations. The developed technique can guide healthcare professionals to make accurate clinical decisions. Moreover, our model can support many patients with various clinical conditions concurrently by utilizing the power of cloud computing technology. The developed method also reduces the rate of false predictions in remote monitoring centres. Results In the experimental settings, the statistical features and correlations of six vital signs are formulated as multi-label classification problem. Eight multi-label classification algorithms along with three fundamental machine learning algorithms are used and tested on a public dataset of 85 patients. Different multi-label classification evaluation measures such as Hamming score, F1-micro average, and accuracy are used for interpreting the prediction performance of patient-specific situation classifications. We achieved 90–95% Hamming score values across 24 classifier combinations for 85 different patients used in our experiment. The results are compared with single-label classifiers and without considering the correlations among the vitals. The comparisons show that multi-label method is the best technique for this problem domain. Conclusions The evaluation results reveal that multi-label classification techniques using the correlations among multiple vitals are effective ways for early estimation of future values of those vitals. In context-aware remote monitoring this process can greatly help the doctors in quick diagnostic decision making."
"A collaborative clinical analysis service based on theory of evidence, fuzzy linguistic sets and prospect theory and its application to craniofacial disorders in infants","Nowadays, it is more and more important to diagnose several kinds of pathologies at their early stage, in order to take the necessary countermeasures before having permanent consequences. Unfortunately, though many pathologies are widespread, there does not exist a unique standardized reference or Gold Standard according to which it is possible to evaluate the patients, mainly when the pathology is in the early stages or is not very noticeable, and the doctor is not sufficiently expert in the problem domain. In this work, we deal with this problem, by envisioning new healthcare services supporting a collaborative clinical analysis of symptoms collected from the patients and forwarded to a group of experts, which are geographically distributed. The experts return back their assessment and diagnosis and the system combines these by means of the Theory of the Evidence, in order to provide a single response. The above services can be easily implemented on top of state-of-the-art distributed computing facilities such as Grids or Clouds, providing a connected environment for medical data distributed over different sites and allowing medical experts to collaborate without being co-located, thereby providing transparent access to data and computing resources. Additionally, such services can provide feedbacks to each expert, in order to improve its own knowledge and experience in the case of divergence between the expert response and the global combined diagnosis in recognizing and classifying the received symptomatic indexes from the patient. We have considered the craniofacial pathologies in infant population as a practical example for better explaining the proposed solution."
"A collaborative computer aided diagnosis (C-CAD) system with eye-tracking, sparse attentional model, and deep learning","Computer aided diagnosis (CAD) tools help radiologists to reduce diagnostic errors such as missing tumors and misdiagnosis. Vision researchers have been analyzing behaviors of radiologists during screening to understand how and why they miss tumors or misdiagnose. In this regard, eye-trackers have been instrumental in understanding visual search processes of radiologists. However, most relevant studies in this aspect are not compatible with realistic radiology reading rooms. In this study, we aim to develop a paradigm shifting CAD system, called collaborative CAD (C-CAD), that unifies CAD and eye-tracking systems in realistic radiology room settings. We first developed an eye-tracking interface providing radiologists with a real radiology reading room experience. Second, we propose a novel algorithm that unifies eye-tracking data and a CAD system. Specifically, we present a new graph based clustering and sparsification algorithm to transform eye-tracking data (gaze) into a graph model to interpret gaze patterns quantitatively and qualitatively. The proposed C-CAD collaborates with radiologists via eye-tracking technology and helps them to improve their diagnostic decisions. The C-CAD uses radiologists’ search efficiency by processing their gaze patterns. Furthermore, the C-CAD incorporates a deep learning algorithm in a newly designed multi-task learning platform to segment and diagnose suspicious areas simultaneously. The proposed C-CAD system has been tested in a lung cancer screening experiment with multiple radiologists, reading low dose chest CTs. Promising results support the efficiency, accuracy and applicability of the proposed C-CAD system in a real radiology room setting. We have also shown that our framework is generalizable to more complex applications such as prostate cancer screening with multi-parametric magnetic resonance imaging (mp-MRI)."
A combined computational and experimental characterization of lean premixed turbulent low swirl laboratory flames II. Hydrogen flames,"We present simulations of laboratory-scale Low Swirl Burner (LSB) flames in order to develop a characterization of the interaction of thermal/diffusive unstable flames with turbulence at the correct scales of laboratory experiments. A Lagrangian diagnostic was developed to overcome the pitfalls of traditional Eulerian analysis techniques when applied to cellular flame systems, including the lack of a well-defined measure of “flame progress” and the time-dependent strain and curvature fields that evolve at scales that are faster than the residence time in the flame zone. An integrated measure of consumption along the pathlines was shown to serve as a generalized analog of the Eulerian-computed consumption-based burning speed. The diffusive fuel flux divergence along the pathlines was shown to correlate directly with integrated consumption rate. Insights gained through the Lagrangian diagnostic analysis served as the underpinning of a new procedure to interpret OH-PLIF images from the LSB experiment. This new diagnostic is able to provide a more physically meaningful approximation to the “flame surface area” than traditional approaches based on PIV processing."
A computer-human interaction model to improve the diagnostic accuracy and clinical decision-making during 12-lead electrocardiogram interpretation,"Introduction The 12-lead Electrocardiogram (ECG) presents a plethora of information and demands extensive knowledge and a high cognitive workload to interpret. Whilst the ECG is an important clinical tool, it is frequently incorrectly interpreted. Even expert clinicians are known to impulsively provide a diagnosis based on their first impression and often miss co-abnormalities. Given it is widely reported that there is a lack of competency in ECG interpretation, it is imperative to optimise the interpretation process. Predominantly the ECG interpretation process remains a paper based approach and whilst computer algorithms are used to assist interpreters by providing printed computerised diagnoses, there are a lack of interactive human-computer interfaces to guide and assist the interpreter. Methods An interactive computing system was developed to guide the decision making process of a clinician when interpreting the ECG. The system decomposes the interpretation process into a series of interactive sub-tasks and encourages the clinician to systematically interpret the ECG. We have named this model ‘Interactive Progressive based Interpretation’ (IPI) as the user cannot ‘progress’ unless they complete each sub-task. Using this model, the ECG is segmented into five parts and presented over five user interfaces (1: Rhythm interpretation, 2: Interpretation of the P-wave morphology, 3: Limb lead interpretation, 4: QRS morphology interpretation with chest lead and rhythm strip presentation and 5: Final review of 12-lead ECG). The IPI model was implemented using emerging web technologies (i.e. HTML5, CSS3, AJAX, PHP and MySQL). It was hypothesised that this system would reduce the number of interpretation errors and increase diagnostic accuracy in ECG interpreters. To test this, we compared the diagnostic accuracy of clinicians when they used the standard approach (control cohort) with clinicians who interpreted the same ECGs using the IPI approach (IPI cohort). Results For the control cohort, the (mean; standard deviation; confidence interval) of the ECG interpretation accuracy was (45.45%; SD=18.1%; CI=42.07, 48.83). The mean ECG interpretation accuracy rate for the IPI cohort was 58.85% (SD=42.4%; CI=49.12, 68.58), which indicates a positive mean difference of 13.4%. (CI=4.45, 22.35) An N−1 Chi-square test of independence indicated a 92% chance that the IPI cohort will have a higher accuracy rate. Interpreter self-rated confidence also increased between cohorts from a mean of 4.9/10 in the control cohort to 6.8/10 in the IPI cohort (p=0.06). Whilst the IPI cohort had greater diagnostic accuracy, the duration of ECG interpretation was six times longer when compared to the control cohort. Conclusions We have developed a system that segments and presents the ECG across five graphical user interfaces. Results indicate that this approach improves diagnostic accuracy but with the expense of time, which is a valuable resource in medical practice."
A data mining-based framework for the identification of daily electricity usage patterns and anomaly detection in building electricity consumption data,"With the development of advanced information techniques, ﻿smart energy meters have made a considerable amount of real-time electricity consumption data available. These data provide a promising way to understand energy usage patterns and improve building energy management. However, previous studies have paid more attention to methodologies for the identification of energy usage patterns and are limited in the interpretability and applications of the patterns. In this context, this paper proposes a general data mining-based framework that can extract typical electricity load patterns (TELPs) and discover insightful information hidden in the patterns. The framework integrates multiple data mining techniques and mainly consists of three phases: data preparation, identification of TELPs and knowledge discovery in the patterns. A new clustering method with a two-step clustering analysis is proposed to identify the TELPs at the individual building level. Before clustering, five statistical features that represent the shapes of electricity load profiles are first defined to reduce the dimensions of daily electricity load profiles. The first clustering step aims at detecting outliers of daily electricity load profiles (DELPs) by using the density-based spatial clustering application with noise (DBSCAN) algorithm clustering technique, which addresses the data quality issues for electricity consumption data derived from energy consumption monitoring platforms (ECMPs). The second clustering step aims at grouping similar DELPs by means of the k-means algorithm to extract TELPs. The effectiveness of the proposed clustering method is demonstrated by a comparison with two single-step clustering techniques. Furthermore, a classification and regression tree (CART) algorithm is employed to discover insightful knowledge on TELPs and improve the interpretability of clustering results, namely, to explain the relations between dynamic influencing factors related to electricity consumption and TELPs. The proposed framework is applied to analyze the time-series electricity consumption data of three practical office buildings in Chongqing, and its effectiveness has been confirmed. A potential application of discovered knowledge is presented: early fault detection of anomalous electricity load profiles. The proposed framework can provide building managers with an efficient way to understand the characteristics of building electricity usage patterns and detect anomalies therein."
A dataset and a methodology for intraoperative computer-aided diagnosis of a metastatic colon cancer in a liver,"The lack of pixel-wise annotated images severely hinders the deep learning approach to computer-aided diagnosis in histopathology. This research creates a public database comprised of: (i) a dataset of 82 histopathological images of hematoxylin-eosin stained frozen sections acquired intraoperatively on 19 patients diagnosed with metastatic colon cancer in a liver; (ii) corresponding pixel-wise ground truth maps annotated by four pathologists, two residents in pathology, and one final-year student of medicine. The Fleiss' kappa equal to 0.74 indicates substantial inter-annotator agreement; (iii) two datasets with images stain-normalized relative to two target images; (iv) development of two conventional machine learning and three deep learning-based diagnostic models. The database is available at http://cocahis.irb.hr. For binary, cancer vs. non-cancer, pixel-wise diagnosis we develop: SVM, kNN, U-Net, U-Net++, and DeepLabv3 classifiers that combine results from original images and stain-normalized images, which can be interpreted as different views. On average, deep learning classifiers outperformed SVM and kNN classifiers on an independent test set 14% in terms of micro balanced accuracy, 15% in terms of the micro F1 score, and 26% in terms of micro precision. As opposed to that, the difference in performance between deep classifiers is within 2%. We found an insignificant difference in performance between deep classifiers trained from scratch and corresponding classifiers pre-trained on non-domain image datasets. The best micro balanced accuracy estimated on the independent test set by the U-Net++ classifier equals 89.34%. Corresponding amounts of F1 score and precision are, respectively, 83.67% and 81.11%."
A decision tree based data-driven diagnostic strategy for air handling units,"Data-driven methods for fault detection and diagnosis of air handling units (AHUs) have attracted wide attention as they do not require high-level expert knowledge of the system of concern. This paper presents a decision tree based data-driven diagnostic strategy for AHUs, in which classification and regression tree (CART) algorithm is used for decision tree induction. A great advantage of the decision tree is that it can be understood and interpreted and therefore its reliability in fault diagnosis can be validated by both testing data and expert knowledge. A steady-state detector and a regression model are incorporated into the strategy to increase the interpretability of the diagnostic strategy developed. The proposed strategy is validated using the data from ASHRAE 1312-RP. It is shown that this strategy can achieve a good diagnostic performance with an average F-measure of 0.97. The interpretation of the diagnostic decision tree using expert knowledge showed that some diagnostic rules generated in the decision tree comply with expert knowledge. Nevertheless, the interpretation also indicated that some diagnostic rules generated are not reliable and some of them are only valid under certain operating conditions, which indirectly demonstrated the importance of the interpretability of fault diagnostic models developed using data-driven methods."
A Decision Tree-Initialised Neuro-fuzzy Approach for Clinical Decision Support,"Apart from the need for superior accuracy, healthcare applications of intelligent systems also demand the deployment of interpretable machine learning models which allow clinicians to interrogate and validate extracted medical knowledge. Fuzzy rule-based models are generally considered interpretable that are able to reflect the associations between medical conditions and associated symptoms, through the use of linguistic if-then statements. Systems built on top of fuzzy sets are of particular appealing to medical applications since they enable the tolerance of vague and imprecise concepts that are often embedded in medical entities such as symptom description and test results. They facilitate an approximate reasoning framework which mimics human reasoning and supports the linguistic delivery of medical expertise often expressed in statements such as ‘weight low’ or ‘glucose level high’ while describing symptoms. This paper proposes an approach by performing data-driven learning of accurate and interpretable fuzzy rule bases for clinical decision support. The approach starts with the generation of a crisp rule base through a decision tree learning mechanism, capable of capturing simple rule structures. The crisp rule base is then transformed into a fuzzy rule base, which forms the input to the framework of adaptive network-based fuzzy inference system (ANFIS), thereby further optimising the parameters of both rule antecedents and consequents. Experimental studies on popular medical data benchmarks demonstrate that the proposed work is able to learn compact rule bases involving simple rule antecedents, with statistically better or comparable performance to those achieved by state-of-the-art fuzzy classifiers."
A Decision Tree-Initialised Neuro-fuzzy Approach for Clinical Decision Support,"Apart from the need for superior accuracy, healthcare applications of intelligent systems also demand the deployment of interpretable machine learning models which allow clinicians to interrogate and validate extracted medical knowledge. Fuzzy rule-based models are generally considered interpretable that are able to reflect the associations between medical conditions and associated symptoms, through the use of linguistic if-then statements. Systems built on top of fuzzy sets are of particular appealing to medical applications since they enable the tolerance of vague and imprecise concepts that are often embedded in medical entities such as symptom description and test results. They facilitate an approximate reasoning framework which mimics human reasoning and supports the linguistic delivery of medical expertise often expressed in statements such as ‘weight low’ or ‘glucose level high’ while describing symptoms. This paper proposes an approach by performing data-driven learning of accurate and interpretable fuzzy rule bases for clinical decision support. The approach starts with the generation of a crisp rule base through a decision tree learning mechanism, capable of capturing simple rule structures. The crisp rule base is then transformed into a fuzzy rule base, which forms the input to the framework of adaptive network-based fuzzy inference system (ANFIS), thereby further optimising the parameters of both rule antecedents and consequents. Experimental studies on popular medical data benchmarks demonstrate that the proposed work is able to learn compact rule bases involving simple rule antecedents, with statistically better or comparable performance to those achieved by state-of-the-art fuzzy classifiers."
A Decision Tree-Initialised Neuro-fuzzy Approach for Clinical Decision Support,"Apart from the need for superior accuracy, healthcare applications of intelligent systems also demand the deployment of interpretable machine learning models which allow clinicians to interrogate and validate extracted medical knowledge. Fuzzy rule-based models are generally considered interpretable that are able to reflect the associations between medical conditions and associated symptoms, through the use of linguistic if-then statements. Systems built on top of fuzzy sets are of particular appealing to medical applications since they enable the tolerance of vague and imprecise concepts that are often embedded in medical entities such as symptom description and test results. They facilitate an approximate reasoning framework which mimics human reasoning and supports the linguistic delivery of medical expertise often expressed in statements such as ‘weight low’ or ‘glucose level high’ while describing symptoms. This paper proposes an approach by performing data-driven learning of accurate and interpretable fuzzy rule bases for clinical decision support. The approach starts with the generation of a crisp rule base through a decision tree learning mechanism, capable of capturing simple rule structures. The crisp rule base is then transformed into a fuzzy rule base, which forms the input to the framework of adaptive network-based fuzzy inference system (ANFIS), thereby further optimising the parameters of both rule antecedents and consequents. Experimental studies on popular medical data benchmarks demonstrate that the proposed work is able to learn compact rule bases involving simple rule antecedents, with statistically better or comparable performance to those achieved by state-of-the-art fuzzy classifiers."
A deep learning driven method for fault classification and degradation assessment in mechanical equipment,"Mechanical degradation may cause equipment to break down with serious safety, environment, and economic impact. Since the equipment usually operates under a tough working environment, which makes it vulnerable and increases the complexity of fault diagnosis. Simultaneously, the requirement of manufacturing systems with reliable self-assessment has been increasingly raised with the trend of smart industry. The aim of this paper is to fill this gap by providing a deep learning driven method for fault classification and degradation assessment. Compared with conventional data-driven methods, deep neural network has the ability to learn multiple nonlinear transformation with high complexity through multiple hidden layers, which helps to capture the main variations and discover the discriminative information from the industrial data. During the experiment, to confirm the effectiveness of deep learning for fault classification and degradation assessment, similar popular data-driven methods, including support vector machine, deep belief network, back propagation neural network, and k-nearest neighbour classification are employed to present a comprehensive comparison in both fault classification and degradation assessment. According to the numerical results, the proposed method outperforms the other conventional approaches and demonstrate its superiority in degradation assessment for mechanical equipment."
A deep learning interpretable classifier for diabetic retinopathy disease grading,"In this paper we present a diabetic retinopathy deep learning interpretable classifier. On one hand, it classifies retina images into different levels of severity with good performance. On the other hand, this classifier is able of explaining the classification results by assigning a score for each point in the hidden and input spaces. These scores indicate the pixel contribution to the final classification. To obtain these scores, we propose a new pixel-wise score propagation model that for every neuron, divides the observed output score into two components. With this method, the generated visual maps can be easily interpreted by an ophthalmologist in order to find the underlying statistical regularities that help to the diagnosis of this eye disease."
A diagnostic tool for population models using non-compartmental analysis: The ncappc package for R,"Background and objective Non-compartmental analysis (NCA) calculates pharmacokinetic (PK) metrics related to the systemic exposure to a drug following administration, e.g. area under the concentration–time curve and peak concentration. We developed a new package in R, called ncappc, to perform (i) a NCA and (ii) simulation-based posterior predictive checks (ppc) for a population PK (PopPK) model using NCA metrics. Methods The nca feature of ncappc package estimates the NCA metrics by NCA. The ppc feature of ncappc estimates the NCA metrics from multiple sets of simulated concentration–time data and compares them with those estimated from the observed data. The diagnostic analysis is performed at the population as well as the individual level. The distribution of the simulated population means of each NCA metric is compared with the corresponding observed population mean. The individual level comparison is performed based on the deviation of the mean of any NCA metric based on simulations for an individual from the corresponding NCA metric obtained from the observed data. The ncappc package also reports the normalized prediction distribution error (NPDE) of the simulated NCA metrics for each individual and their distribution within a population. Results The ncappc produces two default outputs depending on the type of analysis performed, i.e., NCA and PopPK diagnosis. The PopPK diagnosis feature of ncappc produces 8 sets of graphical outputs to assess the ability of a population model to simulate the concentration–time profile of a drug and thereby evaluate model adequacy. In addition, tabular outputs are generated showing the values of the NCA metrics estimated from the observed and the simulated data, along with the deviation, NPDE, regression parameters used to estimate the elimination rate constant and the related population statistics. Conclusions The ncappc package is a versatile and flexible tool-set written in R that successfully estimates NCA metrics from concentration–time data and produces a comprehensive set of graphical and tabular output to summarize the diagnostic results including the model specific outliers. The output is easy to interpret and to use in evaluation of a population PK model. ncappc is freely available on CRAN (http://cran.r-project.org/web/packages/ncappc/index.html/) and GitHub (https://github.com/cacha0227/ncappc/)."
A direct role of collagen glycation in bone fracture,"Non-enzymatic glycation (NEG) is an age-related process accelerated by diseases like diabetes, and causes the accumulation of advanced glycation end-products (AGEs). NEG-mediated modification of bone’s organic matrix, principally collagen type-I, has been implicated in impairing skeletal physiology and mechanics. Here, we present evidence, from in vitro and in vivo models, and establish a causal relationship between collagen glycation and alterations in bone fracture at multiple length scales. Through atomic force spectroscopy, we established that NEG impairs collagen’s ability to dissipate energy. Mechanical testing of in vitro glycated human bone specimen revealed that AGE accumulation due to NEG dramatically reduces the capacity of organic and mineralized matrix to creep and caused bone to fracture under impact at low levels of strain (3000–5000 μstrain) typically associated with fall. Fracture mechanics tests of NEG modified human cortical bone of varying ages, and their age-matched controls revealed that NEG disrupted microcracking based toughening mechanisms and reduced bone propagation and initiation fracture toughness across all age groups. A comprehensive mechanistic model, based on experimental and modeling data, was developed to explain how NEG and AGEs are causal to, and predictive of bone fragility. Furthermore, fracture mechanics and indentation testing on diabetic mice bones revealed that diabetes mediated NEG severely disrupts bone matrix quality in vivo. Finally, we show that AGEs are predictive of bone quality in aging humans and have diagnostic applications in fracture risk."
A disentangled generative model for disease decomposition in chest X-rays via normal image synthesis,"The interpretation of medical images is a complex cognition procedure requiring cautious observation, precise understanding/parsing of the normal body anatomies, and combining knowledge of physiology and pathology. Interpreting chest X-ray (CXR) images is challenging since the 2D CXR images show the superimposition on internal organs/tissues with low resolution and poor boundaries. Unlike previous CXR computer-aided diagnosis works that focused on disease diagnosis/classification, we firstly propose a deep disentangled generative model (DGM) simultaneously generating abnormal disease residue maps and “radiorealistic” normal CXR images from an input abnormal CXR image. The intuition of our method is based on the assumption that disease regions usually superimpose upon or replace the pixels of normal tissues in an abnormal CXR. Thus, disease regions can be disentangled or decomposed from the abnormal CXR by comparing it with a generated patient-specific normal CXR. DGM consists of three encoder-decoder architecture branches: one for radiorealistic normal CXR image synthesis using adversarial learning, one for disease separation by generating a residue map to delineate the underlying abnormal region, and the other one for facilitating the training process and enhancing the model’s robustness on noisy data. A self-reconstruction loss is adopted in the first two branches to enforce the generated normal CXR image to preserve similar visual structures as the original CXR. We evaluated our model on a large-scale chest X-ray dataset. The results show that our model can generate disease residue/saliency maps (coherent with radiologist annotations) along with radiorealistic and patient specific normal CXR images. The disease residue/saliency map can be used by radiologists to improve the CXR reading efficiency in clinical practice. The synthesized normal CXR can be used for data augmentation and normal control of personalized longitudinal disease study. Furthermore, DGM quantitatively boosts the diagnosis performance on several important clinical applications, including normal/abnormal CXR classification, and lung opacity classification/detection."
A fault diagnosis method based on signed directed graph and matrix for nuclear power plants,"In order to solve SDG online fault diagnosis and inference, matrix diagnosis and inference methods are proposed for fault detection and diagnosis (FDD). Firstly, “rules matrix” based on SDG model is used for FDD. Secondly, “status matrix” is proposed to achieve SDG online inference. According to different diagnosis results, “status matrix” is applied for the depth-first search and the breadth-first search respectively to find the propagation paths of each fault. Finally, the SDG model of the secondary-loop system in pressurized water reactor (PWR) is built to verify the effectiveness of the proposed method. The simulation experiment results indicate that the “status matrix” used for online inference can be used to find the fault propagation paths and to explain the causes for fault. Therefore, it can be concluded that the proposed method is one of the fault diagnosis for nuclear power plants (NPPs), which can be used to facilitate the development of fault diagnostic system."
A finite element analysis of diaphragmatic hernia repair on an animal model,"The diaphragm is a mammalian skeletal muscle that plays a fundamental role in the process of respiration. Alteration of its mechanical properties due to a diaphragmatic hernia contributes towards compromising its respiratory functions, leading to the need for surgical intervention to restore the physiological conditions by means of implants. This study aims to assess via numerical modeling biomechanical differences between a diaphragm in healthy conditions and a herniated diaphragm surgically repaired with a polymeric implant, in a mouse model. Finite Element models of healthy and repaired diaphragms are developed from diagnostic images and anatomical samples. The mechanical response of the diaphragmatic tendon is described by assuming an isotropic hyperelastic model. A similar constitutive model is used to define the mechanical behavior of the polymeric implant, while the muscular tissue is modeled by means of a three-element Hill’s model, specifically adapted to mouse muscle fibers. The Finite Element Analysis is addressed to simulate diaphragmatic contraction in the eupnea condition, allowing the evaluation of diaphragm deformation in healthy and herniated-repaired conditions. The polymeric implant reduces diaphragm excursion compared to healthy conditions. This explains the possible alteration in the mechanical functionality of the repaired diaphragm. Looking to the surgical treatment of diaphragmatic hernia in human neonatal subjects, this study suggests the implementation of alternative approaches based on the use of biological implants."
A forest-based algorithm for selecting informative variables using Variable Depth Distribution,"Predictive maintenance of systems and their components in technical systems is a promising approach to optimize system usage and reduce system downtime. Various sensor data are logged during system operation for different purposes, but sometimes not directly related to the degradation of a specific component. Variable selection algorithms are necessary to reduce model complexity and improve interpretability of diagnostic and prognostic algorithms. This paper presents a forest-based variable selection algorithm that analyzes the distribution of a variable in the decision tree structure, called Variable Depth Distribution, to measure its importance. The proposed variable selection algorithm is developed for datasets with correlated variables that pose problems for existing forest-based variable selection methods. The proposed variable selection method is evaluated and analyzed using three case studies: survival analysis of lead–acid batteries in heavy-duty vehicles, engine misfire detection, and a simulated prognostics dataset. The results show the usefulness of the proposed algorithm, with respect to existing forest-based methods, and its ability to identify important variables in different applications. As an example, the battery prognostics case study shows that similar predictive performance is achieved when only 17% percent of the variables are used compared to all measured signals."
A framework for the automatic detection and characterization of brain malformations: Validation on the corpus callosum,"In this paper, we extend the one-class Support Vector Machine (SVM) and the regularized discriminative direction analysis to the Multiple Kernel (MK) framework, providing an effective analysis pipeline for the detection and characterization of brain malformations, in particular those affecting the corpus callosum. The detection of the brain malformations is currently performed by visual inspection of MRI images, making the diagnostic process sensible to the operator experience and subjectiveness. The method we propose addresses these problems by automatically reproducing the neuroradiologist’s approach. One-class SVMs are appropriate to cope with heterogeneous brain abnormalities that are considered outliers. The MK framework allows to efficiently combine the different geometric features that can be used to describe brain structures. Moreover, the regularized discriminative direction analysis is exploited to highlight the specific malformative patterns for each patient. We performed two different experiments. Firstly, we tested the proposed method to detect the malformations of the corpus callosum on a 104 subject dataset. Results showed that the proposed pipeline can classify the subjects with an accuracy larger than 90% and that the discriminative direction analysis can highlight a wide range of malformative patterns (e.g., local, diffuse, and complex abnormalities). Secondly, we compared the diagnosis of four neuroradiologists on a dataset of 128 subjects. The diagnosis was performed both in blind condition and using the classifier and the discriminative direction outputs. Results showed that the use of the proposed pipeline as an assisted diagnosis tool improves the inter-subject variability of the diagnosis. Finally, a graphical representation of the discriminative direction analysis was proposed to enhance the interpretability of the results and provide the neuroradiologist with a tool to fully and clearly characterize the patient malformations at single-subject level."
A fuzzy-ontology-oriented case-based reasoning framework for semantic diabetes diagnosis,"Objective Case-based reasoning (CBR) is a problem-solving paradigm that uses past knowledge to interpret or solve new problems. It is suitable for experience-based and theory-less problems. Building a semantically intelligent CBR that mimic the expert thinking can solve many problems especially medical ones. Methods Knowledge-intensive CBR using formal ontologies is an evolvement of this paradigm. Ontologies can be used for case representation and storage, and it can be used as a background knowledge. Using standard medical ontologies, such as SNOMED CT, enhances the interoperability and integration with the health care systems. Moreover, utilizing vague or imprecise knowledge further improves the CBR semantic effectiveness. This paper proposes a fuzzy ontology-based CBR framework. It proposes a fuzzy case-base OWL2 ontology, and a fuzzy semantic retrieval algorithm that handles many feature types. Material This framework is implemented and tested on the diabetes diagnosis problem. The fuzzy ontology is populated with 60 real diabetic cases. The effectiveness of the proposed approach is illustrated with a set of experiments and case studies. Results The resulting system can answer complex medical queries related to semantic understanding of medical concepts and handling of vague terms. The resulting fuzzy case-base ontology has 63 concepts, 54 (fuzzy) object properties, 138 (fuzzy) datatype properties, 105 fuzzy datatypes, and 2640 instances. The system achieves an accuracy of 97.67%. We compare our framework with existing CBR systems and a set of five machine-learning classifiers; our system outperforms all of these systems. Conclusion Building an integrated CBR system can improve its performance. Representing CBR knowledge using the fuzzy ontology and building a case retrieval algorithm that treats different features differently improves the accuracy of the resulting systems."
A general anomaly detection framework for fleet-based condition monitoring of machines,"Machine failures decrease up-time and can lead to extra repair costs or even to human casualties and environmental pollution. Recent condition monitoring techniques use artificial intelligence in an effort to avoid time-consuming manual analysis and handcrafted feature extraction. Many of these only analyze a single machine and require a large historical data set. In practice, this can be difficult and expensive to collect. However, some industrial condition monitoring applications involve a fleet of similar operating machines. In most of these applications, it is safe to assume healthy conditions for the majority of machines. Deviating machine behavior is then an indicator for a machine fault. This work proposes an unsupervised, generic, anomaly detection framework for fleet-based condition monitoring. It uses generic building blocks and offers three key advantages. First, a historical data set is not required due to online fleet-based comparisons. Second, it allows incorporating domain expertise by user-defined comparison measures. Finally, contrary to most black-box artificial intelligence techniques, easy interpretability allows a domain expert to validate the predictions made by the framework. Two use-cases on an electrical machine fleet demonstrate the applicability of the framework to detect a voltage unbalance by means of electrical and vibration signatures."
A hierarchical approach for causal modeling of process systems,"Cause-and-effect reasoning is at the core of fault diagnosis and hazards analysis in process systems, thereby requiring the development and use of causal models for automated approaches. Furthermore, causal models are also required to explain the decisions and recommendations of artificial intelligence-based systems, lack of which is a serious drawback of purely data-driven approaches. Here, we demonstrate an approach for building multi-level causal models. A hierarchical approach is proposed to capture both cyclic and non-cyclic features of a process plant. Decoupling these features of the plant by constructing two tiers of digraphs, one tier representing overall plant and the other representing individual subsystems, helps in better inference of causal relations present in the system. An algorithm that subsides the effects of indirect causal interactions using reachability matrix and adjacency matrix ideas is also proposed. The algorithm is tested on the Tennessee Eastman benchmark process and the resulting causal model is found to represent the true causal interactions present in the system."
A Hierarchical Dimension Reduction Approach for Big Data with Application to Fault Diagnostics,"About four zetta bytes of data, which falls into the category of big data, is generated by complex manufacturing systems annually. Big data can be utilized to improve the efficiency of an aging manufacturing system, provided, several challenges are handled. In this paper, a novel methodology is presented to detect faults in manufacturing systems while overcoming some of these challenges. Specifically, a generalized distance measure is proposed in conjunction with a novel hierarchical dimension reduction (HDR) approach. It is shown that the HDR can tackle challenges that are frequently observed during distance calculation in big data scenarios, such as norm concentration, redundant dimensions, and a non-invertible correlation matrices. Subsequently, a probabilistic methodology is developed for isolation and detection of faults. Here, Edgeworth expansion based expressions are derived to approximate the density function of the data. The performance of the dimension reduction methodology is demonstrated to be efficient with simulation results involving the use of big data sets. It is shown that HDR is able to explain almost 90% of the total information. Furthermore, the proposed dimension reduction methodology is seen to outperform standard dimension reduction approaches and is able to improve the performance of standard classification methodologies in high dimensional scenarios."
A holistic review of Network Anomaly Detection Systems: A comprehensive survey,"Network Anomaly Detection Systems (NADSs) are gaining a more important role in most network defense systems for detecting and preventing potential threats. The paper discusses various aspects of anomaly-based Network Intrusion Detection Systems (NIDSs). The paper explains cyber kill chain models and cyber-attacks that compromise network systems. Moreover, the paper describes various Decision Engine (DE) approaches, including new ensemble learning and deep learning approaches. The paper also provides more details about benchmark datasets for training and validating DE approaches. Most of NADSs’ applications, such as Data Centers, Internet of Things (IoT), as well as Fog and Cloud Computing, are also discussed. Finally, we present several experimental explanations which we follow by revealing various promising research directions."
A hospital wide predictive model for unplanned readmission using hierarchical ICD data,"Background and objective Hospitals already acquire a large amount of data, mainly for administrative, billing and registration purposes. Tapping on these already available data for additional purposes, aiming at improving care, without significant incremental effort and cost. This potential of secondary patient data is explored through modeling administrative and billing data, as well as the hierarchical structure of pathology codes of the International Classification of Diseases (ICD) in the prediction of unplanned readmissions, as a clinically relevant outcome parameter that can be impacted on in a quality improvement program. Methods In this single-center, hospital-wide observational cohort study, we included all adult patients discharged in 2016 after applying an exclusion protocol (n = 29,702). In addition to administrative variables, such as age and length of stay, structured pathology data were taken into account in predictive models. As a first research question, we compared logistic regression against penalized logistic regression, gradient boosting and Random Forests to predict unplanned readmission. As a second research goal, we investigated the level of hierarchy within the pathology data needed to achieve the best accuracy. Finally, we investigated which prediction variables play a prominent role in predicting hospital readmission. The performance of all models was evaluated using the Area Under the ROC Curve (AUC) measure. Results All models have the best predictive results using Random Forests. An added value of 7% is observed compared to a baseline method such as logistic regression. The best model, based on Random Forests, achieved an AUC of 0.77, using the diagnosis category and procedure code as lowest level of the hierarchical pathology data. Conclusions The most accurate model to predict hospital wide unplanned readmission is based on Random Forests and includes the ICD hierarchy, especially diagnosis category. Such an approach lowers the number of predictor variables and yields a higher interpretability than a model based on a detailed diagnosis. The performance of the model proved high enough to be used as a decision support tool."
A hybrid deep forest approach for outlier detection and fault diagnosis of variable refrigerant flow system,"This paper presents a hybrid deep forest approach for outlier detection and fault diagnosis. Isolation forest algorithm is combined with Pearson's correlation coefficient for outlier detection. The physical significance of outliers detected by the proposed algorithm is explained by origin analysis, which is rarely mentioned in existing studies. In addition, a novel non-neural network deep learning model-cascade forest model is proposed to fault diagnosis of HVAC system for the first time to achieve high precision accuracy in low-dimensional features. The proposed approach is validated with the refrigerant charge fault of VRF system. The results show that the isolation forest algorithm can improve the performance of fault diagnosis model and the mainly outliers of VRF system are defrosting data. The IF-CF model has short operation time, and high accuracy in low-dimensional features. When the dimension drops to 6, the accuracy of the IF-CF model is 94.16%, which is 5.26%, 10.02%, 5.87% and 3.34% higher than the IF-MLP, IF-BPNN, IF-SVM and IF-LSTM models, respectively. Moreover, IF-CF model does not require complex hyper-parameter optimization strategy because its maximum accuracy difference in different hyper-parameters is 2.04%. This study is enlightening which may inspire the potential of outlier detection technology and deep learning in HVAC field."
A hybrid deep learning-based method for short-term building energy load prediction combined with an interpretation process,"Data driven-based building energy load prediction is of great value for building energy management tasks such as fault diagnosis and optimal control. However, there are two challenges for conventional data driven-based prediction methods. The first challenge is that time-lag measurements such as historical cooling loads still cannot be taken full advantage of. To deal with this challenge, a hybrid prediction method is proposed based on long short-term memory networks and artificial neural networks. The second challenge is that data driven-based models are hard to explain by domain knowledge. To deal with this challenge, an interpretation method is proposed based on a dimensionless sensitivity index and a weighted Manhattan distance. Operation data of a public building are utilized to evaluate the proposed methods. Results show that the proposed hybrid prediction method has higher prediction accuracy than conventional prediction methods in one-hour-ahead cooling load prediction. Crucial factors affecting building cooling loads are revealed successfully based on the proposed sensitivity index. Moreover, the weighted Manhattan distance is utilized to quantify the difference between predicted conditions and known conditions of training data. Results show that the prediction accuracy of data driven-based methods is reduced with the increase of the weighted Manhattan distance. It is further discovered that relationships between logarithmic prediction residuals and corresponding logarithmic weighted Manhattan distances are approximatively linear."
A hybrid fault diagnosis methodology with support vector machine and improved particle swarm optimization for nuclear power plants,"The safety and public health during nuclear power plant operation can be enhanced by accurately recognizing and diagnosing potential problems when a malfunction occurs. However, there are still obvious technological gaps in fault diagnosis applications, mainly because adopting a single fault diagnosis method may reduce fault diagnosis accuracy. In addition, some of the proposed solutions rely heavily on fault examples, which cannot fully cover future possible fault modes in nuclear plant operation. This paper presents the results of a research in hybrid fault diagnosis techniques that utilizes support vector machine (SVM) and improved particle swarm optimization (PSO) to perform further diagnosis on the basis of qualitative reasoning by knowledge-based preliminary diagnosis and sample data provided by an on-line simulation model. Further, SVM has relatively good classification ability with small samples compared to other machine learning methodologies. However, there are some challenges in the selection of hyper-parameters in SVM that warrants the adoption of intelligent optimization algorithms. Hence, the major contribution of this paper is to propose a hybrid fault diagnosis method with a comprehensive and reasonable design. Also, improved PSO combined with a variety of search strategies are achieved and compared with other current optimization algorithms. Simulation tests are used to verify the accuracy and interpretability of research findings presented in this paper, which would be beneficial for intelligent execution of nuclear power plant operation."
A hybrid feature extraction approach for brain MRI classification based on Bag-of-words,"Magnetic resonance imaging (MRI) has attracted considerable attention in medical engineering community, since it is a non-invasive diagnostic technique and for its importance in medicine applications. With the aim to study and interpret the image more clearly, a computer-aided diagnosis (CAD) is required. Many automatic classification methods are proposed to classify the human brain MRI (normal/abnormal) to enhance the classification time and decrease the human error. This paper discusses different techniques for MR image classification where different tools are used for features extraction and classification. Based on these reviewed techniques, a new scheme is proposed. Our technique system exploits the benefits of two techniques: Discrete Wavelet Transform (DWT) and Bag-of-Words (BoW). For the validation step, we carried out several experiments based on 256 × 256 images from three datasets (DS-66, DS-160, DS-255) provided by Harvard Medical School. We applied 10 repetitions of k-fold stratified Cross Validation (CV) technique to validate the system performance. The Accuracies reached are respectively 100%, 100%, and 99.61% for DS-66, DS-160, and DS-255 datasets. The overall computation time is about 0.027 s for each MR image. A comparative study with several works showed efficiency and robustness of our system."
A hybrid probabilistic framework for model validation with application to structural dynamics modeling,"Identifying useful mathematical models of physical systems is an essential part of computational modeling and simulation. Once appropriate models are identified, they can be used for applications such as response prediction, structural control, monitoring structural integrity, lifetime prognosis, etc. The number of models and model classes available to the modeler to represent a physical phenomenon, however, can be very large. Retaining all available models throughout a study can be computationally burdensome, so the modeler has the significant problem of identifying the valid models to be used in further studies. To address this challenge, a probabilistic framework is proposed herein for validating models by intertwining the concepts of model falsification and Bayesian model selection. Model falsification, based on the philosophy that measurements can only be used to falsify models, is used in this framework in both pre- and postprocessing steps to eliminate models and model classes, respectively, that cannot explain the measurements. This is the first study to propose a framework to integrate these two paradigms. A likelihood-bound model falsification, previously introduced by the authors, determines the validity of the initial candidate model classes, using the false discovery rate (FDR), and removes most of the incorrect ones without incurring any significant additional computational burden. Next, Bayesian model selection, which assigns posterior model class probabilities based on Bayes’ theorem, is applied to the remaining model classes to identify the model(s) and model class(es) that provide predictions that probabilistically best fit the data. Finally, a postprocessing likelihood-bound falsification checks the validity of the final model class(es). The proposed framework is first illustrated through two nonlinear structural dynamics examples that show the efficacy of the proposed framework in identifying models for these structures as well as reducing the computational burden relative to Bayesian model selection applied alone. Finally, a third example uses measurement data from experiments performed on a full-scale four-story base-isolated building at the world’s largest shake table in Japan’s “E-Defense” laboratory."
A logician’s approach to meta-analysis with unexplained heterogeneity,"Meta-analysis is a powerful tool for combining related studies but such an aggregation would be flawed if studies investigated different populations or applied different methods in this investigation. While studies with differences that are statistically detected but not explained can be analysed by techniques of random-effects meta-analysis, it is difficult to analyse a study if it provides us with complex knowledge. This paper introduces a new method for meta-analysis that deals with both complex knowledge and unexplained heterogeneity, and which shares some properties with Bayesian methods. The newly developed method is applicable in a wide range of medical and also non-medical problems. A demonstration will be provided on a real medical example concerning the one-year incidence of diagnosis of cancer in patients with unprovoked venous thromboembolism. Our main findings based on several recent statistically heterogeneous studies indicate significant improvement in the cancer detection rate if routine evaluation for those patients is performed jointly with extensive screening techniques."
A Machine Learning-Aided Global Diagnostic and Comparative Tool to Assess Effect of Quarantine Control in COVID-19 Spread,"We have developed a globally applicable diagnostic COVID-19 model by augmenting the classical SIR epidemiological model with a neural network module. Our model does not rely upon previous epidemics like SARS/MERS and all parameters are optimized via machine learning algorithms used on publicly available COVID-19 data. The model decomposes the contributions to the infection time series to analyze and compare the role of quarantine control policies used in highly affected regions of Europe, North America, South America, and Asia in controlling the spread of the virus. For all continents considered, our results show a generally strong correlation between strengthening of the quarantine controls as learnt by the model and actions taken by the regions' respective governments. In addition, we have hosted our quarantine diagnosis results for the top 70 affected countries worldwide, on a public platform."
A method for detecting and characterizing outbreaks of infectious disease from clinical reports,"Outbreaks of infectious disease can pose a significant threat to human health. Thus, detecting and characterizing outbreaks quickly and accurately remains an important problem. This paper describes a Bayesian framework that links clinical diagnosis of individuals in a population to epidemiological modeling of disease outbreaks in the population. Computer-based diagnosis of individuals who seek healthcare is used to guide the search for epidemiological models of population disease that explain the pattern of diagnoses well. We applied this framework to develop a system that detects influenza outbreaks from emergency department (ED) reports. The system diagnoses influenza in individuals probabilistically from evidence in ED reports that are extracted using natural language processing. These diagnoses guide the search for epidemiological models of influenza that explain the pattern of diagnoses well. Those epidemiological models with a high posterior probability determine the most likely outbreaks of specific diseases; the models are also used to characterize properties of an outbreak, such as its expected peak day and estimated size. We evaluated the method using both simulated data and data from a real influenza outbreak. The results provide support that the approach can detect and characterize outbreaks early and well enough to be valuable. We describe several extensions to the approach that appear promising."
A method for reduction of Acoustic Emission (AE) data with application in machine failure detection and diagnosis,"The use of AE in machine failure diagnosis has increased over the last years. Most AE-based failure diagnosis strategies use digital signal processing and thus require the sampling of AE signals. High sampling rates are required for this purpose (e.g. 2MHz or higher), leading to streams of large amounts of data. This situation is aggravated if fine resolution and/or multiple sensors are required. These facts combine to produce bulky data, typically in the range of GBytes, for which sufficient storage space and efficient signal processing algorithms are required. This situation probably explains why, in practice, AE-based methods consist mostly in the calculation of scalar quantities such as RMS and Kurtosis, and the analysis of their evolution in time. While the scalar-based approach offers the advantage of maximum data reduction; it has the disadvantage that most part of the information contained in the raw AE signal is lost unrecoverably. This work presents a method offering large data reduction, while keeping the most important information conveyed by the raw AE signal, useful for failure detection and diagnosis. The proposed method consist in the construction of a synthetic, unevenly sampled signal which envelopes the AE bursts present on the raw AE signal in a triangular shape. The constructed signal – which we call TriSignal – also permits the estimation of most scalar quantities typically used for failure detection. But more importantly, it contains the information of the time of occurrence of the bursts, which is key for failure diagnosis. Lomb-Scargle normalized periodogram is used to construct the TriSignal spectrum, which reveals the frequency content of the TriSignal and provides the same information as the classic AE envelope. The paper includes application examples in planetary gearbox and low-speed rolling element bearing."
A methodology using the spectral coherence and healthy historical data to perform gearbox fault diagnosis under varying operating conditions,"Condition monitoring is usually performed over long periods of time when critical rotating machines such as wind turbine gearboxes are monitored. There are many potential signal processing and analysis techniques that can be utilised to diagnose the machine from the condition monitoring data, however, they seldom incorporate the available healthy historical data of a machine systematically in the fault diagnosis process. Hence, a methodology is proposed in this article which supplements the order-frequency spectral coherence with historical data from a healthy machine to perform automatic fault detection, automatic fault localisation and fault trending. This has the benefit that the order-frequency spectral coherence, a very powerful technique for rotating machine fault diagnosis under varying speed conditions, can be utilised without requiring an expert to interpret the results. In this methodology, an extended version of the improved envelope spectrum is utilised to extract features from the order-frequency spectral coherence, whereafter a probabilistic model is carefully used to calculate a diagnostic metric for automatic fault detection and localisation. The methodology is investigated on numerical gearbox data as well as experimental gearbox data, both acquired under time-varying operating conditions with two probabilistic models, namely a Gaussian model and a kernel density estimator, compared as well. The results indicate the potential of this methodology for performing gearbox fault diagnosis under varying operating conditions."
A Mueller matrix approach to flat gold mirror analysis and polarization balancing for use in retinal birefringence scanning systems,"In a polarization-sensitive optical system, it is important to understand the effects that mirrors have on polarization at a specific wavelength of interest, angle of incidence, reflective material, coating, etc. When modeling reflection, the optical designer oftentimes needs to know the Mueller matrix for the particular mirror, in order to assess its overall impact on polarization and possibly reduce the mirror-related effects in the entire system’s performance. Based on an example of a protected gold flat mirror, we investigated two methods, one analytical and one experimental, for determining the Mueller matrix of a noble metallic mirror. The measured Mueller matrix was shown to be nondepolarizing. Then, using polar decomposition, the measured matrix was interpreted as a combination of a pure retarder and a diattenuator. Further investigation showed that the diattenuation property was very weak, which offered the opportunity of largely balancing the introduced polarization deviation by means of a linear compensating retarder. The results of this work were used successfully for improving the design of a diagnostic device for ophthalmology based on retinal birefringence scanning. The described approach also greatly improves the precision of Mueller-matrix-based computer modeling of polarization sensitive systems."
A Multi-Agent Approach Based on Machine-Learning for Fault Diagnosis,"This paper introduces an approach for real-time fault diagnosis in industrial processes. The approach aims to build a decision support tool (DST) that helps operators in large-scale processes diagnose faults and make the correct decisions that will keep production schedules on track. The idea is to combine diversified supervised and semi-supervised machine-learning methods to exploit the strength of each of them in fault diagnosis. Despite their accuracy in classifying faults, supervised methods have two limitations: they do not provide meaningful explanations about their results and cannot diagnose novel faults. Semi-supervised methods can detect and isolate novel faults but cannot disclose their root causes. The proposed approach uses the best of both, providing operators with a descriptive decision that improves the diagnosability of detected faults, whether novel or not. The effectiveness of the proposed approach is demonstrated using a benchmark industrial process."
A multi-chamber model of combustion instabilities and its assessment using kilohertz laser diagnostics in a gas turbine model combustor,"A multi-chamber model for the combustion instabilities manifested in a gas turbine model combustor was developed. The proposed model was used to explain the dependencies of instability frequency on burner geometry and other flow parameters, some of which could not be reconciled with previous models. The new model was built upon the Helmholtz analysis of two connected resonators. The instability frequency as well as the complex pressure ratio between two chambers were predicted by solving ordinary differential equations. To assess the assumptions and predictions of the proposed model, the spectra and magnitude of the oscillations of pressure, heat release rate, and velocity were measured for four different operating conditions: rich (R1), lean (L1), stoichiometric (S1), and reduced flow rate (R2) with a kilohertz laser diagnostic system. These measurements reconfirmed that the instability is of Helmholtz type. A global equivalence ratio that is consistently greater than unity was identified to be an enabling factor for combustion instability. This is also in agreement with the predictions made by the proposed model. Furthermore, the model was shown to be able to predict the right trend of instability frequency when multiple parameters were changed. It is concluded that the current model is an improvement over previous models, because the acoustic coupling between different chambers of the burner was considered."
A multipoint voltage-monitoring method for fuel cell inconsistency analysis,"Traditionally, only one set voltage monitor was used for fuel cell diagnosis and bipolar plate was regarded as the equipotential body. This study presents a novel analysis about this classic method, and proves that only one-set voltage monitor cannot obtain the real health status of every cell. Experimental results point out that the voltage of each cell along the cell gas channel is variable. The current and voltage redistribution phenomenon, caused by the cell interaction, was observed in the fuel cell stack with graphite bipolar plate. Thus, a one-dimensional model has been proposed to explain the mechanism of the current and voltage redistribution with cell interaction. Simulation result shows that, as long as the abnormal cell exists, the current and voltage distribution of the whole stack is significantly affected. Finally, a multipoint voltage-monitoring method has been developed for fuel cell consistency analysis. New method was applied in a specially designed four-cell stack to validate the model analysis. Additionally, the proposed method was used to diagnose anode starvation, which cannot be easily detected using only the one-set voltage monitor for the anode inlet. All experiments and modeling works prove that the multipoint voltage-monitoring method is effective for fuel cell consistency analysis."
A new decision support model for preanesthetic evaluation,"Background and objective The principal challenges in the field of anesthesia and intensive care consist of reducing both anesthetic risks and mortality rate. The ASA score plays an important role in patients' preanesthetic evaluation. In this paper, we propose a methodology to derive simple rules which classify patients in a category of the ASA scale on the basis of their medical characteristics. Methods This diagnosis system is based on MR-Sort, a multiple criteria decision analysis model. The proposed method intends to support two steps in this process. The first is the assignment of an ASA score to the patient; the second concerns the decision to accept—or not—the patient for surgery. Results In order to learn the model parameters and assess its effectiveness, we use a database containing the parameters of 898 patients who underwent preanesthesia evaluation. The accuracy of the learned models for predicting the ASA score and the decision of accepting the patient for surgery is assessed and proves to be better than that of other machine learning methods. Furthermore, simple decision rules can be explicitly derived from the learned model. These are easily interpretable by doctors, and their consistency with medical knowledge can be checked. Conclusions The proposed model for assessing the ASA score produces accurate predictions on the basis of the (limited) set of patient attributes in the database available for the tests. Moreover, the learned MR-Sort model allows for easy interpretation by providing human-readable classification rules."
A new method to study the performance of safety alarm system in process operations,"A new scoring function to learn the dependence structure from a process alarm dataset is proposed here. Based on the new function, a Bayesian network based method is developed to analyze alarm system performance. The proposed method is composed of four features: i) identifying essential variables from alarm data, ii) learning the structure of the Bayesian network from the alarm data; iii) learning the quantitative dependence of the variables in the Bayesian network, and iv) quantitatively describing the strength among the dependent variables. The proposed method helps to describe alarm variables relationships better, enable alarm diagnostic to identify the root causes, and study the variables dependence strength and finally improve the alarm system performance. The proposed method is first explained with a simple example, and further application is demonstrated with a case study of an industrial distributed control system-based alarm system."
A new vibration analysis approach for transformer fault prognosis over cloud environment,"Internet of Things (IoT) and its applications are becoming more prevalent among researchers and companies across the world. IoT technologies offer solutions to many industrial challenges and, as such, they replace classical diagnostic methods with prognostic techniques that can potentially lead to smart monitoring systems. One of the vital applications of IoT is in smart monitoring of major electric power equipment such as transformers whilst in service. Mechanical integrity and operation condition of energized transformers might be evaluated by employing vibration method, which is a non-destructive and economic approach. However, researchers have not yet reached a consensus on how to interpret the results of this method. A new approach has been introduced in this study in order to evaluate transformer real-time vibration signal. A detailed discussion has been provided on transformer vibration modelling and interpretation challenges of the results. Furthermore, a novel method is introduced to evaluate transformer vibration signal during short circuit contingency. As we show, it is straightforward to implement the introduced methods over the cloud environment. Practical studies are conducted on two distribution transformers to examine the introduced methods. The results demonstrate that the methods are remarkably effective, fast and feasible to be programmed over cloud for transformer short circuit fault prognosis."
A non-invasive diagnosis technique of chick embryonic cardiac arrhythmia using near infrared light,"Cardiac arrhythmia is considered important cause of cardiovascular disorder in birds. Little is known, however, about arrhythmia during embryonic stages, chiefly because of their relative rarity and difficulty in diagnosis. Electrocardiogram (ECG) can’t be applied in avian samples non-destructively due to the eggshell. Therefore, an alternative method is an important tool if avian or reptilian embryo research in this area is to advance. Here, we addressed, a non-invasive method to diagnosis cardiac arrhythmia based on optical sensor together with signal processing. In such a system, the intensity of transmitted light is mostly affected by pulsatile blood volumes and mechanical activity of the heart during blood pumping. In our measurement procedure, the time domain transmissive signal was transformed into the frequency domain using fast Fourier transform (FFT). We interpreted the cardiac signal of embryo with a naturally occurred bradycardia throughout the incubation. We found a normal heart rate (HR) during first half of incubation, but HR had two frequency components in the mid incubation and finally a much lower frequency heart rate up until hatching. Early detection of potentially abnormal chicks with cardiovascular defects could significantly contribute to the humane treatment of these embryos and increase production efficiencies by identifying high quality embryos. Moreover, this method could be used in developmental physiology, and cardiovascular medicine research in avian and reptile protocols in the future."
A nonlinear dynamic vibration model of defective bearings – The importance of modelling the finite size of rolling elements,"This paper presents an improved nonlinear dynamic model of the contact forces and vibration response generated in defective rolling element bearings. The improvement comes about by considering the finite size of the rolling elements which overcomes the limitations exhibited by previous models caused by the modelling of rolling elements as point masses. For line spall defects, a low frequency event occurs in the measured vibration response when a rolling element enters the defect. Previous models were not able to accurately predict this event without making the simulated and actual defect geometries significantly different. Comparisons between the proposed model, previous models and experimental results are carried out to show that the low and high frequency events are more accurately predicted. This analysis identifies and explains the mechanisms leading to inaccuracy of the previous models. The model developed here can be used to aid in the development of new diagnostic algorithms."
A novel class imbalance-robust network for bearing fault diagnosis utilizing raw vibration signals,"Recently, although vast intelligent fault diagnosis methods are proposed, their validities are mostly confirmed via balanced datasets, which cannot always hold for the class-imbalance problem prevails among datasets in real-world applications. Hence, a class imbalance-robust network is proposed for bearing fault diagnosis, which tackles class imbalance both in the feature extraction and classification stages. For feature extraction, balanced sparse filtering (BSF) is proposed, which innovatively introduces kurtosis into balancing the discriminative feature extraction capabilities of different classes. Meanwhile, the balancing matrix is also proposed in BSF to remedy the parameter updating imbalance caused by class imbalance. For feature classification, the balancing matrix is also embedded into softmax regression to enhance the balancing capability. Furthermore, extensive experiments on bearing vibration signal datasets are conducted in validity confirmation. Additionally, an interesting property of BSF is investigated, and the phenomenon that class imbalance is actually a two-edge sword is interpreted."
A novel DNA methylation biosensor by combination of isothermal amplification and lateral flow device,"The aberration in DNA methylation pattern has been proved as a promising biomarker for early cancer diagnosis, prognosis and therapy from clinical perspective. In the era of precision medicine, current technologies for DNA methylation identification require not only professional and time-consuming operation but also sophisticated apparatus and complex data interpretation, which has highlighted the need to develop a straightforward and competent strategy for DNA methylation detection. Therefore, the present research is dedicated to establish a novel biosensor by integrating of loop-mediated isothermal amplification (LAMP) and lateral flow device (LFD) to identify trace of DNA methylation from highly heterogeneous cancerous specimen. In order to resolve the natural contradiction between sensitivity and specificity in nucleic acid amplification-based detection methods, we introduce the biotin labeled inner primer and digoxigenin labeled dUTP into LAMP-LFD for the first time, which allow this biosensor to identify as low as 150 copies or 0.1 % target variants under strong interferential background within 35 min after bisulfite conversion. Furthermore, the gold magnetic nanoparticle serving as a signal generator in LFD enables the result to be interpreted through both visual and magnetic detection. The promoter methylation pattern of miR-34a, a significant tumor suppressor gene, has been investigated using LAMP-LFD with cell lines and clinical samples, which indicate that this biosensor may be readily adapted for detection of other DNA methylation biomarkers as a sensitive screening tool for cancer diagnosis."
A novel fault diagnosis method for rotor rub-impact based on nonlinear output frequency response functions and stochastic resonance,"The rub-impact fault of the rotor system is the research object in this paper, and the system is modeled by the finite element method. In the process of the study, the high-order harmonic components appear in the output response of the rotor system, which imply that the rotor system is nonlinear. Nonlinear output frequency response functions (NOFRFs) solve nonlinear problems in the frequency domain and explain these nonlinear phenomena. Based on NOFRFs, a method named weighted contribution rate of NOFRFs (WNOFRFs) is used to improve the weak high-order harmonic components proposed by the authors. In this paper, the multi-parameter optimized genetic algorithm is used to calculate the obstacle parameters of the stochastic resonance (SR) which is used to enhance weak signal components. In view of the fact that both WNOFRFs and SR can improve the weak signal components, a new method of weighted contribution rate of NOFRFs incorporating stochastic resonance (WNS) algorithm is proposed in this paper. Based on the algorithm, the variations of index WS show a trend of approximately linear growth, which indicates that index WS has certain advantages for the fault diagnosis of the rotor rub-impact."
A novel quality evaluation method for resistance spot welding based on the electrode displacement signal and the Chernoff faces technique,"To develop a visual and reliable weld quality assessment method for resistant spot welding, the electrode displacement signal was measured and analyzed. Some statistical features closely related to the weld quality were extracted. The multi-dimensional extracted features from the signal were sketched as Chernoff face which characterizes the welding quality through the different facial expressions. The weld quality was summarized as four levels, namely poor, good, excellent and expulsion, and each level corresponded to one kind of facial expression. By visual judgment, good or bad welds were identified accurately and rapidly even though the weld was made under abnormal welding conditions. In particular, the visual characteristic makes the diagnostic procedure for welding quality easily understood and interpreted."
A one-class classification decision tree based on kernel density estimation,"One-class Classification (OCC) is an important field of machine learning which aims at predicting a single class on the basis of its lonely representatives and potentially some additional counter-examples. OCC is thus opposed to traditional classification problems involving two or more classes, and addresses the issue of class unbalance. There is a wide range of one-class models which give satisfaction in terms of performance. But at the time of explainable artificial intelligence, there is an increasing need for interpretable models. The present work advocates a novel one-class model which tackles this challenge. Within a greedy and recursive approach, our proposal for an explainable One-Class decision Tree (OC-Tree) rests on kernel density estimation to split a data subset on the basis of one or several intervals of interest. Thus, the OC-Tree encloses data within hyper-rectangles of interest which can be described by a set of rules. Against state-of-the-art methods such as Cluster Support Vector Data Description (ClusterSVDD), One-Class Support Vector Machine (OCSVM) and isolation Forest (iForest), the OC-Tree performs favorably on a range of benchmark datasets. Furthermore, we propose a real medical application for which the OC-Tree has demonstrated effectiveness, through the ability to tackle interpretable medical diagnosis aid based on unbalanced datasets."
A patient-similarity-based model for diagnostic prediction,"Objective To simulate the clinical reasoning of doctors, retrieve analogous patients of an index patient automatically and predict diagnoses by the similar/dissimilar patients. Methods We proposed a novel patient-similarity-based framework for diagnostic prediction, which is inspired by the structure-mapping theory about analogy reasoning in psychology. Patient similarity is defined as the similarity between two patients’ diagnoses sets rather than a dichotomous (absence/presence of just one disease). The multilabel classification problem is converted to a single-value regression problem by integrating the pairwise patients’ clinical features into a vector and taking the vector as the input and the patient similarity as the output. In contrast to the common k-NN method which only considering the nearest neighbors, we not only utilize similar patients (positive analogy) to generate diagnostic hypotheses, but also utilize dissimilar patients (negative analogy) are used to reject diagnostic hypotheses. Results The patient-similarity-based models perform better than the one-vs-all baseline and traditional k-NN methods. The f-1 score of positive-analogy-based prediction is 0.698, significantly higher than the scores of baselines ranging from 0.368 to 0.661. It increases to 0.703 when the negative analogy method is applied to modify the prediction results of positive analogy. The performance of this method is highly promising for larger datasets. Conclusion The patient-similarity-based model provides diagnostic decision support that is more accurate, generalizable, and interpretable than those of previous methods and is based on heterogeneous and incomplete data. The model also serves as a new application for the use of clinical big data through artificial intelligence technology."
A phase angle based diagnostic scheme to planetary gear faults diagnostics under non-stationary operational conditions,"Planetary gearbox is a critical component for rotating machinery. It is widely used in wind turbines, aerospace and transmission systems in heavy industry. Thus, it is important to monitor planetary gearboxes, especially for fault diagnostics, during its operational conditions. However, in practice, operational conditions of planetary gearbox are often characterized by variations of rotational speeds and loads, which may bring difficulties for fault diagnosis through the measured vibrations. In this paper, phase angle data extracted from measured planetary gearbox vibrations is used for fault detection under non-stationary operational conditions. Together with sample entropy, fault diagnosis on planetary gearbox is implemented. The proposed scheme is explained and demonstrated in both simulation and experimental studies. The scheme proves to be effective and features advantages on fault diagnosis of planetary gearboxes under non-stationary operational conditions."
A phenomenological model for investigating unequal planet load sharing in epicyclic gearboxes,"Vibration signals of epicyclic gearboxes often present abundant modulation sidebands in the spectrum, and investigations of these sidebands are helpful for the understanding of vibration mechanism and fault diagnosis of epicyclic gearboxes. In epicyclic gearboxes, both the carrier rotation and unequal load sharing among planets can lead to abundant sidebands near the meshing frequency. Modulation sidebands due to the carrier rotation have been extensively investigated, while additional sidebands caused by unequal load sharing among planet gears are seldom studied. Aiming at this issue, a phenomenological model of vibration signals of epicyclic gearboxes is developed considering unequal planet load sharing conditions. In this model, an angular shift of a planet gear is assumed to simulate the manufacturing or assembly errors in gearboxes, and different load sharing ratios among planets are calculated. After that, spectral structures of vibration signals of epicyclic gearboxes under unequal load sharing conditions are derived from the phenomenological model. Through the deduced results, it is shown that unequal planet load sharing can influence the spectral structures largely and the formation mechanisms of some additional sidebands in the spectrum are explained. Finally, experiments under three health conditions, i.e., healthy, cracked sun gear and cracked planet gear, are conducted on an epicyclic gearbox test rig. The effectiveness of theoretical deductions of phenomenological models is validated by the collected vibration signals from the test rig."
"A physics-based model explains the prion-like features of neurodegeneration in Alzheimer’s disease, Parkinson’s disease, and amyotrophic lateral sclerosis","Prion disease is characterized by a chain reaction in which infectious misfolded proteins force native proteins into a similar pathogenic structure. Recent studies have reinforced the hypothesis that the prion paradigm–the templated growth and spreading of misfolded proteins–could help explain the progression of a variety of neurodegenerative disorders. However, our current understanding of prion-like growth and spreading is rather empirical. Here we show that a physics-based reaction-diffusion model can explain the growth and spreading of misfolded protein in Alzheimer’s disease, Parkinson’s disease, and amyotrophic lateral sclerosis. To characterize the progression of misfolded proteins across the brain, we combine the classical Fisher–Kolmogorov equation for population dynamics with an anisotropic diffusion model and simulate misfolding across a sagittal section and across the entire brain. In a systematic sensitivity analysis, we probe the role of the individual model parameters and show that the misfolded protein concentration is sensitive to the coefficients of growth, extracellular diffusion, and axonal transport, to the axonal fiber orientation, and to the initial seeding region. Our model correctly predicts amyloid-β deposits and tau inclusions in Alzheimer’s disease, α-synuclein inclusions in Parkinson’s disease, and TDP-43 inclusions in amyotrophic lateral sclerosis and displays excellent agreement with the histological patterns in diseased human brains. When integrated across the brain, our concentration profiles result in biomarker curves that display a striking similarity with the sigmoid shape and qualitative timeline of clinical biomarker models. Our results suggest that misfolded proteins in various neurodegenerative disorders grow and spread according to a universal law that follows the basic physical principles of nonlinear reaction and anisotropic diffusion. Our findings substantiate the notion of a common underlying principle for the pathogenesis of a wide variety of neurodegenerative disorders, the prion paradigm. A more quantitative understanding of the growth and spreading of misfolded amyloid-β, tau, α-synuclein, and TDP-43 would allow us to establish a prognostic timeframe of disease progression. This could have important clinical implications, ranging from more accurate estimates of the socioeconomic burden of neurodegeneration to a more informed design of clinical trials and pharmacological intervention."
A prognostic algorithm to prescribe improvement measures on throughput bottlenecks,"Throughput bottleneck analysis is important in prioritising production and maintenance measures in a production system. Due to system dynamics, bottlenecks shift between different production resources and across production runs. Therefore, it is important to predict where the bottlenecks will shift to and understand the root causes of predicted bottlenecks. Previous research efforts on bottlenecks are limited to only predicting the shifting location of throughput bottlenecks; they do not give any insights into root causes. Therefore, the aim of this paper is to propose a data-driven prognostic algorithm (using the active-period bottleneck analysis theory) to forecast the durations of individual active states of bottleneck machines from machine event-log data from the manufacturing execution system (MES). Forecasting the duration of active states helps explain the root causes of bottlenecks and enables the prescription of specific measures for them. It thus forms a machine-states-based prescriptive approach to bottleneck management. Data from real-world production systems is used to demonstrate the effectiveness of the proposed algorithm. The practical implications of these results are that shop-floor production and maintenance teams can be forewarned, before a production run, about bottleneck locations, root causes (in terms of machine states) and any prescribed measures, thus forming a prescriptive approach. This approach will enhance the understanding of bottleneck behaviour in production systems and allow data-driven decision making to manage bottlenecks proactively."
A quantitative performance study of two automatic methods for the diagnosis of ovarian cancer,"We present a quantitative study of the performance of two automatic methods for the early detection of ovarian cancer that can exploit longitudinal measurements of multiple biomarkers. The study is carried out for a subset of the data collected in the UK Collaborative Trial of Ovarian Cancer Screening (UKCTOCS). We use statistical analysis techniques, such as the area under the Receiver Operating Characteristic (ROC) curve, for evaluating the performance of two techniques that aim at the classification of subjects as either healthy or suffering from the disease using time-series of multiple biomarkers as inputs. The first method relies on a Bayesian hierarchical model that establishes connections within a set of clinically interpretable parameters. The second technique is a purely discriminative method that employs a recurrent neural network (RNN) for the binary classification of the inputs. For the available dataset, the performance of the two detection schemes is similar (the area under ROC curve is 0.98 for the combination of three biomarkers) and the Bayesian approach has the advantage that its outputs (parameters estimates and their uncertainty) can be further analysed by a clinical expert."
A risk prediction model of gene signatures in ovarian cancer through bagging of GA-XGBoost models,"Introduction Ovarian cancer (OC) is one of the most frequent gynecologic cancers among women, and high-accuracy risk prediction techniques are essential to effectively select the best intervention strategies and clinical management for OC patients at different risk levels. Current risk prediction models used in OC have low sensitivity, and few of them are able to identify OC patients at high risk of mortality, which would both optimize the treatment of high-risk patients and prevent unnecessary medical intervention in those at low risk. Objectives To this end, we have developed a bagging-based algorithm with GA-XGBoost models that predicts the risk of death from OC using gene expression profiles. Methods Four gene expression datasets from public sources were used as training (n = 1) or validation (n = 3) sets. The performance of our proposed algorithm was compared with fine-tuning and other existing methods. Moreover, the biological function of selected genetic features was further interpreted, and the response to a panel of approved drugs was predicted for different risk levels. Results The proposed algorithm showed good sensitivity (74–100%) in the validation sets, compared with two simple models whose sensitivity only reached 47% and 60%. The prognostic gene signature used in this study was highly connected to AKT, a key component of the PI3K/AKT/mTOR signaling pathway, which influences the tumorigenesis, proliferation, and progression of OC. Conclusion These findings demonstrated an improvement in the sensitivity of risk classification of OC patients with our risk prediction models compared with other methods. Ongoing effort is needed to validate the outcomes of this approach for precise clinical treatment."
A simple cleft shaped hydrazine-functionalized colorimetric new Schiff base chemoreceptor for selective detection of F− in organic solvent through PET signaling: Development of a chemoreceptor based sensor kit for detection of fluoride,"Novel colorimetric hydrazine-functionalized Schiff base chemoreceptor [N1N3bis(perfluorobenzylidene)isophthalahydrazide] NBPBIH has been prepared for selective detection of F−. In this receptor more NH and CN units are incorporated for better colorimetric responses as compared to systems having lesser number of such units. NBPBIH turns from colorless to dark yellow on exposure to F−. The detection event is well supported by UV–vis, fluorescence, 1H and 19F-NMR like spectrophotometric and cyclic voltammetric studies in DMSO because of enhanced fluorescence responses, higher Stokes shift value and for its less toxic nature compared to other solvents. Quenching of fluorescence is explained with photoinduced electron transfer mechanism (PET). The binding constant of NBPBIH with F− is around 0.84×105M−1 and limit of detection of F− is found 1.42×10−5M. Our concern is also to address fluorosis: an issue related to global health problem, affecting millions of common people. It is noteworthy that the existing diagnostic and treatment options are of huge expenses. As an artefact, chemoreceptor assisted simple prototype for detecting excessive fluoride in sample solution has been designed and developed which has potential and good prospect to be applied as a low cost affordable diagnostic kit for fluorosis in largely affected countries like China, India and several others."
A Simplified Waveform Energetics Approach to Interpreting Arterial and Venous Pressure,"This paper develops a novel methodology for extracting simple, beat-to-beat, lumped metrics of systemic circulation performance by comparing the catheter pressure waveforms from the femoral artery and vena cava. Their diagnostic potential is compared to the similar, clinically used metric ΔMP, the drop in mean blood pressure between the aorta and vena cava, across an experimental cohort encompassing the progression of sepsis and several clinical interventions known to alter circulatory state. Both Ofe→vc, the model derived pressure attenuation between the femoral artery and vena cava, and the clinical metric, ΔMP, performed well. However, Ofe→vc reduced the optimal average time to sepsis detection from endotoxin infusion from 46.2 minutes for ΔMP to 11.6 minutes, for a slight increase in false positive rate from 1.8% to 6.2%. Thus, the potential diagnostic benefits of this novel approach are demonstrated, and a case is made for further investigation."
A spectral clustering method with semantic interpretation based on axiomatic fuzzy set theory,"Owing to good performance in clustering non-convex datasets, spectral clustering has attracted much attention and become one of the most popular clustering algorithms in the last decades. However, the existing spectral clustering methods are sensitive to parameter settings in building the affinity matrix, which seriously jeopardizes the algorithm's immunity to noise data. Moreover, in many application domains, including credit rating and medical diagnosis, it is very important that the learned model is capable of understandability and interpretability. To make spectral clustering competitive in both classification rate and comprehensibility, we propose a spectral clustering method with semantic interpretation based on axiomatic fuzzy set (AFS) theory, which integrates the representation capability of AFS and the classification competence of spectral clustering (N-cut). The effectiveness of the proposed approach is demonstrated by using real-word datasets, and the experimental results indicate that the performance of our method is comparable with that of classic spectral clustering algorithms (NJW, SM, Diffuzzy, AASC and SOM-SC) and other clustering methods, including K-means, fuzzy c-means, and MinMax K-means. Meanwhile, the proposed method can be used to explore the underlying clusters and give their characteristics in the form of fuzzy descriptions."
A study of laser induced ignition of methane–air mixtures inside a Rapid Compression Machine,"Presented herein is a fundamental study of laser ignition of methane/air mixtures at temperatures and pressures representative of an internal combustion engine. An Nd:YAG laser operating at λ=1064nm was used to ignite methane/air mixtures at equivalence ratios of 0.4 ≤ Φ ≤ 1 in a Rapid Compression Machine (RCM). Experiments were conducted to study the lean limit, minimum spark energy (MSE), and minimum ignition energy (MIE). The results show that laser ignition exhibits a stochastic behavior which must be interpreted statistically. A 90% probability of occurrence was used to evaluate the MSE and MIE, which resulted in MSE90 =2.3mJ and MIE90 =7.2mJ at an equivalence ratio Φ=0.4 at compressed pressure and temperature of Pcomp=29bar and Tcomp=750K, respectively. The lean limit was characterized based on the fraction of chemical energy converted into thermal energy, which was determined by calculating the apparent rate of heat release as derived from RCM high speed pressure data. A lean limit for 90% chemical energy conversion was found to correspond to an equivalence ratio of 0.47 (Tcomp=782K). Schlieren photography was employed as a diagnostics tool to visualize the flame initiation and propagation inside the RCM."
A supervised machine learning-based methodology for analyzing dysregulation in splicing machinery: An application in cancer diagnosis,"Deregulated splicing machinery components have shown to be associated with the development of several types of cancer and, therefore, the determination of such alterations can help the development of tumor-specific molecular targets for early prognosis and therapy. Determining such splicing components, however, is not a straightforward task mainly due to the heterogeneity of tumors, the variability across samples, and the fat-short characteristic of genomic datasets. In this work, a supervised machine learning-based methodology is proposed, allowing the determination of subsets of relevant splicing components that best discriminate samples. The methodology comprises three main phases: first, a ranking of features is determined by means of applying feature weighting algorithms that compute the importance of each splicing component; second, the best subset of features that allows the induction of an accurate classifier is determined by means of conducting an effective heuristic search; then the confidence over the induced classifier is assessed by means of explaining the individual predictions and its global behavior. At the end, an extensive experimental study was conducted on a large collection of transcript-based datasets, illustrating the utility and benefit of the proposed methodology for analyzing dysregulation in splicing machinery."
A survey of medical animations,"Animation is a potentially powerful instrument to convey complex information with movements, smooth transitions between different states that employ the strong human capabilities to perceive and interpret motion. Animation is a natural choice to display time-dependent medical image data or unsteady biomedical simulation data where the dynamic nature of the data is mapped to a kind of video. Animation, however, may also be employed for static data, e.g., to move a camera along a predefined path to convey complex anatomical structures or the spatial relations around a pathology. Clipping planes may be smoothly translated and object transparency adapted to control visibility and further support emphasis of spatial relations, e.g., around a tumor. Virtual endoscopy, where the virtual camera is moved inside an air-filled or fluid-filled structure, is a prominent example of animating static data. Animations, however, are complex visualizations that may depict a larger number of changes in a short period of time. Thus, we also consider cognitive limitations, e.g., change blindness, and discuss methods to reduce the complexity of animations. Emphasis techniques may guide the viewer’s attention and improve the perception of essential features. Finally, interaction beyond the typical video recorder functionality is considered. In this paper, we give a survey of medical animations and discuss the research potential that arises. Although we focus on medicine, the discussion of a research agenda is partially based on cartography, where animation is widely used."
A systematic review of deep transfer learning for machinery fault diagnosis,"With the popularization of the intelligent manufacturing, much attention has been paid in such intelligent computing methods as deep learning ones for machinery fault diagnosis. Thanks to the development of deep learning models, the interference of the human experience can be greatly reduced, and the fault diagnosis accuracy can also be increased under certain conditions. To improve the generalization ability of the intelligent fault diagnostics, the deep transfer learning consisting of both transfer learning and deep learning components was accordingly developed. This paper reviews the research progress of the deep transfer learning for the machinery fault diagnosis in recently years. It is summarizing, classifying and explaining many publications on this topic with discussing various deep transfer architectures and related theories. On this basis, this review expounds main achievements, challenges and future research of the deep transfer learning. This provides clear directions for the selection, design or implementation of the deep transfer learning architecture in the field of the machinery fault diagnostics."
A temperature-driven MPCA method for structural anomaly detection,"An important issue in structural health monitoring (SHM) is to develop appropriate algorithms that can explicitly extract meaningful changes in measurements due to structural anomalies, especially damage. However, the effects due to environmental factors, especially temperature variations may produce significant misinterpretations. Consequently, developing solutions to identify the structural anomaly, accounting for temperature influence, from measurements, is crucial and highly anticipated. This paper presents a Temperature-driven Moving Principal Component Analysis method, designated as Td-MPCA, for anomaly detection. The Td-MPCA introduces the idea of blind source separation (BSS) for thermal identification with intent to enhance the performance of Moving Principal Component Analysis (MPCA) for anomaly detection. To achieve this target, temperature-induced strain variations are first investigated and revealed by employing Independent Component Analysis based on maximization non-Gaussianity, also known as Fast ICA. Afterwards, the MPCA is adopted for anomaly detection on the separated temperature-related response. Three case studies are provided in this paper to evaluate the proposed method. The first one is a numerical truss bridge with a simulated 5% stiffness reduction. The results confirm that Td-MPCA is more sensitive than MPCA in detecting anomalies, where the simulated stiffness loss fails to be detected by MPCA. The second case study is on an experimental truss bridge where two damage scenarios are introduced and interpreted. The detection results show that Td-MPCA outperforms MPCA since the damage is identified at the expected time by Td-MPCA but not by MPCA. The third case study is an in-situ curved viaduct in Switzerland. Data acquired during both construction period and normal service period has been used for interpretation. Results demonstrate that Td-MPCA is able to identify the date of change in construction process without any delay when compared with the application of MPCA only."
A virtual injection sensor by means of time frequency analysis,"A time frequency analysis has been applied to an experimental pressure signal that is measured along a high-pressure pipe that connects the rail to an injector in a fuel injection system for diesel engines. An experimental test campaign has been carried out at a high-performance hydraulic rig for Common Rail systems. The variations in the mean instantaneous frequency, which is calculated from a spectrogram of the experimental injector inlet pressure time histories, are related to key events pertaining to the injection phase. The physical meaning of these variations in the mean instantaneous frequency is identified through a comparison with the outcomes of a numerical simulation using a previously developed one-dimensional model of the injection system. This tool allows the lift of the mobile valves, the instantaneous injected flow-rate and the pressure transients inside the injector to be predicted accurately. The objective of the research has been to realize a virtual sensor to detect the opening and closure instants of the mobile valves within the injector, on the basis of the pattern of the mean instantaneous frequency trace. The realized virtual sensor has been applied to the analysis of both single and multiple injection events."
A visual approach to explainable computerized clinical decision support,"Clinical Decision Support Systems (CDSS) provide assistance to physicians in clinical decision-making. Based on patient-specific evidence items triggering the inferencing process, such as examination findings, and expert-modeled or machine-learned clinical knowledge, these systems provide recommendations in finding the right diagnosis or the optimal therapy. The acceptance of, and the trust in, a CDSS are highly dependent on the transparency of the recommendation’s generation. Physicians must know both the key influences leading to a specific recommendation and the contradictory facts. They must also be aware of the certainty of a recommendation and its potential alternatives. We present a glyph-based, interactive multiple views approach to explainable computerized clinical decision support. Four linked views (1) provide a visual summary of all evidence items and their relevance for the computation result, (2) present linked textual information, such as clinical guidelines or therapy details, (3) show the certainty of the computation result, which includes the recommendation and a set of clinical scores, stagings etc., and (4) facilitate a guided investigation of the reasoning behind the recommendation generation as well as convey the effect of updated evidence items. We demonstrate our approach for a CDSS based on a causal Bayesian network representing the therapy of laryngeal cancer. The approach has been developed in close collaboration with physicians, and was assessed by six expert otolaryngologists as being tailored to physicians’ needs in understanding a CDSS."
Accelerating the Diffusion-Weighted Imaging Biomarker in the clinical practice: comparative study,"Diffusion Weighted Imaging (DWI) methods (ADC and IVIM models) extract meaningful information about the microscopic motions of water of human tissues from MRIs. This is a non invasive method which plays an important role in the diagnosis of ischemic strokes, high grade gliomas or tumors. In the La Fe Polytechnic and University Hospital, the DWI methods aforementioned are used in clinical practice and Matlab is used as a development tool for his out of box performance and fast prototyping. However, each image takes hours to compute due to Matlab’s environment and interpreted functions. Because of this, its use in clinical practice is limited. In this paper we present three compiled versions on which different parallel paradigms based on multicore (OpenMP) and GPU (CUDA) are applied. These implementations have managed to reduce the computation time to less than one minute, therefore, it allows easing their use in daily clinical practice at a cheap acquisition cost."
Accuracy is in the eyes of the pathologist: The visual interpretive process and diagnostic accuracy with digital whole slide images,"Digital whole slide imaging is an increasingly common medium in pathology, with application to education, telemedicine, and rendering second opinions. It has also made it possible to use eye tracking devices to explore the dynamic visual inspection and interpretation of histopathological features of tissue while pathologists review cases. Using whole slide images, the present study examined how a pathologist’s diagnosis is influenced by fixed case-level factors, their prior clinical experience, and their patterns of visual inspection. Participating pathologists interpreted one of two test sets, each containing 12 digital whole slide images of breast biopsy specimens. Cases represented four diagnostic categories as determined via expert consensus: benign without atypia, atypia, ductal carcinoma in situ (DCIS), and invasive cancer. Each case included one or more regions of interest (ROIs) previously determined as of critical diagnostic importance. During pathologist interpretation we tracked eye movements, viewer tool behavior (zooming, panning), and interpretation time. Models were built using logistic and linear regression with generalized estimating equations, testing whether variables at the level of the pathologists, cases, and visual interpretive behavior would independently and/or interactively predict diagnostic accuracy and efficiency. Diagnostic accuracy varied as a function of case consensus diagnosis, replicating earlier research. As would be expected, benign cases tended to elicit false positives, and atypia, DCIS, and invasive cases tended to elicit false negatives. Pathologist experience levels, case consensus diagnosis, case difficulty, eye fixation durations, and the extent to which pathologists’ eyes fixated within versus outside of diagnostic ROIs, all independently or interactively predicted diagnostic accuracy. Higher zooming behavior predicted a tendency to over-interpret benign and atypia cases, but not DCIS cases. Efficiency was not predicted by pathologist- or visual search-level variables. Results provide new insights into the medical interpretive process and demonstrate the complex interactions between pathologists and cases that guide diagnostic decision-making. Implications for training, clinical practice, and computer-aided decision aids are considered."
Accuracy of rule extraction using a recursive-rule extraction algorithm with continuous attributes combined with a sampling selection technique for the diagnosis of liver disease,"Although liver cancer is the second most common cause of death from cancer worldwide, because of the limited accuracy and interpretability of extracted classification rules, the diagnosis of liver disease remains difficult. In addition, hepatitis, which is inflammation of the liver, can progress to fibrosis, cirrhosis, or even liver cancer. Numerous methods for diagnosing liver disease have been applied, but most current diagnostic methods are black box models that cannot adequately reveal information hidden in the data. In the medical setting, extracted rules must be not only highly accurate, but also highly interpretable. The Recursive-Rule eXtraction (Re-RX) algorithm is a white box model that generates highly accurate and interpretable classification rules on the basis of both discrete and continuous attributes; however, it tends to generate more rules than other rule extraction algorithms. The objectives of this study were to use a new rule extraction algorithm, Continuous Re-RX combined with sampling selection techniques (Sampling-Continuous Re-RX), to achieve highly accurate and interpretable diagnostic rules for the BUPA and Hepatitis datasets and to quantify the associations between the presence and severity of ascites and serum biomarkers with the risk of developing hepatitis in consideration of Child-Pugh scores. The performance of Sampling-Continuous Re-RX was compared with existing techniques, and as a result, it was found to extract more accurate, concise, and interpretable rules for the BUPA and Hepatitis datasets compared with previous extraction algorithms. In addition, the rules extracted using the proposed method were close to the trade-off curve, which indicated that they were more accurate and interpretable, and therefore more suitable in the medical setting."
Active learning for semi-supervised structural health monitoring,"A critical issue for structural health monitoring (SHM) strategies based on pattern recognition models is a lack of diagnostic labels to explain the measured data. In an engineering context, these descriptive labels are costly to obtain, and as a result, conventional supervised learning is not feasible. Active learning tools look to solve this issue by selecting a limited number of the most informative observations to query for labels. This work presents the application of cluster-adaptive active learning to measured data from aircraft experiments. These tests successfully illustrate the advantages of utilising active learning tools for SHM, and they present the first application/adaptation of active learning methods to engineering data — a MATLAB package is available via GitHub: https://github.com/labull/cluster_based_active_learning."
Adaptive Kurtogram and its applications in rolling bearing fault diagnosis,"As one of the most important components in the rotating machinery, the rolling bearing will affect the operation precision of the equipment, the running state of the gear, the degree of the axis and even cause the damage of the equipment. Therefore, it is very necessary to improve the processing methods of non-stationary signals. In this paper, an adaptive Kurtogram (AK) method is proposed. The greatest advantage of this method is the use of the order statistics filter (OSF) to estimate and divide the effective modal components from the spectrum to replace the fast Kurtogram (FK). The minimum envelope value of the signal in frequency domain is obtained and taken as the boundaries. Change the sliding window width to divide different boundaries and form an array. The empirical wavelet transform (EWT) is used to reconstruct the signal components according to the boundary array. Then their kurtosis values are calculated. The frequency band with the largest kurtosis value contains the impact information, and the corresponding time domain component presents periodicity impact. AK improves the shortcomings of the center frequency and the bandwidth of the fast Kurtogram (FK) that cannot be explained theoretically. The method of dividing the boundaries in the frequency domain is optimized. After verification by the simulated signals and the actual signals, this method is faster and more efficient."
Adaptive multinomial regression with overlapping groups for multi-class classification of lung cancer,"Multi-class classification has attracted much attention in cancer diagnosis and treatment and many machine learning methods have emerged for addressing this issue recently. However, class imbalance and gene selection problems occur in classifying lung cancer data. In this paper, an adaptive multinomial regression with a sparse overlapping group lasso penalty is proposed to perform classification and grouped gene selection for lung cancer gene expression data. An overlapped grouping strategy with biological interpretability is proposed, which highlights the importance of gene groups from the minority classes. By using the conditional mutual information, the gene significance within each group is evaluated and the data-driven weights are constructed. Based on the grouping strategy and constructed weights, a regularized adaptive multinomial regression is presented and the solving algorithm is developed, which can not only select the important gene groups for each class in performing multi-class classification, but also adaptively select important genes within each group. The experiment results show that the proposed method significantly outperforms the other 6 methods on classification accuracy, and the selected genes are disease-causing genes for lung cancer."
Additional injection timing effects on first cycle during gasoline engine cold start based on ion current detection system,"This paper focuses on the first firing cycle of a cold start process as a means to improve combustion performance and reduce emissions during the cold start of a combined injection strategy engine. A novel additional firing strategy, based on a modified form tandem ion current (IC) signal detection system, was applied to avoid an in-cycle misfire condition. Specifically, by detecting the misfire with the IC signal and then using an additional injection and spark ignition strategy, misfire can be avoided in the current cycle by successful survival combustion. However, if the quantity of additional injection fuel is improper, a misfire may still happen even if this strategy is applied. Furthermore, the requirement for additional fuel was found to be sensitive to the primary ignition timing. Thus, the effects of different ignition timings on the combustion and emissions of the first cycle were also studied. If the additional injection occurs near top dead centre, less fuel needs to be injected to avoid misfire. Having the additional injection timing occur too early or too late were both disadvantageous for additional spark ignition. This is determined by spray condition and piston movement in the cylinder, which is explained in detail by numerical simulation in this paper. Increasing the amount of additional injection fuel can stabilise combustion, but this also increases the hydrocarbon (HC), particulate number (PN), and particulate mass (PM) emissions. If the additional spark ignition fails to cause the combustion after additional injection and ignition, the HC emissions will not dramatically increase compared with basic misfire operation, but the PM will. However, the PM emissions are still at the same level as in normal combustion because the basic misfire condition causes ultra-low PM emissions."
ADSAD: An unsupervised attention-based discrete sequence anomaly detection framework for network security analysis,"Detecting anomalous discrete sequences such as payloads and syscall traces is a crucial task of network security analysis for discovering novel attacks. The data characteristics that lack of labels, very long sequences and irregularly variable lengths make generating proper representations for the sequences for anomaly detection quite challenging. Traditional methods combining shallow models with feature engineering require lots of time and effort from researchers. And they only catch short patterns for the sequences. Recently deep learning is paid more and more attention due to its excellent performance on data representation. Current works simply adopt recurrent neural network based models to this task. They learn the local patterns of the sequences but can not view the sequences globally. Besides, the variable length makes the deep models that accept fixed-size inputs unavailable. Moreover, the deep models usually lack interpretability. Here an unsupervised deep learning framework utilizing attention mechanism called ADSAD is proposed to address these issues. ADSAD takes both the data characteristics and the limitation of the deep models into consideration and generate the global representations for the sequences by two steps, in which the attention mechanism is applied to improve the interpretability. The empirical results showed that the ADSAD instances significantly outperformed the state-of-the-art deep models, with the relative AUC improvement of up to 7%. The attention mechanism not only enhanced the detection performance by up to 73% in terms of AUC but was also able to assist experts for anomaly analysis by visualization."
Advanced methods for image registration applied to JET videos,"The last years have witnessed a significant increase in the use of digital cameras on JET. They are routinely applied for imaging in the IR and visible spectral regions. One of the main technical difficulties in interpreting the data of camera based diagnostics is the presence of movements of the field of view. Small movements occur due to machine shaking during normal pulses while large ones may arise during disruptions. Some cameras show a correlation of image movement with change of magnetic field strength. For deriving unaltered information from the videos and for allowing correct interpretation an image registration method, based on highly distinctive scale invariant feature transform (SIFT) descriptors and on the coherent point drift (CPD) points set registration technique, has been developed. The algorithm incorporates a complex procedure for rejecting outliers. The method has been applied for vibrations correction to videos collected by the JET wide angle infrared camera and for the correction of spurious rotations in the case of the JET fast visible camera (which is equipped with an image intensifier). The method has proved to be able to deal with the images provided by this camera frequently characterized by low contrast and a high level of blurring and noise."
AIDIS: Detecting and classifying anomalous behavior in ubiquitous kernel processes,"Targeted attacks on IT systems are a rising threat against the confidentiality, integrity, and availability of critical information and infrastructures. With the rising prominence of advanced persistent threats (APTs), identifying and understanding such attacks has become increasingly important. Current signature-based systems are heavily reliant on fixed patterns that struggle with unknown or evasive applications, while behavior-based solutions usually leave most of the interpretative work to a human analyst. In this article we propose AIDIS, an Advanced Intrusion Detection and Interpretation System capable to explain anomalous behavior within a network-enabled user session by considering kernel event anomalies identified through their deviation from a set of baseline process graphs. For this purpose we adapt star structures, a bipartite representation used to approximate the edit distance between two graphs. Baseline templates are generated automatically and adapt to the nature of the respective operating system process. We prototypically implemented smart anomaly classification through a set of competency questions applied to graph template deviations and evaluated the approach using both Random Forest and linear kernel support vector machines. The determined attack classes are ultimately mapped to a dedicated APT attacker/defender meta model that considers actions, actors, as well as assets and mitigating controls, thereby enabling decision support and contextual interpretation of ongoing attacks."
An alternate method to extract performance characteristics in dye sensitized solar cells,"Modeling the electrical properties of dye-sensitized solar cells (DSSCs) can fill the gap between the experimental and ideal performance observations for a reliable device diagnosis, design and optimization. The complex physical and chemical reactions between nanocrystalline semiconductor, electrolyte ions and dye molecules make their simulation an open issue to the researchers. Compared to the research works presented in literature, here, we provide a simpler, but more meaningful fit of current −voltage curves by developing a simulation model. The present work provides a reliable framework to extract electrical transport properties of the device, namely, diffusion coefficient, transport time, diffusion length, series resistance and performance parameters from steady state current–voltage curves without using interpretable frequency dependent methods, as well as transient characteristics. The model versatility makes it also capable of predicting the dye regeneration efficiency under short circuit, mid-voltages and high voltage ranges. The simulation method can be also implemented to compare the effect of different electrolytes as well as their species concentrations on regeneration efficiency and overall DSSCs performance. The whole model is designed in a flexible framework to be adapted to various kind of solar cells such as quantum dot and perovskite solar cells."
An anomaly detection framework for dynamic systems using a Bayesian hierarchical framework,"Complex systems are susceptible to many types of anomalies, faults, and abnormal behavior caused by a variety of off-nominal conditions that may ultimately result in major failures or catastrophic events. Early and accurate detection of these anomalies using system inputs and outputs collected from sensors and smart devices has become a challenging problem and an active area of research in many application domains. In this article, we present a new Bayesian hierarchical framework that is able to model the relationship between system inputs (sensor measurements) and outputs (response variables) without imposing strong distributional/parametric assumptions while using only a subset of training samples and sensor attributes. Then, an optimal cost-sensitive anomaly detection framework is proposed to determine whether a sample is an anomalous one taking into consideration the trade-off between misclassification errors and detection rates. The model can be used for both supervised and unsupervised settings depending on the availability of data regarding the behavior of the system under anomaly conditions. The unsupervised model is particularly useful when it is prohibitive to identify in advance the anomalies that a system may present and where no data are available regarding the behavior of the system under anomaly conditions. A Bayesian hierarchical setting is used to structure the proposed framework and help with accommodating uncertainty, imposing interpretability, and controlling the sparsity and complexity of the proposed anomaly detection framework. A Markov chain Monte Carlo algorithm is also developed for model training using past data. The numerical experiments conducted using a simulated data set and a wind turbine data set demonstrate the successful application of the proposed work for system response modeling and anomaly detection."
An application of generalized matrix learning vector quantization in neuroimaging,"Background and objective: Neurodegenerative diseases like Parkinson’s disease often take several years before they can be diagnosed reliably based on clinical grounds. Imaging techniques such as MRI are used to detect anatomical (structural) pathological changes. However, these kinds of changes are usually seen only late in the development. The measurement of functional brain activity by means of [18F]fluorodeoxyglucose positron emission tomography (FDG-PET) can provide useful information, but its interpretation is more difficult. The scaled sub-profile model principal component analysis (SSM/PCA) was shown to provide more useful information than other statistical techniques. Our objective is to improve the performance further by combining SSM/PCA and prototype-based generalized matrix learning vector quantization (GMLVQ). Methods: We apply a combination of SSM/PCA and GMLVQ as a classifier. In order to demonstrate the combination’s validity, we analyze FDG-PET data of Parkinson’s disease (PD) patients collected at three different neuroimaging centers in Europe. We determine the diagnostic performance by performing a ten times repeated ten fold cross validation. Additionally, discriminant visualizations of the data are included. The prototypes and relevance of GMLVQ are transformed back to the original voxel space by exploiting the linearity of SSM/PCA. The resulting prototypes and relevance profiles have then been assessed by three neurologists. Results: One important finding is that discriminative visualization can help to identify disease-related properties as well as differences which are due to center-specific factors. Secondly, the neurologist assessed the interpretability of the method and confirmed that prototypes are similar to known activity profiles of PD patients. Conclusion: We have shown that the presented combination of SSM/PCA and GMLVQ can provide useful means to assess and better understand characteristic differences in FDG-PET data from PD patients and HCs. Based on the assessments by medical experts and the results of our computational analysis we conclude that the first steps towards a diagnostic support system have been taken successfully."
An application of generalized matrix learning vector quantization in neuroimaging,"Background and objective: Neurodegenerative diseases like Parkinson’s disease often take several years before they can be diagnosed reliably based on clinical grounds. Imaging techniques such as MRI are used to detect anatomical (structural) pathological changes. However, these kinds of changes are usually seen only late in the development. The measurement of functional brain activity by means of [18F]fluorodeoxyglucose positron emission tomography (FDG-PET) can provide useful information, but its interpretation is more difficult. The scaled sub-profile model principal component analysis (SSM/PCA) was shown to provide more useful information than other statistical techniques. Our objective is to improve the performance further by combining SSM/PCA and prototype-based generalized matrix learning vector quantization (GMLVQ). Methods: We apply a combination of SSM/PCA and GMLVQ as a classifier. In order to demonstrate the combination’s validity, we analyze FDG-PET data of Parkinson’s disease (PD) patients collected at three different neuroimaging centers in Europe. We determine the diagnostic performance by performing a ten times repeated ten fold cross validation. Additionally, discriminant visualizations of the data are included. The prototypes and relevance of GMLVQ are transformed back to the original voxel space by exploiting the linearity of SSM/PCA. The resulting prototypes and relevance profiles have then been assessed by three neurologists. Results: One important finding is that discriminative visualization can help to identify disease-related properties as well as differences which are due to center-specific factors. Secondly, the neurologist assessed the interpretability of the method and confirmed that prototypes are similar to known activity profiles of PD patients. Conclusion: We have shown that the presented combination of SSM/PCA and GMLVQ can provide useful means to assess and better understand characteristic differences in FDG-PET data from PD patients and HCs. Based on the assessments by medical experts and the results of our computational analysis we conclude that the first steps towards a diagnostic support system have been taken successfully."
An approach for de-noising and contrast enhancement of retinal fundus image using CLAHE,"Now-a-days medical fundus images are widely used in clinical diagnosis for the detection of retinal disorders. Fundus images are generally degraded by noise and suffer from low contrast issues. These issues make it difficult for ophthalmologist to detect and interpret diseases in fundus images. This paper presents a noise removal and contrast enhancement algorithm for fundus image. Integration of filters and contrast limited adaptive histogram equalization (CLAHE) technique is applied for solving the issues of de-noising and enhancement of color fundus image. The efficacy of the proposed method is evaluated through different performance parameters like Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), Correlation coefficient (CoC) and Edge preservation index (EPI). The proposed method achieved 7.85% improvement in PSNR, 1.19% improvement in SSIM, 0.12% improvement in CoC and 1.28% improvement in EPI when compared to the state of the art method."
An Artificial-Intelligence-Based Method to Automatically Create Interpretable Models from Data Targeting Embedded Control Applications,"The development of new automotive drivetrain layouts requires modeling of the involved components to allow for ideal control strategies. The creation of these models is both costly and challenging, specifically because interpretability, accuracy, and computational effort need to be balanced. In this study, a method is put forward which supports experts in the modeling process and in making an educated choice to balance these constraints. The method is based on the artificial intelligence technique of genetic programming. By solving a symbolic regression problem, it automatically identifies equation-based models from data. To address possible system complexities, data-based expressions like curves and maps can additionally be employed for the model identification. The performance of the method is demonstrated based on two examples: 1. Identification of a pure equation based model, demonstrating the benefit of interpretability. 2. Creation of a hybrid-model, combining a base equation with data-based expressions. Possible applications of the method are model creation, system identification, structural optimization, and model reduction. The existing implementation in ETAS ASCMO-MOCA also offers a high efficiency increase by combining and automating the two procedural steps of embedded function engineering and calibration."
An automatic diagnostic network using skew-robust adversarial discriminative domain adaptation to evaluate the severity of depression,"Background and Objective Deep learning provides an automatic and robust solution to depression severity evaluation. However, despite it is powerful, there is a trade-off between robust performance and the cost of manual annotation. Methods Motivated by knowledge evolution and domain adaptation, we propose a deep evaluation network using skew-robust adversarial discriminative domain adaptation (SRADDA), which adaptively shifts its domain from a large-scale Twitter dataset to a small-scale depression interview dataset for evaluating the severity of depression. Results Without top-down selection, SRADDA-based severity evaluation network achieves regression errors of 6.38 (Root Mean Square Error,RMSE) and 4.93 (Mean Absolute Error,MAE), which outperforms baselines provided by the Audio/Visual Emotion Challenge and Workshop(AVEC 2017). However, with top-down selection, the network achieves comparable results (RMSE = 5.13, MAE = 4.08). Conclusions Results show that SRADDA not only represents features robustly, but also performs comparably to state-of-the-art results on small-scale dataset, DAIC-WOZ."
An automatic method for lung segmentation and reconstruction in chest X-ray using deep neural networks,"Background and Objective Chest X-ray (CXR) is one of the most used imaging techniques for detection and diagnosis of pulmonary diseases. A critical component in any computer-aided system, for either detection or diagnosis in digital CXR, is the automatic segmentation of the lung field. One of the main challenges inherent to this task is to include in the segmentation the lung regions overlapped by dense abnormalities, also known as opacities, which can be caused by diseases such as tuberculosis and pneumonia. This specific task is difficult because opacities frequently reach high intensity values which can be incorrectly interpreted by an automatic method as the lung boundary, and as a consequence, this creates a challenge in the segmentation process, because the chances of incomplete segmentations are increased considerably. The purpose of this work is to propose a method for automatic segmentation of lungs in CXR that addresses this problem by reconstructing the lung regions “lost” due to pulmonary abnormalities. Methods The proposed method, which features two deep convolutional neural network models, consists of four steps main steps: (1) image acquisition, (2) initial segmentation, (3) reconstruction and (4) final segmentation. Results The proposed method was experimented on 138 Chest X-ray images from Montgomery County’s Tuberculosis Control Program, and has achieved as best result an average sensitivity of 97.54%, an average specificity of 96.79%, an average accuracy of 96.97%, an average Dice coefficient of 94%, and an average Jaccard index of 88.07%. Conclusions We demonstrate in our lung segmentation method that the problem of dense abnormalities in Chest X-rays can be efficiently addressed by performing a reconstruction step based on a deep convolutional neural network model."
An effective fault diagnosis method for centrifugal chillers using associative classification,"Fault diagnosis for centrifugal chillers is very important for saving energy and maintaining optimal operating conditions. A fault diagnosis method for centrifugal chillers is proposed based on the associative classification (AC) algorithm, which constructs an associative classifier by excavating strong rules between fault classes and physical attributes. First, association rules with significant support and high confidence values are discovered. Instead of the Apriori algorithm, FP-growth is adopted to accelerate association rule mining. Second, only association rules named class association rules (CARs) whose consequents are limited to fault classes are preserved. Third, pruned CARs are obtained by means of ranking CARs and pruning the redundant rules according to the concept of “higher rank”. Fourth, a limited number of rules are selected out of pruned CARs based on the AC algorithm to construct an associative classifier. This approach is validated using experimental centrifugal chiller data from the ASHRAE Research Project 1043 (RP-1043). Results demonstrate that this proposed AC-based approach can effectively identify seven common chiller faults at both low and high severity levels and the average correct fault diagnosis ratio can be examined up to 86.3%."
An efficient cervical disease diagnosis approach using segmented images and cytology reporting,"Cervical cancer is the second most common cancer in women globally. A computer aided cervical disease diagnosis system that can relieve pressure on medical experts and save the cost is proposed. To implement our approach in the reality of cervical diseases diagnosis, a multi-modal framework is designed for three kinds of cervical diseases diagnosis that integrates uterine cervix images, Thinprep Cytology Test, human papillomavirus test, and patients’ age. However, too many features increase memory storage costs and computational costs, and it affects the spread of this system in poor areas. Feature selection not only eliminates redundant or irrelevant features but also finds the factors that influence the disease most first is performed in multi-modal frameworks for cervical diseases diagnosis. The detailed process of the method is as follows: first, according the representative color, an efficient image segmentation algorithm is developed; then from three different types of segmented images, we extract color features and texture features for interpreting uterine cervix images; next, Boruta algorithm is applied to feature selection; finally, the performance of Random Forests that utilizes selected features for cervical disease diagnosis is investigated. In the experiment, the proposed multi-modal diagnostic approach gives the final diagnosis for three different kinds of cervical diseases with 83.1% accuracy, which significantly outperforms methods using any single source of information alone. The validation cohort is applied to validate the efficiency of our method, and the performance of random forest obtained by using only 1.2% of features is like or even better than using 100% of features."
An efficient reachability query based pruning algorithm in e-health scenario,"We propose a Disease-Symptom graph database for our mobile-assisted e-healthcare application. A large Disease-Symptom graph is stored in the cloud and accessed using mobile devices over the Internet. Query and search are the fundamental operations of graph databases. However, while searching the Disease-Symptom graph for making preliminary diagnosis of diseases, queries become complex due to the complex structure of data and also queries are too hard to write and interpret. Moreover, it is not possible to access the graph frequently due to limited bandwidth of the network, transmission delay, and higher cost. Subgraph generation or pruning algorithm for appropriate inputs is one of the solutions to this problem. In this paper, we propose an efficient pruning algorithm by introducing a new approach to decompose the Disease-Symptom graph into a series of symptom trees (ST). All the Symptom trees are merged to build a pruned subgraph which is our requirement. We demonstrate the efficiency and effectiveness of our pruning algorithm both analytically and empirically and validate on Disease-Symptom graph database, as well as other real graph databases. Also a comparison is done with an efficient existing reachability based Chain Cover algorithm after modifying it ChainCoverPrune as pruning algorithm. These two algorithms are tested for storage and access parametric measures for querying the synthetic and real directed databases to show the efficiency of the proposed algorithm."
An embedded electrical impedance analyzer based on the AD5933 for the determination of voice coil motor mechanical properties,"The voice coil motor (VCM) is a simple electromechanical linear motor used in space constrained applications such as pumps, precision positioning, and mobile camera lens actuation. The motion of a VCM is determined by the applied current as well as the mechanical parameters of mass, spring constant, and damping coefficient. VCM motion and mechanical parameters can be determined by a position sensor, but such a sensor may be too bulky for a miniaturized solution. To overcome this limitation, measurements of the VCM electrical impedance versus frequency can be combined with an electromechanical model to identify mechanical parameters. Here we detail an analytical model that relates the electrical impedance to mechanical parameters and demonstrate a miniaturized electrical impedance analyzer for VCMs designed around the AD5933 integrated circuit. The electrical instrument measures impedances of a typical VCM coil of ∼10 Ω with a signal-to-noise ratio of 84.7 dB. We display the effectiveness of the analytical model and impedance analyzer by identifying mechanical parameters of mass, spring constant, and damping coefficient using electrical impedance measurements alone. We experimentally modified the system mass and detected the changes using electrical impedance with a mean error of 5.6% and a Pearson's correlation coefficient of ρ = 0.95. Repeated measurements of a single VCM configuration demonstrated that the natural frequency, knowledge of which is critical for optimal efficiency, was detected with a variation of 0.2%. A departure from harmonic motion was observed at low velocities. We explain this departure by adding static friction to the model of VCM motion. The AD5933-based miniaturized VCM driver and impedance analyzer coupled with a model that relates mechanical motion to electrical impedance is a viable instrument for in-situ diagnostics and tuning of VCMs."
An enhanced variable selection and Isolation Forest based methodology for anomaly detection with OES data,"The development of efficient and interpretable anomaly detection systems is fundamental to keeping production costs low, and is an active area of research in semiconductor manufacturing, particularly in the context of using Optical Emission Spectroscopy (OES) data. The high dimension and correlated nature of OES data can limit the performance achievable with anomaly detection systems. In this paper we present a dimensionality reducing variable selection and isolation forest based anomaly detection and diagnosis methodology that addresses these issues. In particular, it takes account of isolated variables that can be overlooked when using conventional approaches such as PCA, and provides greater interpretability than afforded by PCA. The proposed methodology is illustrated with the aid of simulated and industrial plasma etch case studies."
An experimental and kinetic modeling study of phenylacetylene decomposition and the reactions with acetylene/ethylene under shock tube pyrolysis conditions,"Pyrolysis of phenylacetylene with and without the presence of C2 hydrocarbons (acetylene or ethylene) was studied in a single-pulse shock tube coupled to gas chromatography/gas chromatography-mass spectrometry equipment for speciation diagnostics. Quantitative speciation profiles were probed from each reaction system over the temperature range of 1100–1700 K at a nominal pressure of 20 bar. A kinetic model was proposed to interpret how phenylacetylene is consumed under high-pressure pyrolytic conditions and how the resulting intermediates react to form polycyclic aromatic hydrocarbons (PAHs), and furthermore, how the extra acetylene or ethylene alter the reaction schemes. It was found that the bimolecular reaction between phenylacetylene and hydrogen atom leading to phenyl and acetylene dominates phenylacetylene decomposition throughout the temperature window. The addition/elimination reactions between phenylacetylene and phenyl not only produce hydrogen atoms to maintain the reactivity of the fuel decay, but also directly lead to the formation of several C14H10 PAH isomers including diphenylacetylene, 9-methylene-fluorene and phenanthrene. Intermediates pools, regarding both species categories and abundance, are changed by the two C2 fuels introduced into the reaction system. The added acetylene enables the Hydrogen-Abstraction-Acetylene-Addition (HACA) mechanism starting from the phenylacetylene radical to occur at relatively low temperatures. But the yielded naphthyl core does not stabilize in naphthalene due to the lack of hydrogen atoms in the reaction system, and instead, it carries on the HACA route by further combining with another acetylene molecule, ending up in acenaphthylene. Differently, the added ethylene intensifies the HACA routes by contributing to the acetylene formation, and more importantly, provides hydrogen atoms participating in the naphthalene formation from naphthyl radical."
An explainable algorithm for detecting drug-induced QT-prolongation at risk of torsades de pointes (TdP) regardless of heart rate and T-wave morphology,"Torsade de points (TdP), a life-threatening arrhythmia that can increase the risk of sudden cardiac death, is associated with drug-induced QT-interval prolongation on the electrocardiogram (ECG). While many modern ECG machines provide automated measurements of the QT-interval, these automated QT values are usually correct only for a noise-free normal sinus rhythm, in which the T-wave morphology is well defined. As QT-prolonging drugs often affect the morphology of the T-wave, automated QT measurements taken under these circumstances are easily invalidated. An additional challenge is that the QT-value at risk of TdP varies with heart rate, with the slower the heart rate, the greater the risk of TdP. This paper presents an explainable algorithm that uses an understanding of human visual perception and expert ECG interpretation to automate the detection of QT-prolongation at risk of TdP regardless of heart rate and T-wave morphology. It was tested on a large number of ECGs (n=5050) with variable QT-intervals at varying heart rates, acquired from a clinical trial that assessed the effect of four known QT-prolonging drugs versus placebo on healthy subjects. The algorithm yielded a balanced accuracy of 0.97, sensitivity of 0.94, specificity of 0.99, F1-score of 0.88, ROC (AUC) of 0.98, precision-recall (AUC) of 0.88, and Matthews correlation coefficient (MCC) of 0.88. The results indicate that a prolonged ventricular repolarisation area can be a significant risk predictor of TdP, and detection of this is potentially easier and more reliable to automate than measuring the QT-interval distance directly. The proposed algorithm can be visualised using pseudo-colour on the ECG trace, thus intuitively ‘explaining’ how its decision was made, which results of a focus group show may help people to self-monitor QT-prolongation, as well as ensuring clinicians can validate its results."
An explainable algorithm for detecting drug-induced QT-prolongation at risk of torsades de pointes (TdP) regardless of heart rate and T-wave morphology,"Torsade de points (TdP), a life-threatening arrhythmia that can increase the risk of sudden cardiac death, is associated with drug-induced QT-interval prolongation on the electrocardiogram (ECG). While many modern ECG machines provide automated measurements of the QT-interval, these automated QT values are usually correct only for a noise-free normal sinus rhythm, in which the T-wave morphology is well defined. As QT-prolonging drugs often affect the morphology of the T-wave, automated QT measurements taken under these circumstances are easily invalidated. An additional challenge is that the QT-value at risk of TdP varies with heart rate, with the slower the heart rate, the greater the risk of TdP. This paper presents an explainable algorithm that uses an understanding of human visual perception and expert ECG interpretation to automate the detection of QT-prolongation at risk of TdP regardless of heart rate and T-wave morphology. It was tested on a large number of ECGs (n=5050) with variable QT-intervals at varying heart rates, acquired from a clinical trial that assessed the effect of four known QT-prolonging drugs versus placebo on healthy subjects. The algorithm yielded a balanced accuracy of 0.97, sensitivity of 0.94, specificity of 0.99, F1-score of 0.88, ROC (AUC) of 0.98, precision-recall (AUC) of 0.88, and Matthews correlation coefficient (MCC) of 0.88. The results indicate that a prolonged ventricular repolarisation area can be a significant risk predictor of TdP, and detection of this is potentially easier and more reliable to automate than measuring the QT-interval distance directly. The proposed algorithm can be visualised using pseudo-colour on the ECG trace, thus intuitively ‘explaining’ how its decision was made, which results of a focus group show may help people to self-monitor QT-prolongation, as well as ensuring clinicians can validate its results."
An Explainable Machine Learning Model for Early Detection of Parkinson's Disease using LIME on DaTSCAN Imagery,"Parkinson's Disease (PD) is a degenerative and progressive neurological condition. Early diagnosis can improve treatment for patients and is performed through dopaminergic imaging techniques like the SPECT DaTSCAN. In this study, we propose a machine learning model that accurately classifies any given DaTSCAN as having Parkinson's disease or not, in addition to providing a plausible reason for the prediction. This kind of reasoning is done through the use of visual indicators generated using Local Interpretable Model-Agnostic Explainer (LIME) methods. DaTSCANs were drawn from the Parkinson's Progression Markers Initiative database and trained on a CNN (VGG16) using transfer learning, yielding an accuracy of 95.2%, a sensitivity of 97.5%, and a specificity of 90.9%. Keeping model interpretability of paramount importance, especially in the healthcare field, this study utilises LIME explanations to distinguish PD from non-PD, using visual superpixels on the DaTSCANs. It could be concluded that the proposed system, in union with its measured interpretability and accuracy may effectively aid medical workers in the early diagnosis of Parkinson's Disease."
An Explainable Machine Learning Model for Early Detection of Parkinson's Disease using LIME on DaTSCAN Imagery,"Parkinson's Disease (PD) is a degenerative and progressive neurological condition. Early diagnosis can improve treatment for patients and is performed through dopaminergic imaging techniques like the SPECT DaTSCAN. In this study, we propose a machine learning model that accurately classifies any given DaTSCAN as having Parkinson's disease or not, in addition to providing a plausible reason for the prediction. This kind of reasoning is done through the use of visual indicators generated using Local Interpretable Model-Agnostic Explainer (LIME) methods. DaTSCANs were drawn from the Parkinson's Progression Markers Initiative database and trained on a CNN (VGG16) using transfer learning, yielding an accuracy of 95.2%, a sensitivity of 97.5%, and a specificity of 90.9%. Keeping model interpretability of paramount importance, especially in the healthcare field, this study utilises LIME explanations to distinguish PD from non-PD, using visual superpixels on the DaTSCANs. It could be concluded that the proposed system, in union with its measured interpretability and accuracy may effectively aid medical workers in the early diagnosis of Parkinson's Disease."
An Explainable Multi-Instance Multi-Label Classification Model for Full Slice Brain CT Images,"Brain CT is the first choice for diagnosing intracranial diseases. However, the doctors who can accurate diagnosis is insufficient with the increasing number of patients. Nowadays, many computer-aided diagnosis algorithms were developed to help doctors diagnose and reduce time. However, most of the research classifies each slice isolated, regard this as an image-level classification problem. It is not comprehensive enough because many conditions can only be diagnosed by considering adjacent slices and the relationships between diseases. In order to better fit the characteristics of this task, we formal it as a Multi-instance Multi-label (MIML) learning problem at sequence-level. In this paper, we analyze the difficulties in the brain CT images classification domain. And we propose an efficient model that can improve performance, reduce the number of parameters and give model explanations. In our model, the convolution neural network (CNN) extracts the feature vector from each image of a set of full slice brain CT. The multi-instance detect module focuses on the key images which could assist doctors quickly locate suspicious images and avoid mistakes. We evaluated our model on two datasets: CQ500 and RSNA. The F1 scores are 0.897 and 0.854 respectively. The proposed model outperforms the previous sequence-level model SDLM with only a quarter of the parameters. Low computation and high performance make the model have clinical applicability."
An improved random forest-based rule extraction method for breast cancer diagnosis,"Breast cancer has been becoming the main cause of death in women all around the world. An accurate and interpretable method is necessary for diagnosing patients with breast cancer for well-performed treatment. Nowadays, a great many of ensemble methods have been widely applied to breast cancer diagnosis, capable of achieving high accuracy, such as Random Forest. However, they are black-box methods which are unable to explain the reasons behind the diagnosis. To surmount this limitation, a rule extraction method named improved Random Forest (RF)-based rule extraction (IRFRE) method is developed to derive accurate and interpretable classification rules from a decision tree ensemble for breast cancer diagnosis. Firstly, numbers of decision tree models are constructed using Random Forest to generate abundant decision rules available. And then a rule extraction approach is devised to detach decision rules from the trained trees. Finally, an improved multi-objective evolutionary algorithm (MOEA) is employed to seek for an optimal rule predictor where the constituent rule set is the best trade-off between accuracy and interpretability. The developed method is evaluated on three breast cancer data sets, i.e., the Wisconsin Diagnostic Breast Cancer (WDBC) dataset, Wisconsin Original Breast Cancer (WOBC) dataset, and Surveillance, Epidemiology and End Results (SEER) breast cancer dataset. The experimental results demonstrate that the developed method can primely explain the black-box methods and outperform several popular single algorithms, ensemble learning methods, and rule extraction methods from the view of accuracy and interpretability. What is more, the proposed method can be popularized to other cancer diagnoses in practice, which provides an option to a more interpretable, more accurate cancer diagnosis process."
"An innovative urban energy system constituted by a photovoltaic/thermal hybrid solar installation: Design, simulation and monitoring","The case study presented in this paper is an innovative urban roof-mounted energy system constituted by a hybrid solar system for domestic use. Utilizing this untapped energy is the key value for home renewable energy supply. It allows the improvement of the energy yield per area unit of roof or façade. The Photovoltaic/Thermal (PVT) panel considered presents the particularity of the addition of a transparent insulating cover to reduce the heat losses on its front side. It has been developed by the manufacturer Endef Engineering in collaboration with the University of Zaragoza. In this paper, the design of a PVT system to feed the domestic heat water requirements of multi-housing building is explained. The electricity production is also considered, in accordance with the Spanish regulation for self-consumption. The work developed began with the redesign of the hybrid solar plant, which individually supplies hot water to each dwelling and power for common consumption of the building, including a charging system for electric vehicles. Then, the PVT panel developed and manufactured by Endef Engineering and the complete thermal and electrical system are simulated in Trnsys. The monitoring of a real working installation is used in this paper to validate the proposed model. The presented case study is located in Zaragoza (Spain), in a residential apartment block, with an electrical installed power of 4.14kWp and 20.5kW of thermal capacity. The proposed model allows the calculation of the heat and electricity production and efficiency of the whole system with error lower than 6.5%."
An integral approach to inferential quality control with self-validating soft-sensors,"This paper presents an integral technique for designing an inferential quality control applicable to multivariate processes. The technique includes a self-validating soft-sensor and a multivariate quality control index that depends on the specifications. Based on a partial least squares (PLS) decomposition of the online process measurements, a fault detection and diagnosis technique is used to develop an improved self-validation strategy that is able to confirm, correct or reject the soft-sensor predictions. Model extrapolations, disturbances or sensor faults are first detected through a combined statistic (that considers the calibration region); then, a diagnosis is made by combining statistics pattern recognition, contribution analysis, and disturbance isolation based on historical fault patterns. An off-spec alarm is produced when the proposed index detects that an operating point lies outside the integral design space driven by the specifications. The effectiveness of the proposed technique is evaluated by means of two numerical examples. First, a synthetic example is used to interpret the fundamentals of the method. Then, the technique is applied to the industrial Styrene-Butadiene rubber process, which is emulated through an available numerical simulator."
An interpretable deep-learning model for early prediction of sepsis in the emergency department,"Sepsis is a life-threatening condition with high mortality rates and expensive treatment costs. Early prediction of sepsis improves survival in septic patients. In this paper, we report our top-performing method in the 2019 DII National Data Science Challenge to predict onset of sepsis 4 h before its diagnosis on electronic health records of over 100,000 unique patients in emergency departments. A long short-term memory (LSTM)-based model with event embedding and time encoding is leveraged to model clinical time series and boost prediction performance. Attention mechanism and global max pooling techniques are utilized to enable interpretation for the deep-learning model. Our model achieved an average area under the curve of 0.892 and was selected as one of the winners of the challenge for both prediction accuracy and clinical interpretability. This study paves the way for future intelligent clinical decision support, helping to deliver early, life-saving care to the bedside of septic patients."
An interpretable deep-learning model for early prediction of sepsis in the emergency department,"Sepsis is a life-threatening condition with high mortality rates and expensive treatment costs. Early prediction of sepsis improves survival in septic patients. In this paper, we report our top-performing method in the 2019 DII National Data Science Challenge to predict onset of sepsis 4 h before its diagnosis on electronic health records of over 100,000 unique patients in emergency departments. A long short-term memory (LSTM)-based model with event embedding and time encoding is leveraged to model clinical time series and boost prediction performance. Attention mechanism and global max pooling techniques are utilized to enable interpretation for the deep-learning model. Our model achieved an average area under the curve of 0.892 and was selected as one of the winners of the challenge for both prediction accuracy and clinical interpretability. This study paves the way for future intelligent clinical decision support, helping to deliver early, life-saving care to the bedside of septic patients."
An Investigation into the Clinical Utility of Transfer Functions between the Aortic and Femoral Pressure Waveforms,"Management of cardiovascular disease in intensive care would benefit from improved methods for data from clinically available, information rich arterial pressure waveforms. This paper explores the feasibility of using the transfer function between the aortic and femoral pressure waveforms as a diagnostic tool, over an experimental cohort including the progression of sepsis. Transfer functions appeared as physiologically expected, with Bode plot peaks near breathing and heartbeat frequencies. The Bode plot response to clinical interventions and disease progression also matched physiological expectations, with peaks increasing in magnitude in response to fluid infusion and attenuating in response to the progression of sepsis. While there are clear potential diagnostic benefits to the approach, further work is needed to make this information easier to rapidly interpret in a clinical environment, and to evaluate the specificity of the transfer function responses presented here to the progression of sepsis."
An investigation into the performance and diagnostics from different chordal integration schemes in asymmetric flow,"The purpose of the work is firstly to explore the proposal from a previous North Sea Flow Measurement Workshop paper that increasing the number of chordal locations above 4 yields little improvement in the performance of ultrasonic flow meters. Secondly the work examines the relationship between profile factor and symmetry on theoretical velocity profiles and how that changes with an increased number of chordal locations. Finally, the benefits of more paths for the purposes of swirl cancellation are reviewed, particularly when axial asymmetry and swirl are present concurrently. It is found that in the presence of axial asymmetry there is a significant performance improvement to be had with an increased number of chordal locations. A relationship between profile factor and error is also apparent. The interpolation polynomial and weight function are used to visualise the quadrature and explain the correlation. The theory that more chordal locations will improve the correlation between error and profile factor is investigated, however initial results suggest this is not true. The theory behind swirl cancellation is explained and the velocity profile diagnostics of common designs in the presence of swirl and axial asymmetry are shown. Examples of how the user can be fooled into thinking all is well in the presence of both axial asymmetry and swirl are also shown. The sensitivity of some designs to swirl is also examined. The modelling suggests that in cases where swirl and axial asymmetry are present together and non-axial velocity components are not removed by swirl cancellation or flow conditioners, a change in traditional velocity profile diagnostics cannot be used to indicate the magnitude of the error."
An on-line framework for monitoring nonlinear processes with multiple operating modes,"A multivariate statistical process monitoring scheme should be able to describe multimodal data. Multimodality typically arises in process data due to varying production regimes. Moreover, multimodality may influence how easy it is for process operators to interpret the monitoring results. To address these challenges, this paper proposes an on-line monitoring framework for anomaly detection where an anomaly may either indicate a fault occurring and developing in the process or the process moving to a new operating mode. The framework incorporates the Dirichlet process, which is an unsupervised clustering method, and kernel principal component analysis with a new kernel specialized for multimode data. A monitoring model is trained using the data obtained from several healthy operating modes. When on-line, if a new healthy operating mode is confirmed by an operator, the monitoring model is updated using data collected in the new mode. Implementation issues of this framework, including the parameter tuning for the kernel and the selection of anomaly indicators, are also discussed. A bivariate numerical simulation is used to demonstrate the performance of anomaly detection of the monitoring model. The ability of this framework in model updating and anomaly detection in new operating modes is shown on data from an industrial-scale process using the PRONTO benchmark dataset. The examples will also demonstrate the industrial applicability of the proposed framework."
An unsupervised feature learning framework for basal cell carcinoma image analysis,"Objective The paper addresses the problem of automatic detection of basal cell carcinoma (BCC) in histopathology images. In particular, it proposes a framework to both, learn the image representation in an unsupervised way and visualize discriminative features supported by the learned model. Materials and methods This paper presents an integrated unsupervised feature learning (UFL) framework for histopathology image analysis that comprises three main stages: (1) local (patch) representation learning using different strategies (sparse autoencoders, reconstruct independent component analysis and topographic independent component analysis (TICA), (2) global (image) representation learning using a bag-of-features representation or a convolutional neural network, and (3) a visual interpretation layer to highlight the most discriminant regions detected by the model. The integrated unsupervised feature learning framework was exhaustively evaluated in a histopathology image dataset for BCC diagnosis. Results The experimental evaluation produced a classification performance of 98.1%, in terms of the area under receiver-operating-characteristic curve, for the proposed framework outperforming by 7% the state-of-the-art discrete cosine transform patch-based representation. Conclusions The proposed UFL-representation-based approach outperforms state-of-the-art methods for BCC detection. Thanks to its visual interpretation layer, the method is able to highlight discriminative tissue regions providing a better diagnosis support. Among the different UFL strategies tested, TICA-learned features exhibited the best performance thanks to its ability to capture low-level invariances, which are inherent to the nature of the problem."
Analysis of a failure in a molded package caused by electrochemical migration,"Electrical components can fail during their function in an electrical circuit, especially in a harsh environment. One of the possible failure reasons is the electrochemical migration, which leads to a short circuit or change of electrical parameters of components. This paper focuses on electrochemical effects and explains mechanisms leading to the formation of a conductive path within a component molded in a plastic package. Appropriate diagnostic methods - SEM/EDS and penetration tests have been chosen in order to find the root cause for short circuit creation. It was found that the conductive path appeared due to electrochemical migration of silver between the Electrically Conductive Adhesive (ECA) joints connecting the capacitor package. A tiny gap that was found between the molding compound and the leadframe of the package, showed to be the necessary condition for the electrochemical migration to appear. The main aim of this work was not just to identify the cause of an inadvertent conductive path creation, but also to identify the part of the manufacturing process, where similar problems can be prevented."
"Analysis of an electronic methods for nasopharyngeal carcinoma: Prevalence, diagnosis, challenges and technologies","Context Nasopharyngeal Carcinoma (NPC) is a category of cancer in the head and neck regions, and this cancer prevails in the throat area amongst the pharynx and nasal cavity. This NPC represent major health problem in Malaysia, being the fourth most common tumor among Malaysians and third most common cancer disease among Malaysian men. NPC is emphatically connected with Epstein-Barr Virus (EBV) and happens at the back of the nose. Objectives The overall objective of this research is to discuss the prevalence of NPC in selected regions with a focus on Asia Pacific and Malaysia in particular, the categories, severities, symptoms and the existing electronic methods of investigating NPC symptoms. The research also identifies the weaknesses and strengths of the existing methods as inputs for propose improvement. Most part of the study reviewed the current approaches and techniques that are being proposed in other literatures in order to improve the process of investigating NPC. The study critically evaluates these different methods and the problems associated with them. This study provides some review of literatures indicating relationship between cancer milestones configuration and some NPC patient’s attributes. Methods This research presents a detailed description of image based NPC, NPC imagery techniques and image types. Next, continue to NPC segmentation, which includes modelling of NPC detection, and subsequently followed by a NPC image classification process. Finally, the research paper discussing the meaning and impact of approaches and the contribution of researchers in this area. The different of NPC symptoms and their basic method of diagnosis were briefly explained. The basic diagnostic procedure is depicted with six procedures where a patient undergoes if he or she is reported with NPC. The six procedures include Trans oral mirror examination, EBV Serology, Circulating EBV DNA in plasma (Plasma DNA), Nasoendoscopy, Biopsy and NP Screen™. Finding The strengths and weaknesses of the existing methods are identified and identification model for NPC is proposed. The lesson learned from other methods were evaluated and incorporated into proposed model. This study further describes that segmentation, classification and predication of NPC based on many methods Additionally, a literature survey on segmentation, classification and predication of NPC based on real studies has been discussed."
Analysis of Clinical Discussions Based on Argumentation Schemes,"Clinical discussions usually taking place in healthcare structures allow medical specialists to focus on critical cases, debate about different diagnostic hypotheses, identify therapeutic protocols, or choose among alternative treatments. This paper presents an argumentation-based approach to the analysis of clinical discussions, with the aim of providing a multidisciplinary medical team with a support tool that may help discover whether clinical discussions are affected by any weak points, such as contradicting conclusions, invalid reasoning steps, hidden assumptions, or missing evidences. To this end, we have adopted an approach based on argumentation schemes, which provide an intuitive yet well structured representation of general reasoning patterns. Argumentation schemes include one or more premises, a conclusion, and a set of critical questions that challenge the validity of the relation between premises and conclusion. We exploit argumentation schemes to interpret the assertions made by the participants in a meeting and to generate a graph of arguments connected through edges that represent support or attack relations existing among them. The resulting graph is then used to carry out a logical analysis of the discussion, highlighting, for instance, conflicting opinions or suggesting the need for gathering additional information. To show the potential of our approach, we have developed a sample case based on a clinical discussion taken from literature. After having identified a set of argumentation schemes appropriate for the medical domain considered, the case has been analyzed and a detailed logical analysis has been carried out."
Analysis of electrical parameters of InGaN-based LED packages with aging,"As the light-emitting diode (LED) becomes a mature technology in the general illumination space, there is a tendency to operate LEDs at high current densities and temperatures in order to gain higher light output at lower cost. Further, there is interest among intelligent-lighting platform developers to offer predictive maintenance capabilities to users. The existing useful life prediction model defines LED lifetime based on parametric failure; however, there is a need for a useful life prediction model based on catastrophic failure, which can occur with the degradation of components in an LED package. Electrical parameters, especially package series resistance, are good indicators of LED package health (i.e., remaining useful life) and could potentially be sensed real-time in an application. In this study, the series resistance variation pattern until catastrophic failure was measured at different current and temperature stress conditions. The degradation mechanisms at each phase of variation were explained and, using available models, activation energies and exponents were extracted. The experimental data suggest electromigration-induced metal migration from the contact metallization layer to the semiconductor is the cause of short circuit catastrophic failure of LED packages. The variation patterns of ideality factor and reverse leakage current support this hypothesis. The information presented can be used to develop a catastrophic life estimation model for LED packages under current and temperature stress."
Analysis of stator vibration response for the diagnosis of rub in a coupled rotor-stator system,"The present day rotors are more susceptible to rub due to reduced clearance that adversely affects the performance of the machines. In most of the theoretical investigations on rub, the structural connectivity between rotor and the stator casing is ignored and the dynamics of the rotor are studied with stator interaction as external to the system. Thus, in the present study, a coupled rotor-stator system is modelled where the vibrating rotor transmits the force to the stator structure through the bearing support and during the direct contact. The rub interaction is defined using contact mechanics based Lagrange multiplier method. The model ensures dynamic rotor-stator contact boundary and more realistic contact constraints in contrast to most of the earlier approaches. The stator thus responds through its own vibratory motion and the instantaneous clearance decides the nature of interaction between them. The stator vibration data at the casing location is explored to provide conclusive indication of the rub phenomenon. The intermittent rub interactions that are non-synchronous to rotational frequency cause the system to exhibit quasi-periodic response and the transient excitation due to impacting rotor generates natural frequencies in the system response. In the stator vibration spectrum, the presence of sub harmonic frequency components and stronger presence of harmonics of rotational frequency near the natural frequencies of the system indicate the occurrence of rub. Only low amplitude rotational frequency component is observed otherwise. Strongly correlated with rotor vibration response, the stator vibration response and its rub specific features are proposed for the rub diagnosis."
Analysis of the classifier fusion efficiency in the diagnostics of the accelerometer,"The paper presents the construction and application of multiple classifiers to increase the accuracy of the fault detection module in the diagnostic task. The structure of the ensemble of classifiers is presented and the applied voting mechanisms explained. Methods of storing knowledge in the intelligent diagnostic systems are introduced and their taxonomy provided. Next, the selected algorithms implemented in the fault detection operation are briefly described. Problems with the practical implementation of the proposed solution are considered. The scheme is used to detect faults in the analog part of the MEMS accelerometer. The paper is concluded with the possible prospects for the proposed scheme."
Analyzing a turbulent pipe flow via the one-point structure tensors: Vorticity crawlers and streak shadows,"Efforts to identify and visualize near-wall structures typically focus on the region y+≳5, where large-scale structures with significant turbulent kinetic energy content reside, such as the high-speed and low-speed streaks associated with sweep and ejection events. While it is true that the level of the turbulent kinetic energy drops to zero as one approaches the wall, the organization of near-wall turbulence does not end at y+≈5. Large-scale structures with significant streamwise extent and spatial organization exist even in the immediate proximity of the wall y+<5. These coherent structures have received less attention so far, but it would be both useful and enlightening to bring them to focus in order, on one hand, to understand them, but also to analyze their interaction with the energetic structures that reside at somewhat higher distances from the wall. We have recently developed a rigorous mathematical and computational framework that can be used for the calculation of the turbulence structure tensors in arbitrary flow configurations. In this work, we use this new framework to compute, for the first time, the structure tensors in a fully-developed turbulent pipe flow. We perform Direct Numerical Simulation (DNS) at Reynolds number Reb=5300, based on the bulk velocity and the pipe diameter. We demonstrate the diagnostic properties of the structure tensors, by analyzing the DNS results with a focus on the near-wall structure of the turbulence. We develop a new eduction technique, based on the instantaneous values of the structure tensors, for the identification of inactive structures (i.e. large-scale structures without significant turbulent kinetic energy). This leads to the visualization of “vorticity crawlers” and “streak shadows”, large-scale structures with low energy content in the extreme vicinity of the wall. Furthermore, comparison with traditional eduction techniques (such as instantaneous iso-surfaces of turbulent kinetic energy) shows that the structure-based eduction method seamlessly captures the large-scale energetic structures further away from the wall. We then show that the one-point structure tensors reflect the morphology of the inactive structures in the extreme vicinity of the wall and that of the energy-containing large-scale structures further away from the wall. The emerging complete picture of large-scale structures helps explain the near-wall profiles of all the one-point structure tensors and is likely to have an impact in the further development of Structure-Based Models (SBMs) of turbulence."
Anomaly detection in electric network database of smart grid: Graph matching approach,"Recent studies have shown that the operational modules of an Energy Management System (EMS) are vulnerable to the anomalies that exist in an electric topological and configuration database (DB). In this paper, we focus on the security of EMS modules by detecting anomalies in an electric network DB. Firstly, we explain how an EMS's Optimal Power Flow (OPF) module can be exploited by accidental or deliberate changes in a power system model. As a defense mechanism, for the first time, we propose a graph comparison-based approach for identifying anomalies in an electric network DB. In this study, we formulate the problem as a Quadratic Assignment Problem (QAP) and use the Graduated Assignment algorithm to perform graph matching. To evaluate the effectiveness of the proposed method, we consider different test scenarios considering the IEEE benchmark 24-bus, 30-bus and 118-bus test systems. The results obtained from this analysis show that the proposed method successfully captures DB anomalies at very high detection rates with a smaller time complexity than those obtained from studies published in relevant literature."
"Application of a modeling approach on a cyber-physical system ""RobAIR""","This paper proposes a new method of fault diagnosis of cyber-physical systems (CPS). The complexity of these systems in heterogeneous technologies require elaborate diagnostic techniques and maintenance. We illustrate our methodology on a telepresence robot ""RobAIR"" as an example of CPS. First, we applied a knowledge modeling approach on this case study through the representation of four types of knowledge: functional, structural, topological and behavioral. These representations are achieved using a modeling tool ""Papyrus"". Secondly, from knowledge representation, three algorithms for detection and fault identification are proposed to remove the ambiguity in functions and components. These algorithms will be explained through the RobAIR example."
Application of a thermophoretic immunoassay in the diagnosis of lupus using outer membrane particles from E. coli,"Thermophoresis is the physical diffusion of molecules from hot to cold induced by a thermal gradient. Thermophoresis has been used to evaluate the interaction of biomolecules in solution. In this study, the outer membrane from E. coli was isolated and used to produce OM particles with a diameter of approximately 100 nm. These prepared OM particles were applied in a thermophoretic immunoassay. First, outer membrane (OM) particles with lipopolysaccharides (LPS) and anti-LPS antibodies were used as a model to demonstrate proof of concept and the difference in E. coli thermophoresis was explained by the changes in the molecular surface area (A) and effective charge (σeff). The hydrodynamic size of the molecules was measured as a changing parameter, molecular surface area (A), by dynamic laser scattering (DLS), and the zeta potential was measured as a changing parameter of effective charge (σeff) and then evaluated by the Soret equation. Using the hydrodynamic size and zeta potential values, the interaction between the antigen (OM particle with LPS) and antibody (anti-LPS antibodies) could be monitored and the results were fitted to the thermophoretic immunoassay using the Soret coefficient and equation. Finally, this OM-based immunoassay was applied to the medical diagnosis of systemic lupus erythematosus. Here, OM particles with Ro and La proteins were used to analyze the autoantibodies in patient and control sera. Thermophoretic immunoassay results were also compared to the fitted analysis using hydrodynamic size and zeta potential values and the Soret coefficient and equation."
Application of a two-stage fuzzy neural network to a prostate cancer prognosis system,"Objective This study intends to develop a two-stage fuzzy neural network (FNN) for prognoses of prostate cancer. Methods Due to the difficulty of making prognoses of prostate cancer, this study proposes a two-stage FNN for prediction. The initial membership function parameters of FNN are determined by cluster analysis. Then, an integration of the optimization version of an artificial immune network (Opt-aiNET) and a particle swarm optimization (PSO) algorithm is developed to investigate the relationship between the inputs and outputs. Results The evaluation results for three benchmark functions show that the proposed two-stage FNN has better performance than the other algorithms. In addition, model evaluation results indicate that the proposed algorithm really can predict prognoses of prostate cancer more accurately. Conclusions The proposed two-stage FNN is able to learn the relationship between the clinical features and the prognosis of prostate cancer. Once the clinical data are known, the prognosis of prostate cancer patient can be predicted. Furthermore, unlike artificial neural networks, it is much easier to interpret the training results of the proposed network since they are in the form of fuzzy IF–THEN rules. These rules are very important for medical doctors. This can dramatically assist medical doctors to make decisions."
Application of neural network algorithm in fault diagnosis of mechanical intelligence,"In recent years, mechanical fault diagnosis technology at home and abroad has developed rapidly, and its application has spread to various industrial fields. Due to the complex structure of rotating machinery, the ambiguity and complexity of fault characteristics and causes are common, and it is difficult to carry out fault diagnosis. Although many researches have been carried out and some research results have been obtained, the overall diagnostic level is not very high. High, which is extremely inconsistent with the status quo that is widely used in production. Therefore, it is of great significance to carry out fault diagnosis research on rotating machinery. In this paper, this paper briefly introduces the research and application of intelligent technology in equipment fault diagnosis, and gives the superiority of fuzzy neural network technology application in equipment fault diagnosis, and expounds the basics of fuzzy theory and neural network technology. Based on the principle, the advantages and disadvantages of the two in fault diagnosis are analyzed, and the necessity of combining the two is explained. Based on the previous research on the combination of fuzzy theory and neural network, a new combination method is proposed, and a fuzzy neural network model suitable for fault diagnosis is established. A fuzzy inference method based on fuzzy network is constructed, which realizes the knowledge of information through the extraction, optimization and screening of fuzzy rules. At the same time, the fuzzy neural network learning weights are transformed into case-based reasoning-based diagnostic guidance operators, which play an important role in the rapid extraction of knowledge and improve the diagnostic accuracy. The experimental results show that compared with the commonly used neural network and fuzzy theory fault diagnosis methods, this method can make up for the shortcomings of fuzzy theory and neural network alone. It has higher diagnostic accuracy and has a good application prospect in the field of rotating machinery fault diagnosis."
Application of stacked convolutional and long short-term memory network for accurate identification of CAD ECG signals,"Coronary artery disease (CAD) is the most common cause of heart disease globally. This is because there is no symptom exhibited in its initial phase until the disease progresses to an advanced stage. The electrocardiogram (ECG) is a widely accessible diagnostic tool to diagnose CAD that captures abnormal activity of the heart. However, it lacks diagnostic sensitivity. One reason is that, it is very challenging to visually interpret the ECG signal due to its very low amplitude. Hence, identification of abnormal ECG morphology by clinicians may be prone to error. Thus, it is essential to develop a software which can provide an automated and objective interpretation of the ECG signal. This paper proposes the implementation of long short-term memory (LSTM) network with convolutional neural network (CNN) to automatically diagnose CAD ECG signals accurately. Our proposed deep learning model is able to detect CAD ECG signals with a diagnostic accuracy of 99.85% with blindfold strategy. The developed prototype model is ready to be tested with an appropriate huge database before the clinical usage."
Application of statistical control charts to discriminate transformer winding defects,"A common problem in power transformers is the winding defect which is caused by a variety of internal and external conditions. It is necessary to detect and rectify the winding defect as soon as possible, because they are inherently progressive. One of the effective ways used to diagnose mechanical deformations of power transformers is Frequency Response Analysis (FRA). FRA has been standardized in previous studies but interpretation of the results is still being studied. Recently, results interpretation with the purpose of making the results more objective and quantitative has attracted attentions. To this end, this study presents a novel method for classifying faults which compares transfer function indices called statistical control chart. In order to solve this problem, for the first time, X-bar chart, R-chart, and S-chart with eight tests are used to interpret FRA results to detect different short circuits, axial displacement and radial deformation. The results of the experimental data show that visual fault detection is increased and classification accuracy is improved verifying high performance of this method in detecting and determining various faults."
"Artificial intelligence techniques for stability analysis and control in smart grids: Methodologies, applications, challenges and future directions","Smart grid is the new trend for clean, sustainable, efficient and reliable energy generation, delivery and use. To ensure stable and secure operation is essential for the smart grid, which needs effective stability analysis and control. As the smart grid has evolved through a growing scale of interconnection, increasing integration of renewable energy, widespread operation of direct current power transmission systems, and liberalization of electricity markets, the stability characteristics of it are much more complex than the past. Due to these changes, conventional stability analysis and control approaches have a series of drawbacks in terms of speed, effectiveness and economy. On the contrary, the emerging artificial intelligence (AI) techniques provide powerful and promising tools for stability analysis and control in smart grids and have attracted growing attention. This paper aims to give a comprehensive and clear picture of recent advances in this research area. First, we present a general overview of AI, including its definitions, history and state-of-the-art methodologies. And then, this paper gives a comprehensive review of its applications to security assessment, stability assessment, fault diagnosis, and stability control in smart grids. These applications have achieved impressive results. Nevertheless, we also identify some major challenges these applications face in practice: high requirements on data, imbalanced learning, interpretability of AI, difficulties in transfer learning, the robustness of AI to communication quality, and the robustness against attack or adversarial examples. Furthermore, we provide suggestions for potential important future investigation directions to overcome these challenges and bridge the gap between research and practice."
Assessing diagnostic complexity: An image feature-based strategy to reduce annotation costs,"Computer-aided diagnosis systems can play an important role in lowering the workload of clinical radiologists and reducing costs by automatically analyzing vast amounts of image data and providing meaningful and timely insights during the decision making process. In this paper, we present strategies on how to better manage the limited time of clinical radiologists in conjunction with predictive model diagnosis. We first introduce a metric for discriminating between the different categories of diagnostic complexity (such as easy versus hard) encountered when interpreting CT scans. Second, we propose to learn the diagnostic complexity using a classification approach based on low-level image features automatically extracted from pixel data. We then show how this classification can be used to decide how to best allocate additional radiologists to interpret a case based on its diagnosis category. Using a lung nodule image dataset, we determined that, by a simple division of cases into hard and easy to diagnose, the number of interpretations can be distributed to significantly lower the cost with limited loss in prediction accuracy. Furthermore, we show that with just a few low-level image features (18% of the original set) we are able to determine the easy from hard cases for a significant subset (66%) of the lung nodule image data."
Assessment of a personalized and distributed patient guidance system,"Objectives The MobiGuide project aimed to establish a ubiquitous, user-friendly, patient-centered mobile decision-support system for patients and for their care providers, based on the continuous application of clinical guidelines and on semantically integrated electronic health records. Patients would be empowered by the system, which would enable them to lead their normal daily lives in their regular environment, while feeling safe, because their health state would be continuously monitored using mobile sensors and self-reporting of symptoms. When conditions occur that require medical attention, patients would be notified as to what they need to do, based on evidence-based guidelines, while their medical team would be informed appropriately, in parallel. We wanted to assess the system’s feasibility and potential effects on patients and care providers in two different clinical domains. Materials and methods We describe MobiGuide’s architecture, which embodies these objectives. Our novel methodologies include a ubiquitous architecture, encompassing a knowledge elicitation process for parallel coordinated workflows for patients and care providers; the customization of computer-interpretable guidelines (CIGs) by secondary contexts affecting remote management and distributed decision-making; a mechanism for episodic, on demand projection of the relevant portions of CIGs from a centralized, backend decision-support system (DSS), to a local, mobile DSS, which continuously delivers the actual recommendations to the patient; shared decision-making that embodies patient preferences; semantic data integration; and patient and care provider notification services. MobiGuide has been implemented and assessed in a preliminary fashion in two domains: atrial fibrillation (AF), and gestational diabetes Mellitus (GDM). Ten AF patients used the AF MobiGuide system in Italy and 19 GDM patients used the GDM MobiGuide system in Spain. The evaluation of the MobiGuide system focused on patient and care providers’ compliance to CIG recommendations and their satisfaction and quality of life. Results Our evaluation has demonstrated the system’s capability for supporting distributed decision-making and its use by patients and clinicians. The results show that compliance of GDM patients to the most important monitoring targets – blood glucose levels (performance of four measurements a day: 0.87±0.11; measurement according to the recommended frequency of every day or twice a week: 0.99±0.03), ketonuria (0.98±0.03), and blood pressure (0.82±0.24) – was high in most GDM patients, while compliance of AF patients to the most important targets was quite high, considering the required ECG measurements (0.65±0.28) and blood-pressure measurements (0.75±1.33). This outcome was viewed by the clinicians as a major potential benefit of the system, and the patients have demonstrated that they are capable of self-monitoring – something that they had not experienced before. In addition, the system caused the clinicians managing the AF patients to change their diagnosis and subsequent treatment for two of the ten AF patients, and caused the clinicians managing the GDM patients to start insulin therapy earlier in two of the 19 patients, based on system’s recommendations. Based on the end-of-study questionnaires, the sense of safety that the system has provided to the patients was its greatest asset. Analysis of the patients’ quality of life (QoL) questionnaires for the AF patients was inconclusive, because while most patients reported an improvement in their quality of life in the EuroQoL questionnaire, most AF patients reported a deterioration in the AFEQT questionnaire. Discussion Feasibility and some of the potential benefits of an evidence-based distributed patient-guidance system were demonstrated in both clinical domains. The potential application of MobiGuide to other medical domains is supported by its standards-based patient health record with multiple electronic medical record linking capabilities, generic data insertion methods, generic medical knowledge representation and application methods, and the ability to communicate with a wide range of sensors. Future larger scale evaluations can assess the impact of such a system on clinical outcomes. Conclusion MobiGuide’s feasibility was demonstrated by a working prototype for the AF and GDM domains, which is usable by patients and clinicians, achieving high compliance to self-measurement recommendations, while enhancing the satisfaction of patients and care providers."
Assessment of diagnostic and prognostic condition indices for efficient and robust maintenance decision-making of systems subject to stress corrosion cracking,"Seeking condition indices characterizing the health state of a system is a key problem in condition-based maintenance. For this purpose, diagnostic and prognostic models have been unceasingly developed and improved over the past few decades; nevertheless none of them explains thoroughly the impacts of such indices on the effectiveness of maintenance operations. As a complement to these efforts, this paper analyzes the effectiveness of some well-known diagnostic and prognostic indices for maintenance decision-making. The study is based on a system subject to competing risks due to multiple crack paths. A periodic inspection scheme is used to monitor the system health state. Each inspection returns the perfect diagnostic information: the number of cracks, corresponding crack sizes, and the system failure/working state. Based on this information, two kinds of prognostic condition indices are predicted: the average value and probability law of the system residual useful life. The associated condition-based maintenance strategies and cost models are then developed and compared with the ones whose maintenance decisions are based on diagnostic condition indices. The comparison results allow us to conclude on the performance and on the robustness of these strategies, hence giving some suggestions on the choice of reliable condition indices for maintenance decision-making."
Assessment of the stabilization mechanisms of turbulent lifted jet flames at elevated pressure using combined 2-D diagnostics,"The stabilization mechanisms of turbulent lifted jet flames in a co-flow have been investigated at a pressure of 7 bar. The structure of the flame base was measured with combined OH and CH2O planar laser induced fluorescence (PLIF) and the spatial distribution of equivalence ratio was imaged, simultaneously, with CH4 Raman scattering. The velocity field was also measured with particle imaging velocimetry (PIV). Different bulk jet velocities Uj and co-flow velocities Uc were examined. Data show that flames with Uc = 0.6 m/s stabilize much further away from the nozzle than those with Uc = 0.3 m/s and that their structure does not resemble that of the edge-flames found closer to the nozzle. In addition, for Uc = 0.6 m/s, the measured lift-off height decreases with increasing bulk jet velocity, which is opposite to what is typically observed for lifted flames. Statistical examination of CH4 Raman images shows that the flames with Uc = 0.6 m/s propagate through regions of the flow where the equivalence ratio is not always stoichiometric but, instead, spans the whole flammability range. This is not consistent with edge-flames and is, instead, indicative of premixed burning. This is corroborated by PIV results which show that the flame base velocity exceeds that typically reported for edge-flames. Measurements of relevant flow properties were also conducted in non-reacting jets to predict the turbulent burning velocity of these lifted flames burning in a premixed mode. For Uc = 0.6 m/s and relatively large bulk jet velocities (Uj = 10 and 15 m/s), the predicted turbulent burning velocities are sufficiently high to counter the incoming flow of reactants and, in turn, allow flame stabilization. However, for a lower bulk jet velocity of Uj = 5 m/s, the predicted turbulent burning velocity is much less, leading to blow-out. This explains why the lift-off height decreases with increasing jet velocity for methane at 7 bar and Uc = 0.6 m/s. Data also shows that increasing pressure promotes transition from edge-flames to premixed flames due to reduced laminar burning velocity and enhanced mixing."
Attentive and ensemble 3D dual path networks for pulmonary nodules classification,"Automated pulmonary nodules classification aims at predicting whether a candidate nodule is benign or malignant. It is of great significance for computer-aided diagnosis of lung cancer. Despite the substantial progress achieved by existing methods, several challenges remain, including the lack of fine-grained representations, the interpretability of the reasoning procedure, and the trade-off between true-positive rate and false-positive rate. To tackle these challenges, in this work, we present a novel pulmonary nodule classification framework via attentive and ensemble 3D Dual Path Networks. Specially, we first devise a contextual attention mechanism to model the contextual correlations among adjacent locations, which improves the representativeness of deep features. Second, we employ a spatial attention mechanism to automatically locate the regions essential for nodule classification. Finally, we employ an ensemble of several models to improve the prediction robustness. Extensive experiments are conducted on the LIDC-IDRI database. Results demonstrate the effectiveness of the proposed techniques and the superiority of our model over previous state-of-the-art."
"AuNPs modified, disposable, ITO based biosensor: Early diagnosis of heat shock protein 70","This paper describes a novel, simple, and disposable immunosensor based on indium-tin oxide (ITO) sheets modified with gold nanoparticles to sensitively analyze heat shock protein 70 (HSP70), a potential biomarker that could be evaluated in diagnosis of some carcinomas. Disposable ITO coated Polyethylene terephthalate (PET) electrodes were used and modified with gold nanoparticles in order to construct the biosensors. Optimization and characterization steps were analyzed by electrochemical techniques such as electrochemical impedance spectroscopy (EIS) and cyclic voltammetry (CV). Surface morphology of the biosensor was also identified by electrochemical methods, scanning electron microscopy (SEM), and atomic force microscopy (AFM). To interpret binding characterization of HSP70 to anti-HSP70 single frequency impedance method was successfully operated. Moreover, the proposed HSP70 immunosensor acquired good stability, repeatability, and reproducibility. Ultimately, proposed biosensor was introduced to real human serum samples to determine HSP70 sensitively and accurately."
Automated grading of breast cancer histopathology using cascaded ensemble with combination of multi-level image features,"We present a novel image-analysis based method for automatically distinguishing low, intermediate, and high grades of breast cancer in digitized histopathology. A multiple level feature set, including pixel-, object-, and semantic-level features derived from convolutional neural networks (CNN), is extracted from 106 hematoxylin and eosin stained breast biopsy tissue studies from 106 women patients. These multi-level features allow not only characterization of cancer morphology, but also extraction of structural and interpretable information within the histopathological images. In this study, an improved hybrid active contour model based segmentation method was used to segment nuclei from the images. The semantic-level features were extracted by a CNN approach, which described the proportions of nuclei belonging to the different grades, in conjunction with pixel-level (texture) and object-level (architecture) features, to create an integrated set of image attributes that can potentially outperform either subtype of features individually. We utilized a cascaded approach to train multiple support vector machine (SVM) classifiers using combinations of feature subtypes to enable the possibility of maximizing the performance by leveraging different feature sets extracted from multiple levels. Final class (cancer grade) was determined by combining the scores produced by the individual SVM classifiers. By employing a light (three-layer) CNN model and parallel computing, the presented approach is computationally efficient and applicable to large-scale datasets. The method achieved an accuracy of 0.92 for low versus high, 0.77 for low versus intermediate, and 0.76 for intermediate versus high, and an overall accuracy of 0.69 when discriminating low, intermediate, and high grades of histopathological breast cancer images. This suggested that our grading method could be useful in developing a computational diagnostic tool for differentiating breast cancer grades, which might enable objective and reproducible alternative for diagnosis."
Automated grouping of medical codes via multiview banded spectral clustering,"Objective With its increasingly widespread adoption, electronic health records (EHR) have enabled phenotypic information extraction at an unprecedented granularity and scale. However, often a medical concept (e.g. diagnosis, prescription, symptom) is described in various synonyms across different EHR systems, hindering data integration for signal enhancement and complicating dimensionality reduction for knowledge discovery. Despite existing ontologies and hierarchies, tremendous human effort is needed for curation and maintenance – a process that is both unscalable and susceptible to subjective biases. This paper aims to develop a data-driven approach to automate grouping medical terms into clinically relevant concepts by combining multiple up-to-date data sources in an unbiased manner. Methods We present a novel data-driven grouping approach – multi-view banded spectral clustering (mvBSC) combining summary data from multiple healthcare systems. The proposed method consists of a banding step that leverages the prior knowledge from the existing coding hierarchy, and a combining step that performs spectral clustering on an optimally weighted matrix. Results We apply the proposed method to group ICD-9 and ICD-10-CM codes together by integrating data from two healthcare systems. We show grouping results and hierarchies for 13 representative disease categories. Individual grouping qualities were evaluated using normalized mutual information, adjusted Rand index, and F1-measure, and were found to consistently exhibit great similarity to the existing manual grouping counterpart. The resulting ICD groupings also enjoy comparable interpretability and are well aligned with the current ICD hierarchy. Conclusion The proposed approach, by systematically leveraging multiple data sources, is able to overcome bias while maximizing consensus to achieve generalizability. It has the advantage of being efficient, scalable, and adaptive to the evolving human knowledge reflected in the data, showing a significant step toward automating medical knowledge integration."
Automated interpretable detection of myocardial infarction fusing energy entropy and morphological features,"Background and objective The 12 leads electrocardiogram (ECG) is an effective tool to diagnose myocardial infarction (MI) on account of its inexpensive, noninvasive and convenient. Many methodologies have been widely adopted to detect it. However, much existing method did not integrate with diagnostic logic of clinician and practical application. The aim of the paper is to provide an automated interpretable detection method of myocardial infarction. Methods The paper presents a novel method fusing energy entropy and morphological features for MI detection via 12 leads ECG. Specifically, ECG signals are firstly decomposed by maximal overlap discrete wavelet packet transform (MODWPT), then energy entropy is calculated from the decomposed coefficients as global features. Area, kurtosis coefficient, skewness coefficient and standard deviation extracted from QRS wave and ST-T segment of ECG beat are computed as local morphological features. Combining global features based on record and local features based on beat for single lead, all the 12 leads features are fused as the ultimate feature vector. What's more, different methods including principal component analysis (PCA), linear discriminant analysis (LDA) and locality preserving projection (LPP) are employed to reduce the computational complexity and redundant information. Meanwhile, principal component features are ranked by F-value. To evaluate the proposed method, PTB (Physikalisch-Technische Bundesanstalt) database and inter-patient paradigm are employed. Results Compared with different algorithms, support vector machine (SVM) using radial basis kernel function combined with 10-fold cross validation achieves the best average performance with accuracy of 99.81%, sensitivity of 99.56%, precision of 99.74% and F1 of 99.70% based on 18 features in the intra-patient paradigm. By contrast, the accuracy is 92.69% with only 22 features for the inter-patient paradigm. Conclusions The experimental results present a superior performance compared to the state-of-the-art method. Meanwhile, above approach has the characteristic of interpretability according with diagnostic logic and strategy of clinician and specific change of ECG for MI."
Automated interpretation of congenital heart disease from multi-view echocardiograms,"Congenital heart disease (CHD) is the most common birth defect and the leading cause of neonate death in China. Clinical diagnosis can be based on the selected 2D key-frames from five views. Limited by the availability of multi-view data, most methods have to rely on the insufficient single view analysis. This study proposes to automatically analyze the multi-view echocardiograms with a practical end-to-end framework. We collect the five-view echocardiograms video records of 1308 subjects (including normal controls, ventricular septal defect (VSD) patients and atrial septal defect (ASD) patients) with both disease labels and standard-view key-frame labels. Depthwise separable convolution-based multi-channel networks are adopted to largely reduce the network parameters. We also approach the imbalanced class problem by augmenting the positive training samples. Our 2D key-frame model can diagnose CHD or negative samples with an accuracy of 95.4%, and in negative, VSD or ASD classification with an accuracy of 92.3%. To further alleviate the work of key-frame selection in real-world implementation, we propose an adaptive soft attention scheme to directly explore the raw video data. Four kinds of neural aggregation methods are systematically investigated to fuse the information of an arbitrary number of frames in a video. Moreover, with a view detection module, the system can work without the view records. Our video-based model can diagnose with an accuracy of 93.9% (binary classification), and 92.1% (3-class classification) in a collected 2D video testing set, which does not need key-frame selection and view annotation in testing. The detailed ablation study and the interpretability analysis are provided. The presented model has high diagnostic rates for VSD and ASD that can be potentially applied to the clinical practice in the future. The short-term automated machine learning process can partially replace and promote the long-term professional training of primary doctors, improving the primary diagnosis rate of CHD in China, and laying the foundation for early diagnosis and timely treatment of children with CHD."
Automated ontology generation framework powered by linked biomedical ontologies for disease-drug domain,"Objective and background: The exponential growth of the unstructured data available in biomedical literature, and Electronic Health Record (EHR), requires powerful novel technologies and architectures to unlock the information hidden in the unstructured data. The success of smart healthcare applications such as clinical decision support systems, disease diagnosis systems, and healthcare management systems depends on knowledge that is understandable by machines to interpret and infer new knowledge from it. In this regard, ontological data models are expected to play a vital role to organize, integrate, and make informative inferences with the knowledge implicit in that unstructured data and represent the resultant knowledge in a form that machines can understand. However, constructing such models is challenging because they demand intensive labor, domain experts, and ontology engineers. Such requirements impose a limit on the scale or scope of ontological data models. We present a framework that will allow mitigating the time-intensity to build ontologies and achieve machine interoperability. Methods: Empowered by linked biomedical ontologies, our proposed novel Automated Ontology Generation Framework consists of five major modules: a) Text Processing using compute on demand approach. b) Medical Semantic Annotation using N-Gram, ontology linking and classification algorithms, c) Relation Extraction using graph method and Syntactic Patterns, d), Semantic Enrichment using RDF mining, e) Domain Inference Engine to build the formal ontology. Results: Quantitative evaluations show 84.78% recall, 53.35% precision, and 67.70% F-measure in terms of disease-drug concepts identification; 85.51% recall, 69.61% precision, and F-measure 76.74% with respect to taxonomic relation extraction; and 77.20% recall, 40.10% precision, and F-measure 52.78% with respect to biomedical non-taxonomic relation extraction. Conclusion: We present an automated ontology generation framework that is empowered by Linked Biomedical Ontologies. This framework integrates various natural language processing, semantic enrichment, syntactic pattern, and graph algorithm based techniques. Moreover, it shows that using Linked Biomedical Ontologies enables a promising solution to the problem of automating the process of disease-drug ontology generation."
Automatic epileptic EEG detection using convolutional neural network with improvements in time-domain,"Epilepsy is a neurological disorder, and clinicians usually diagnose epilepsy by interpreting electroencephalogram (EEG) manually. This paper proposes a novel automatic epileptic EEG detection method based on convolutional neural network (CNN) with two innovative improvements and treats this task as a big data classification issue. Due to that CNN could extract and learn features automatically, the multi-channels time-series EEG recordings extracted by a sliding window are fed into the CNN model. Firstly, a 12-layers CNN is designed as the baseline epileptic EEG classification model. Afterward, the merger of the increasing and decreasing sequences (MIDS) is introduced to highlight the characteristic of waveforms. Then, a data augmentation method, Wasserstein Generative Adversarial Nets (WGANs), increases the sample diversity as well as EEG information. In this experiment, the recordings are from CHB-MIT Scalp EEG database, and the patient-cross performance with the train set from other patients and test set from the withheld patient is evaluated. The epileptic EEG classification results show that the original CNN achieves 70.68% sensitivity and 92.30% specificity, while CNN with MIDS and data augmentation yield 74.08% sensitivity, 92.46% specificity and 72.11% sensitivity, 95.89% specificity respectively. These two novel improvements both increased automatic epileptic EEG classification performance. Furthermore, in seizure onset detection, 90.57% seizure events are detected with the mean latency 4.68s using probability smoothing. The proposed method could lighten the EEG interpretation workload of clinicians effectively, and has great significance in auxiliary diagnosis of epilepsy."
Automatic equipment fault fingerprint extraction for the fault diagnostic on the batch process data,"Equipment condition monitoring in semiconductor manufacturing requires prompt, accurate, and sensitive detection and classification of equipment and process faults. Efficient and effective fault diagnostic is essential to minimizing scrapped wafers, reducing unscheduled equipment downtime, and consequently maintaining high production throughput and product yields. Through analyzing the equipment sensor signals as the batch process data, i.e., process timestamp×sensor×wafer, this paper firstly applies the well-known Support Vector Machine (SVM) classifier to detect the abnormal observations. In the second stage, the normal process dynamics are decomposed into different clusters by K-Means clustering. Each part of the process dynamics is further modelled by Principal Component Analysis (PCA). Fault fingerprints then can be extracted by consolidating the out of control scenarios after projecting the abnormal observations into the PCA models. An empirical study is conducted in collaboration with a local IC maker in France to validate the methodology. The result shows that the proposed approach can effectively detect abnormal observations as well as automatically classify the proper fault fingerprints to give evident guidelines in explaining the known faults."
Automatic myocardial infarction detection in contrast echocardiography based on polar residual network,"Purpose Heart disease is one of the leading causes of death. Among patients with cardiovascular diseases, myocardial infarction (MI) is the main reason. Precise and timely identification of MI is significant for early treatment. Myocardial contrast echocardiography (MCE) is widely used for the detection of MI in clinic practice. However, existing clinical exam using MCE is subjective and highly operator dependent and time-consuming. Hence an automatic computer-aided MI detection in MCE is necessary to improve the diagnosis performance and decrease the workload of clinicians. Methods In this study, a novel deep learning model, polar residual network (PResNet) is proposed to identify MI regions in MCE images which design a polar layer considering the ring shape of the myocardium. MCE images are fed into the PResNet and a newly defined polar layer is used to describe the myocardium with a ring shape. The whole polar images are evenly divided into several subsections and a residual network is improved to classify the subsection into normal and abnormal categories. Finally, the detection results are mapped back to the original image to illustrate the infarction regions’ locations for the further process. Results To evaluate the proposed PResNet, a dataset is constructed via performing MCE on five mice, which underwent the left anterior descending artery ligation and receive erythropoietin or saline injection, and the area variation fraction is manually annotated by an experienced expert as golden standards. The results demonstrate that the proposed PResNet model accomplishes high classification precisions with 99.6% and 98.7%, and 0.999 and 0.996 of AUC (area under the receiver operator curve) values on two different testing sets, respectively. Results suggest that the proposed model could enable accurate infarct detection and diagnosis of the MCE images. Conclusion Those efficiency gains highlight the powerful ability to describe and interpret the MCE images using the polar layer and residual network. The proposed PResNet might aid the clinicians in fast and accurate assessing the infarcted myocardium on MCE."
Autonomous Learning Multiple-Model zero-order classifier for heart sound classification,"This paper proposes a new extended zero-order Autonomous Learning Multiple-Model (ALMMo-0*) neuro-fuzzy approach in order to classify different heart disorders through sounds. ALMMo-0* is build upon the recently introduced ALMMo-0. In this paper ALMMo-0 is extended by adding a pre-processing structure which improves the performance of the proposed method. ALMMo-0* has as a learning engine composed of hierarchical a massively parallel set of 0-order fuzzy rules, which are able to self-adapt and provide transparent and human understandable IF ... THEN representation. The heart sound recordings considered in the analysis were sourced from several contributors around the world. Data were collected from both clinical and nonclinical environment, and from healthy and pathological patients. Differently from mainstream machine learning approaches, ALMMo-0* is able to learn from unseen data. The main goal of the proposed method is to provide highly accurate models with high transparency, interpretability, and explainability for heart disorder diagnosis. Experiments demonstrated that the proposed neuro-fuzzy-based modeling is an efficient framework for these challenging classification tasks surpassing its state-of-the-art competitors in terms of classification accuracy. Additionally, ALMMo-0* produced transparent AnYa type fuzzy rules, which are human interpretable, and may help specialists to provide more accurate diagnosis. Medical doctors can easily identify abnormal heart sounds by comparing a patient’s sample with the identified prototypes from abnormal samples by ALMMo-0*."
Bayesian method for HVAC plant sensor fault detection and diagnosis,"Together with the thermo-physical relationships among the flow rates and temperatures of water in a piping system, the Bayesian method was employed to develop a model for detection and evaluation of biases of water flow and temperature sensors in a central chiller plant. The model can handle biases of multiple sensors occurring simultaneously and can remain functional when the coverage of the available measurements is incomplete. A series of case studies was done to verify the performance of the model and for comparison with the conventional method that is based solely on the thermo-physical relationships. The cases studied involved the use of synthetic plant operating data and actual operating records of an existing chiller plant. In this paper, the theoretical basis of the model is outlined, and explanations are given for the superior performance of the Bayesian method in handling cases with data that cannot fully cover the required range of operating chiller patterns. Results of the cases unveiled the effects of the prior belief with or without being updated during the estimation process, and of biases occurring in steps at the same time and at different times, as well as those that would increase with time. Furthermore, the case studies showed that the Bayesian method was able to detect sensor biases of a magnitude of ± 0.5 °C or lower."
Behavioural Approach to Network Anomaly Detection for Resource-Constrained System – Presentation of the Novel Solution – Preliminary Study,"In this paper some preliminary results of network anomaly detection for resource-constrained systems have been presented. All described algorithms are embedded into a three-tier system application. The presented solution is a response to nowadays trends in computer/network security domain that indicate gradual shifting from signature-based anomaly detection typically based on some selected statistical measures toward systems that are based on behavioural analysis of computer systems and/or networks. The presented system requirements, explain relevant terminology as well as we include some selected results and their analysis. The solution is potentially destined for very resource-constrained systems (IoT specifically), all tests were carried out in an example network to additionally analyse their effect of the presented approach on the computer resources."
Bias of Inaccurate Disease Mentions in Electronic Health Record-based Phenotyping,"Objectives Electronic health record (EHR)-based phenotyping is an automated technique for identifying patients diagnosed with a particular disease using EHR data. However, EHR-based phenotyping has difficulties in achieving satisfactorily high performance because clinical notes include disease mentions that ultimately signify something other than the patient’s diagnosis (such as differential diagnosis or screening). Our objective is to quantify the influence of such disease mentions on EHR-based phenotyping performance. Methods Physicians manually reviewed whether the disease mentions indicated the patients’ diseases in 487,300 clinical notes of 4,430 patients. Particular focus was placed on disease mentions that did not signify the patient’s diagnosis even though they did not have any syntactic modifier or indicator in the same sentences. Patients were then classified according to whether their clinical notes included such disease mentions. Results Among the patients whose clinical notes included disease mentions without any modifier or indicator, the proportion of patients whose disease mentions signified the patients’ diagnosis was 78.1% (on average). This value can be interpreted as the bias of disease mentions that did not signify the patient’s diagnosis on the precision of EHR-based phenotyping by extracting disease mentions from clinical notes. Conclusion This study quantified the bias occurred owing to disease mentions that incorrectly signify a patient’s diagnosis in the value of precision of EHR-based phenotyping from four dataset types. The results of this study will help researchers in diverse research environments with different available data types."
Bimodal classification algorithm for atrial fibrillation detection from m-health ECG recordings,"Introduction Atrial Fibrillation (AF) is the most common cardiac arrhythmia, presenting a significant independent risk factor for stroke and thromboembolism. With the emergence of m-Health devices, the importance of automatic detection of AF in an off-clinic setting is growing. This study demonstrates the performance of a bimodal classifier for distinguishing AF from sinus rhythm (SR) that could be used for automated detection of AF episodes. Methods Surface recordings from a hand-held research device and standard electrocardiograms (ECG) were collected and analyzed from 68 subjects. An additional 48 subjects from the MIT-BIH Arrythmia Database were also analyzed. All ECGs were blindly reviewed by physicians independently of the bimodal algorithm analysis. The algorithm selects an artifact-free 6-s ECG segment out of a 20-s long recording and computes a spectral Frequency Dispersion Metric (FDM) and a temporal R-R interval variability (VRR) index. Results Scatter plots of the VRR and FDM indices revealed two distinct clusters. The bimodal scattering of the indices revealed a linear classification boundary that could be employed to differentiate the SR from AF waveforms. The selected classification boundary was able to correctly differentiate all the subjects from both datasets into either SR or AF groups, except for 3 SR subjects from the MIT-BIH dataset. Conclusion Our bimodal classification algorithm was demonstrated to successfully acquire, analyze and interpret ECGs for the presence of AF indicating its potential to support m-Health diagnosis, monitoring, and management of therapy in AF patients."
Biosensing enhancement of dengue virus using microballoon mixers on centrifugal microfluidic platforms,"Dengue is the current leading cause of death among children in several Latin American and Asian countries. Due to poverty in areas where the disease is prevalent and the high cost of conventional diagnostic systems, low cost devices are needed to reduce the burden caused by dengue infection. Centrifugal microfluidic platforms are an alternative solution to reduce costs and increase the availability of a rapid diagnostic system. The rate of chemical reactions in such devices often depends on the efficiency of the mixing techniques employed in their microfluidic networks. This paper introduces a micromixer that operates by the expansion and contraction of a microballoon to produce a consistent periodical 3D reciprocating flow. We established that microballoons reduced mixing time of 12μl liquids from 170min, for diffusional mixing, to less than 23s. We have also tested the effect of the microballoon mixers on the detection of the dengue virus. The results indicate that employing a microballoon mixer enhances the detection sensitivity of the dengue virus by nearly one order of magnitude compared to the conventional ELISA method."
Bootstrap Confidence Interval on IOHMM Parameters for System Health Diagnostic Under Multiple Operating Conditions,The operating conditions have an important impact on system degradation. This paper uses the Input-Output Hidden Markov Model to represent the system degradation having multiple operating conditions. In this paper the bootstrap method is applied to estimate the model parameters and to diagnostic the system health. Parameters of the model are computed with 95% confidence intervals. The uncertainty about multiple data sequences and degradation speed is handled according to the operating conditions. A numerical application is given to explain the methodologies used to estimate the model parameters and diagnostic the system health.
BWR water chemistry guidelines and PWR primary water chemistry guidelines in Japan – Purpose and technical background,"After 40years of light water reactor (LWR) operations in Japan, the sustainable development of water chemistry technologies has aimed to ensure the highest coolant system component integrity and fuel reliability performance for maintaining LWRs in the world; additionally, it aimed to achieve an excellent dose rate reduction. Although reasonable control and diagnostic parameters are utilized by each boiling water reactor (BWR) and pressurized water reactor (PWR) owner, it is recognized that specific values are not shared among everyone involved. To ensure the reliability of BWR and PWR operation and maintenance, relevant members of the Atomic Energy Society of Japan (AESJ) decided to establish guidelines for water chemistry. The Japanese BWR and PWR water chemistry guidelines provide strategies to improve material and fuel reliability performance as well as to reduce dosing rates. The guidelines also provide reasonable “control values”, “diagnostic values” and “action levels” for multiple parameters, and they stipulate responses when these levels are exceeded. Specifically, “conditioning parameters” are adopted in the Japanese PWR primary water chemistry guidelines. Good practices for operational conditions are also discussed with reference to long-term experience. This paper presents the purpose, technical background and framework of the preliminary water chemistry guidelines for Japanese BWRs and PWRs. It is expected that the guidelines will be helpful as an introduction to achieve safety and reliability during operations."
Can online product reviews be more helpful? Examining characteristics of information content by product type,"Many online retailers and other product-oriented websites allow people to post product reviews for use by shoppers. While research indicates that these reviews influence consumers' shopping attitudes and behaviors, questions remain about how consumers evaluate the product reviews themselves. With the current research, we introduce a new methodology for identifying the review factors that shoppers use to evaluate review helpfulness, and we integrate prior literature to provide a framework that explains how these factors reflect readers' general concerns about the diagnosticity (uncertainty and equivocality) and credibility (trust and expertise) of electronic word-of-mouth. Based on this framework, we offer predictions about how the relative importance of diagnosticity and credibility should vary systematically across search and experience product types. By analyzing secondary data consisting of over 8000 helpfulness ratings from product reviews posted by shoppers on Amazon.com, we find that, while review content affects helpfulness in complex ways, these effects are well explained by the proposed framework. Interestingly, the data suggest that review writers who explicitly attempt to enhance review diagnosticity or credibility are often ineffective or systematically unhelpful. Our findings have implications for both IS developers and retailers for designing online decision support systems to optimize communication practices and better manage consumer-generated content and interactions among consumers."
Cancerome: A hidden informative subnetwork of the diseasome,"Neoplastic disorders are a leading cause of mortality and morbidity worldwide. Studying the relationships between different cancers using high throughput-generated data may elucidate undisclosed aspects of cancer etiology, diagnosis, and treatment. Several studies have described relationships between different diseases based on genes, proteins, pathways, gene ontology, comorbidity, symptoms, and other features. In this study, we first constructed an integrated human disease network based on nine different biological aspects, including molecular, functional, and clinical features. Next, we extracted the cancerome as a cancer-related subnetwork. Further investigation of cancerome could reveal hidden mechanisms of cancer and could be useful in developing new diagnostic tests and effective new drugs."
Cardiac arrhythmia classification using tunable Q-wavelet transform based features and support vector machine classifier,"Electrocardiogram (ECG) is a non-invasive clinical tool that reveals the rhythm and functionality of the human heart. It is widely used in the diagnosis of heart diseases including arrhythmia. Abnormal heart rhythms are collectively known as arrhythmia which can be recognized and classified into different types. Arrhythmia classification techniques provide automated ECG analysis in cardiac patient monitoring devices. It helps cardiologists to interpret the ECG signal for diagnosis. In this context, this paper reports a novel and efficient ECG beats classification technique for normal and seven arrhythmia types. The proposed technique utilizes tunable Q-wavelet based features of ECG beats which are acquired from different ECG records of the Massachusetts Institute of Technology-Beth Israel Hospital (MIT-BIH) arrhythmia database. For feature extraction, each ECG beat is decomposed up to the sixth level of the tunable Q-wavelet transform. Approximate coefficients at the sixth level are selected as features of each ECG beats. For classification, features of 14,878 ECG beats are utilized for training of the support vector machine classifier while 26,219 ECG beats are used for the testing purpose. The average accuracy, sensitivity, and specificity offered by the proposed classifier for eight different classes of ECG beats are 99.27%, 96.22%, and 99.58% respectively. The proposed classifier outperforms many recent techniques developed in this field."
Cartesian genetic programming for diagnosis of Parkinson disease through handwriting analysis: Performance vs. interpretability issues,"In the last decades, early disease identification through non-invasive and automatic methodologies has gathered increasing interest from the scientific community. Among others, Parkinson's disease (PD) has received special attention in that it is a severe and progressive neuro-degenerative disease. As a consequence, early diagnosis would provide more effective and prompt care strategies, that cloud successfully influence patients’ life expectancy. However, the most performing systems implement the so called black-box approach, which do not provide explicit rules to reach a decision. This lack of interpretability, has hampered the acceptance of those systems by clinicians and their deployment on the field. In this context, we perform a thorough comparison of different machine learning (ML) techniques, whose classification results are characterized by different levels of interpretability. Such techniques were applied for automatically identify PD patients through the analysis of handwriting and drawing samples. Results analysis shows that white-box approaches, such as Cartesian Genetic Programming and Decision Tree, allow to reach a twofold goal: support the diagnosis of PD and obtain explicit classification models, on which only a subset of features (related to specific tasks) were identified and exploited for classification. Obtained classification models provide important insights for the design of non-invasive, inexpensive and easy to administer diagnostic protocols. Comparison of different ML approaches (in terms of both accuracy and interpretability) has been performed on the features extracted from the handwriting and drawing samples included in the publicly available PaHaW and NewHandPD datasets. The experimental findings show that the Cartesian Genetic Programming outperforms the white-box methods in accuracy and the black-box ones in interpretability."
Cascade knowledge diffusion network for skin lesion diagnosis and segmentation,"Accurate diagnosis and segmentation of skin lesion is critical for early detection and diagnosis of skin cancer. Recent multi-task learning methods require expensive annotations for skin lesion analysis while single-task driven models cannot fully utilize the potential knowledge. The aim of this study is to utilize the neglected knowledge by a flexible architecture in dermoscopy skin lesion classification and segmentation. In this work, we propose a cascade knowledge diffusion network (CKDNet) to transfer and aggregate knowledge learnt from different tasks to simultaneously boost the performances of classification and segmentation. CKDNet consists of a sequence of coarse-level segmentation, classification, and fine-level segmentation networks. We design two novel feature entanglement modules, Entangle-Cls and Entangle-Seg, for classification and segmentation. The Entangle-Cls module aggregates the diffused features from initial segmentation to drive the classification network’s attention to image regions relevant to the disease. The Entangle-Seg module integrates the cascaded context knowledge learnt from classification to benefit fine-level segmentation, especially at uncertain boundaries. The entanglement modules can adaptively control the knowledge that can be diffused from one task to another, which avoids the empirical selection of weights for different learning tasks compared to other multi-task methods. We perform extensive evaluations and comparisons with state-of-the-art methods on skin lesion classification and segmentation with challenge datasets, ISIC2017 and ISIC2018. Our CKDNet demonstrated superior performance without using any ensemble approaches or any external datasets. The effectiveness of each component and loss functions are demonstrated by interpretable results using class activation maps (CAM), t-SNE, and classification and segmentation results."
Causal Modelling for Predicting Machine Tools Degradation in High Speed Production Process⁎⁎This project has received funding from the European Union’s Horizon 2020 research named Z-BRE4K and innovation program under grant agreement No 768869.,"A dynamic health indicator based on regressive event-tracker algorithm is proposed to accurately interpret the condition of critical components of machine tools in a production system and to predict their potential sudden breakdown based on future trends. Through sensors/actuators data acquisition, the algorithm predicts the causal links between various monitored parameters of the system and offers a diagnosis of the health state of the system. A safety and operational robustness regime determines the acceptable thresholds of the operational boundaries of the electro-mechanical components of the machines. The proposed model takes into account the possibilities of sensor values being a piecewise-linear models or a pair of exponential functions with restricted model parameters, which can predict the runs-to-failure or remaining useful life until a safety threshold. The events caused by sensors passing through sub levels of safety threshold are used as a re-enforcement learning for the models. Each remaining useful life estimation diagnosis and prognosis analysis can be conducted on individual or an interconnected network of components within a machine. The overall health indicator based on individual useful life estimation is calculated by deriving the weights from event-clustering algorithm. The work can be extended to a network of machines representing a process. The outcome of the continuously learning real-time condition monitoring modus-operandi is to accurately measure the remaining useful life of the network of critical components of a machine."
Change detection based on tensor RPCA for longitudinal retinal fundus images,"Change detection of longitudinal fundus images is an important problem in computer aided diagnosis system (CAD). Detecting regions of change in multiple fundus images from the same eye is seldom developed in the literature due to the complication and interpretability. This paper presents a longitudinal change detection framework based on tensor robust principal component analysis (RPCA) for a long retinal fundus image serial. The proposed method chooses an image of the best condition in serial as the background, then models each image as a slice of tensor, utilizes total variation to constraint the temporal continuity of change regions, finally obtains the change regions by Tucker decomposition and alternating direction method of multipliers (ADMM). Comparing with the method based on matrix RPCA, tensor RPCA preserves the original spatial structure of each image, imposes the temporal continuity on change regions and models the background by patches to avoid the little disturbance of blood vessels. Results on a real fundus image serial are presented and show the effectiveness of the proposed algorithm."
Changes in life expectancy for cancer patients over time since diagnosis,"The aims of this study were to provide life expectancy (LE) estimates of cancer patients at diagnosis and LE changes over time since diagnosis to describe the impact of cancer during patients' entire lives. Cancer patients' LE was calculated by standard period life table methodology using the relative survival of Italian patients diagnosed in population-based cancer registries in 1985–2011 with follow-up to 2013. Data were smoothed using a polynomial model and years of life lost (YLL) were calculated as the difference between patients' LE and that of the age- and sex-matched general population. The YLL at diagnosis was highest at the youngest age at diagnosis, steadily decreasing thereafter. For patients diagnosed at age 45 years, the YLL was above 20 for lung and ovarian cancers and below 6 for thyroid cancer in women and melanoma in men. LE progressively increased in patients surviving the first years, decreasing thereafter, to approach that of the general population. YLL in the long run mainly depends on attained age. Providing quantitative data is essential to better define clinical follow-up and plan health care resource allocation. These results help assess when the excess risk of death from tumour becomes negligible in cancer survivors."
Characteristics and correlation of nozzle internal flow and jet breakup under flash boiling conditions,"Flash boiling sprays utilize superheated fluid to enhance spray breakup via eruption of flash boiling bubbles near the nozzle exit. Extensive efforts have been made to interpret the underlying complex phase change physics associated with flash boiling sprays. However, the dynamic interaction between the gas phase and liquid phase of the flash boiling sprays has not been adequately investigated yet. This work adopts a two-dimensional optical transparent nozzle to study in-nozzle multiphase flow characteristics as well as spray characteristics outside of the nozzle. Both high-speed and low-speed measurements were carried out using optical diagnostic methods, and flash boiling sprays at different superheat levels were studied. With the experiments, the correlation between the internal flow and spray liquid jet breakup is established and the impact of the gas-liquid correlation on the properties of flash boiling sprays is presented. Furthermore, dynamic interaction between the gas phase and liquid phase in the nozzle and out of the nozzle is analyzed with a center of mass scheme. It is found that the dynamic features of the flash boiling sprays are closely connected with the dynamics of the in-nozzle flow. Such observation suggests that modifying flash boiling bubble characteristics can potentially be utilized to actively control flash boiling sprays for improved spray performance."
Characterization of the hysteresis cycle in a two-stage liquid-fueled swirled burner through numerical simulation,"Lean Premixed Prevaporized combustors are today an interesting alternative to more classical configurations to reduce pollutant emissions. Still they may also give rise to strong flame dynamics and harmful combustion instabilities. In order to improve our control on such systems, multi-stage multi-injection burners offer additional degrees of freedom in the fuel distribution and thus in the combustion regime. The BIMER combustor has been expressly developed by EM2C to investigate this solution at a laboratory scale. It is composed of two swirling stages: the pilot stage, in which liquid fuel is injected through a pressure-swirl atomizer; and the multipoint stage, in which the fuel is injected through 10 holes in a jet-in-crossflow configuration. Successive experimental campaigns demonstrated the key-role played by fuel distribution in the two stages and clearly showed the existence of an hysteresis cycle: several flame stabilization archetypes can exist for the same fuel distribution, depending on the flow, spray and flame history. In-depth analysis through numerous experimental diagnostics permitted to elaborate several scenarii to explain this complex behavior. In the present study, large eddy simulations are carried out with the AVBP code to complement the experimental data in our understanding of the burner stabilization processes. Simulations from full pilot to full multipoint injections are performed, exhibiting the hysteresis cycle observed in the experiments. An original tri-stable point was also encountered for full pilot injection, with three possible flame shapes. This tri-stable point is investigated and the impact of fuel staging for a given flame shape is analyzed. The two bifurcations observed experimentally are finally presented and analyzed along with their inherent mechanisms."
Characterization of wall temperature distributions in a gas turbine model combustor measured by 2D phosphor thermometry,"Instantaneous 2D phosphor thermometry was performed on the windows and combustion chamber posts of an optically accessible dual-swirl gas turbine model combustor operated with CH4 at atmospheric pressure. Two phosphors were used in order to measure different surface temperature ranges: YAP:Eu (temperature range 850 K – 1150 K) and YAG:Eu (temperature range 1100 K – 1300 K). Phosphor coatings were applied to the combustion chamber post and to three vertically oriented stripes on one combustion chamber window. Using interpolation, this allowed the determination of the surface temperature of the complete combustion chamber window. Heat losses across the combustion chamber window were determined by measuring the surface temperature on the inner and outer surface of the window. The phosphor coating was illuminated using the fourth harmonic of a Nd:YAG laser, which was formed into a broad light sheet. The spatially resolved, temperature-dependent decay rate of the phosphorescence was measured with a high speed CMOS camera. Three flames with similar flow fields and thermal powers between 22.5 kW and 30 kW and equivalence ratios between 0.63 and 0.83 were studied. Because the flame characteristics including flow field, heat release and temperature distributions were known from previous measurements the interaction between the flame and the surface temperature could be examined. In this way, the different wall temperatures of the flames could be explained."
ClariSense+: An enhanced traffic anomaly explanation service using social network feeds,"The explosive growth in social networks that publish real-time content begs the question of whether their feeds can complement traditional sensors to achieve augmented sensing capabilities. One such capability is to explain anomalous sensor readings. In our previous conference paper, we built an automated anomaly clarification service, called ClariSense, with the ability to explain sensor anomalies using social network feeds (from Twitter). In this extended work, we present an enhanced anomaly explanation system that augments our base algorithm by considering both (i) the credibility of social feeds and (ii) the spatial locality of detected anomalies. The work is geared specifically for describing small-footprint anomalies, such as vehicular traffic accidents. The original system used information gain to select more informative microblog items to explain physical sensor anomalies. In this paper, we show that significant improvements are achieved in our ability to explain small-footprint anomalies by accounting for information credibility and further discriminating among high-information-gain items according to the size of their spatial footprint. Hence, items that lack sufficient corroboration and items whose spatial footprint in the blogosphere is not specific to the approximate location of the physical anomaly receive less consideration. We briefly demonstrate the workings of such a system by considering a variety of real-world anomalous events, and comparing their causes, as identified by ClariSense+, to ground truth for validation. A more systematic evaluation of this work is done using vehicular traffic anomalies. Specifically, we consider real-time traffic flow feeds shared by the California traffic system. When flow anomalies are detected, our system automatically diagnoses their root cause by correlating the anomaly with feeds on Twitter. For evaluation purposes, the identified cause is then retroactively compared to official traffic and incident reports that we take as ground truth. Results show a great correspondence between our automatically selected explanations and ground-truth data."
ClariSense+: An enhanced traffic anomaly explanation service using social network feeds,"The explosive growth in social networks that publish real-time content begs the question of whether their feeds can complement traditional sensors to achieve augmented sensing capabilities. One such capability is to explain anomalous sensor readings. In our previous conference paper, we built an automated anomaly clarification service, called ClariSense, with the ability to explain sensor anomalies using social network feeds (from Twitter). In this extended work, we present an enhanced anomaly explanation system that augments our base algorithm by considering both (i) the credibility of social feeds and (ii) the spatial locality of detected anomalies. The work is geared specifically for describing small-footprint anomalies, such as vehicular traffic accidents. The original system used information gain to select more informative microblog items to explain physical sensor anomalies. In this paper, we show that significant improvements are achieved in our ability to explain small-footprint anomalies by accounting for information credibility and further discriminating among high-information-gain items according to the size of their spatial footprint. Hence, items that lack sufficient corroboration and items whose spatial footprint in the blogosphere is not specific to the approximate location of the physical anomaly receive less consideration. We briefly demonstrate the workings of such a system by considering a variety of real-world anomalous events, and comparing their causes, as identified by ClariSense+, to ground truth for validation. A more systematic evaluation of this work is done using vehicular traffic anomalies. Specifically, we consider real-time traffic flow feeds shared by the California traffic system. When flow anomalies are detected, our system automatically diagnoses their root cause by correlating the anomaly with feeds on Twitter. For evaluation purposes, the identified cause is then retroactively compared to official traffic and incident reports that we take as ground truth. Results show a great correspondence between our automatically selected explanations and ground-truth data."
Classification of auditory brainstem responses through symbolic pattern discovery,"Introduction Numeric time series are present in a very wide range of domains, including many branches of medicine. Data mining techniques have proved to be useful for knowledge discovery in this type of data and for supporting decision-making processes. Objectives The overall objective is to classify time series based on the discovery of frequent patterns. These patterns will be discovered in symbolic sequences obtained from the time series data by means of a temporal abstraction process. Methods Firstly, we transform numeric time series into symbolic time sequences, where the symbols aim to represent the relevant domain concepts. These symbols can be defined using either public or expert domain knowledge. Then we apply a symbolic pattern discovery technique to the output symbolic sequences. This technique identifies the subsequences frequently found in a population group. These subsequences (patterns) are representative of population groups. Finally, we employ a classification technique based on the identified patterns in order to classify new individuals. Thanks to the inclusion of domain knowledge, the classification results can be explained using domain terminology. This makes the results easier to interpret for the domain specialist (physician). Results This method has been applied to brainstem auditory evoked potentials (BAEPs) time series. Preliminary experiments were carried out to analyse several aspects of the method including the best configuration of the pattern discovery technique parameters. We then applied the method to the BAEPs of 83 individuals belonging to four classes (healthy, conductive hearing loss, vestibular schwannoma—brainstem involvement and vestibular schwannoma—8th-nerve involvement). According to the results of the cross-validation, overall accuracy was 99.4%, sensitivity (recall) was 97.6% and specificity was 100% (no false positives). Conclusion The proposed method effectively reduces dimensionality. Additionally, if the symbolic transformation includes the right domain knowledge, the method arguably outputs a data representation that denotes the relevant domain concepts more clearly. The method is capable of finding patterns in BAEPs time series and is very accurate at correctly predicting whether or not new patients have an auditory-related disorder."
Classification of Cervical Cancer Using Hybrid Multi-layered Perceptron Network Trained by Genetic Algorithm,"Cervical cancer is well known as the third killer for women in Malaysia. The precancerous stage for detection can be determine by screening test, known Pap smear test for avoiding occurrence of cervical cancer. The problem face in the test is the human error in reading the data analysis and also lack number of pathologists to interpret the data analysis. Creating a computer-aided diagnosis system is one of the solutions that can interpret the data. Most of the research available used artificial neural network for diagnosis system to classify the cervical cancer cells data into normal and abnormal. This research creates a neural network (NN) using Hybrid Multi-layered Perceptron (HMLP) trained by Genetic Algorithm (GA) to diagnose the data. The data is extracted from cervical cells and divided into four features as the input, which are size of nucleus, size of cytoplasm, grey level of nucleus, and grey level of cytoplasm. The data is interpreted into three categories; are normal, Low-grade Squamous Intraepithelial Lesion (LSIL) and High-grade Squamous Intraepithelial Lesion (HSIL). These categories will be inserted in to the algorithm to calculate and determined the neural network performance. The data is randomly separated into dataset using 5-fold cross validation technique. The performance is compared with Hybrid Radial Basis Function (HRBF) trained with Adaptive Fuzzy K-means and Moving K-means Clustering Algorithm. This researched shows HMLP trained with GA create a better performance of the network in the accuracy, sensitivity, and specificity to be implemented in the cervical cancer for test the performance improvement."
Classification of diabetes-related retinal diseases using a deep learning approach in optical coherence tomography,"Background and objectives: Spectral Domain Optical Coherence Tomography (SD-OCT) is a volumetric imaging technique that allows measuring patterns between layers such as small amounts of fluid. Since 2012, automatic medical image analysis performance has steadily increased through the use of deep learning models that automatically learn relevant features for specific tasks, instead of designing visual features manually. Nevertheless, providing insights and interpretation of the predictions made by the model is still a challenge. This paper describes a deep learning model able to detect medically interpretable information in relevant images from a volume to classify diabetes-related retinal diseases. Methods: This article presents a new deep learning model, OCT-NET, which is a customized convolutional neural network for processing scans extracted from optical coherence tomography volumes. OCT-NET is applied to the classification of three conditions seen in SD-OCT volumes. Additionally, the proposed model includes a feedback stage that highlights the areas of the scans to support the interpretation of the results. This information is potentially useful for a medical specialist while assessing the prediction produced by the model. Results: The proposed model was tested on the public SERI-CUHK and A2A SD-OCT data sets containing healthy, diabetic retinopathy, diabetic macular edema and age-related macular degeneration. The experimental evaluation shows that the proposed method outperforms conventional convolutional deep learning models from the state of the art reported on the SERI+CUHK and A2A SD-OCT data sets with a precision of 93% and an area under the ROC curve (AUC) of 0.99 respectively. Conclusions: The proposed method is able to classify the three studied retinal diseases with high accuracy. One advantage of the method is its ability to produce interpretable clinical information in the form of highlighting the regions of the image that most contribute to the classifier decision."
Classification of human cancer diseases by gene expression profiles,"A cancers disease in virtually any of its types presents a significant reason behind death surrounding the world. In cancer analysis, classification of varied tumor types is of the greatest importance. Microarray gene expressions datasets investigation has been seemed to provide a successful framework for revising tumor and genetic diseases. Despite the fact that standard machine learning ML strategies have effectively been valuable to realize significant genes and classify category type for new cases, regular limitations of DNA microarray data analysis, for example, the small size of an instance, an incredible feature number, yet reason for limitation its investigative, medical and logical uses. Extending the interpretability of expectation and forecast approaches while holding a great precision would help to analysis genes expression profiles information in DNA microarray dataset all the most reasonable and proficiently. This paper presents a new methodology based on the gene expression profiles to classify human cancer diseases. The proposed methodology combines both Information Gain (IG) and Standard Genetic Algorithm (SGA). It first uses Information Gain for feature selection, then uses Genetic Algorithm (GA) for feature reduction and finally uses Genetic Programming (GP) for cancer types’ classification. The suggested system is evaluated by classifying cancer diseases in seven cancer datasets and the results are compared with most latest approaches. The use of proposed system on cancers datasets matching with other machine learning methodologies shows that no classification technique commonly outperforms all the others, however, Genetic Algorithm improve the classification performance of other classifiers generally."
Cluster analysis-based anomaly detection in building automation systems,"Faults in heating, ventilation, and air-conditioning control networks substantially affect energy and comfort performance in commercial buildings. As these control networks are comprised of many sensors and actuators, it is challenging to identify, often subtle, anomalies caused by these faults. In this paper, we develop a cluster analysis method for anomaly detection. The proposed method consolidates the building automation system data into a small number of distinct patterns of operation. These distinct patterns help energy managers discover and interpret anomalies through visualization of these patterns. The method was demonstrated with a year’s worth of building automation system data from 247 thermal zones and an air handling unit. Anomalies associated with zone temperature and airflow control were identified in about one-third of these zones. At the air handling unit-level, we identified anomalies related with three different faults: the use of economizer mode with perimeter heating, and leaky outdoor and return air dampers. The use of economizer mode with perimeter heating affected 39% to 52% of the total operation period and caused the outdoor air damper to remain fully open and the heat recovery unit to remain off during most of the heating season."
"Clustering of Redundant Parameters for Fault Isolation with Gaussian Residuals⁎⁎The financial support from the Natural Sciences and Engineering Research Council of Canada (NSERC), the German Academic Exchange Service (DAAD) and the Mitacs Globalink Research Award is gratefully acknowledged.","Fault detection and isolation in stochastic systems is typically model-based, meaning fault-indicating residuals are generated based on measurements and compared to equivalent mathematical system models. The residuals often exhibit Gaussian properties or can be transformed into a standard Gaussian framework by means of the asymptotic local approach. The effectiveness of the fault diagnosis depends on the model quality, but an increasing number of model parameters also leads to redundancies which, in turn, can distort the fault isolation. This occurs, for example, in structural engineering, where residuals are generated by comparing structural vibrations to the output of digital twins. This article proposes a framework to find the optimal parameter clusters for such problems. It explains how the optimal solution is a compromise, because with an increasing number of clusters, the fault isolation resolution increases, but the detectability in each cluster decreases, and the number of false alarms changes. To assess these factors during the clustering process, criteria for the minimum detectable change and the false-alarm susceptibility are introduced and evaluated in an optimization scheme."
Coffee Leaf Disease Recognition Based on Deep Learning and Texture Attributes,"An automatic coffee plant disease recognition system is required since coffee is an important commodity in the world economy and its productivity and quality are affected by diseases such as Cercospora and Rust. This research aims to apply computational methods to recognize main diseases in coffee leaves, with the purpose to implement an expert system to assist coffee producers in disease diagnosis during its initial stages. Since these two diseases are shapeless, it inspires a texture attribute extraction approach for pattern recognition. Two texture attributes were considered in this work: statistical attributes and local binary patterns. The texture attribute vector were computed for a collection of images of coffee leaves and used as input to a feedforward neural network. The results were compared with the recognition rate of a convolutional neural network with deep learning applied directly to the same collection of images, without extraction of texture attributes. Surprisingly, this second approach showed better results than the texture extraction method. It could be explained by the small number of diseases we aimed to recognize and a sufficient number of training samples used during the deep learning process. The best Kappa coefficient obtained was 0.970, and sensitivity was 0.980."
Combining anatomical and functional networks for neuropathology identification: A case study on autism spectrum disorder,"While the prevalence of Autism Spectrum Disorder (ASD) is increasing, research continues in an effort to identify common etiological and pathophysiological bases. In this regard, modern machine learning and network science pave the way for a better understanding of the neuropathology and the development of diagnosis aid systems. The present work addresses the classification of neurotypical and ASD subjects by combining knowledge about both the structure and the functional activity of the brain. In particular, we model the brain structure as a graph, and the resting-state functional MRI (rs-fMRI) signals as values that live on the nodes of that graph. We then borrow tools from the emerging field of Graph Signal Processing (GSP) to build features related to the frequency content of these signals. In order to make these features highly discriminative, we apply an extension of the Fukunaga-Koontz transform. Finally, we use these new markers to train a decision tree, an interpretable classification scheme, which results in a final diagnosis aid model. Interestingly, the resulting decision tree outperforms state-of-the-art methods on the publicly available Autism Brain Imaging Data Exchange (ABIDE) collection. Moreover, the analysis of the predictive markers reveals the influence of the frontal and temporal lobes in the diagnosis of the disorder, which is in line with previous findings in the literature of neuroscience. Our results indicate that exploiting jointly structural and functional information of the brain can reveal important information about the complexity of the neuropathology."
Combining anatomical and functional networks for neuropathology identification: A case study on autism spectrum disorder,"While the prevalence of Autism Spectrum Disorder (ASD) is increasing, research continues in an effort to identify common etiological and pathophysiological bases. In this regard, modern machine learning and network science pave the way for a better understanding of the neuropathology and the development of diagnosis aid systems. The present work addresses the classification of neurotypical and ASD subjects by combining knowledge about both the structure and the functional activity of the brain. In particular, we model the brain structure as a graph, and the resting-state functional MRI (rs-fMRI) signals as values that live on the nodes of that graph. We then borrow tools from the emerging field of Graph Signal Processing (GSP) to build features related to the frequency content of these signals. In order to make these features highly discriminative, we apply an extension of the Fukunaga-Koontz transform. Finally, we use these new markers to train a decision tree, an interpretable classification scheme, which results in a final diagnosis aid model. Interestingly, the resulting decision tree outperforms state-of-the-art methods on the publicly available Autism Brain Imaging Data Exchange (ABIDE) collection. Moreover, the analysis of the predictive markers reveals the influence of the frontal and temporal lobes in the diagnosis of the disorder, which is in line with previous findings in the literature of neuroscience. Our results indicate that exploiting jointly structural and functional information of the brain can reveal important information about the complexity of the neuropathology."
Comparative analysis of Granger causality and transfer entropy to present a decision flow for the application of oscillation diagnosis,"Causality analysis techniques can be used for fault diagnosis in industrial processes. Multiple causality analysis techniques have been shown to be effective for fault diagnosis [1]. Comparisons of some of the strengths and weaknesses of these techniques have been presented in literature [2], [3]. However, there are no clear guidelines on which technique to select for a specific application. These comparative studies have not thoroughly addressed all the factors affecting the selection of techniques. In this paper, these two techniques are compared based on their accuracy, precision, automatability, interpretability, computational complexity, and applicability for different process characteristics. Transfer entropy and Granger causality are popular causality analysis techniques, and therefore these two are selected for this study. The two techniques were tested on an industrial case study of a plant wide oscillation and their features were compared. To investigate the accuracy and precision of Granger causality and transfer entropy, their ability to find true connections in a simulated process was also tested. Transfer entropy was found to be more precise and the causality maps derived from it were more visually interpretable. However, Granger causality was found to be easier to automate, much less computationally expensive, and easier to interpret the meaning of the values obtained. A decision flow was developed from these comparisons to aid users in deciding when to use Granger causality or transfer entropy, as well as to aid in the interpretation of the causality maps obtained from these techniques."
Comparison of Temporal and Non-Temporal Features Effect on Machine Learning Models Quality and Interpretability for Chronic Heart Failure Patients,"Chronic diseases are complex systems that can be described by various heteroscedastic data that varies in time. The goal of this work is to determine whether historical data helps to improve machine learning predictive models or is it more efficient to use the latest data describing the disease in particular moment in time. For simplicity we call features from the first group dynamic and features from the second one – static. We study the way both groups affect predictions quality and its interpretation. We set the experiments on data of chronic heart patients from Almazov Medical Research Center. From this data we extracted more than 300 features from patient comorbidity, anamnesis, analysis, etc. In terms of Chronic Heart Failure (CHF) modelling three different tasks have been selected: CHF identification as main diagnosis, CHF stage classification and diastolic blood pressure prediction. For each task several machine learning algorithms on three groups of features: static, dynamic and the whole feature set. The results show that, in general, models perform better on combination of temporal and non-temporal features."
Comparison of Temporal and Non-Temporal Features Effect on Machine Learning Models Quality and Interpretability for Chronic Heart Failure Patients,"Chronic diseases are complex systems that can be described by various heteroscedastic data that varies in time. The goal of this work is to determine whether historical data helps to improve machine learning predictive models or is it more efficient to use the latest data describing the disease in particular moment in time. For simplicity we call features from the first group dynamic and features from the second one – static. We study the way both groups affect predictions quality and its interpretation. We set the experiments on data of chronic heart patients from Almazov Medical Research Center. From this data we extracted more than 300 features from patient comorbidity, anamnesis, analysis, etc. In terms of Chronic Heart Failure (CHF) modelling three different tasks have been selected: CHF identification as main diagnosis, CHF stage classification and diastolic blood pressure prediction. For each task several machine learning algorithms on three groups of features: static, dynamic and the whole feature set. The results show that, in general, models perform better on combination of temporal and non-temporal features."
Computational modeling of blood flow steal phenomena caused by subclavian stenoses,"The study of steal mechanisms caused by vessel obstructions is of the utmost importance to gain understanding about their pathophysiology, as well as to improve diagnosis and management procedures. The goal of this work is to perform a computational study to gain insight into the hemodynamic forces that drive blood flow steal mechanisms caused by subclavian artery stenosis. Such condition triggers a flow disorder known as subclavian steal. When this occurs in patients with internal thoracic artery anastomosed to the coronary vessels, the phenomenon includes a coronary-subclavian steal. True steal can exist in cases of increased arm blood flow, potentially resulting in neurological complications and, in the case of coronary-subclavian steal, graft function failure. In this context, the anatomically detailed arterial network (ADAN) model is employed to simulate subclavian steal and coronary-subclavian steal phenomena. Model results are verified by comparison with published data. It is concluded that this kind of model allows us to effectively address complex hemomdynamic phenomena occurring in clinical practice. More specifically, in the studied conditions it is observed that a regional brain steal occurs, primarily affecting the posterior circulation, not fully compensated by the anterior circulation. In the case of patients with coronary revascularization, it is concluded that there is a large variability in graft hemodynamic environments, which physically explain both the success of the procedure in cases of severe occlusive disease, and the reason for graft dysfunction in mildly stenosed left anterior descending coronary artery, due to alternating graft flow waveform signatures."
Computer aided diagnosis of pulmonary hamartoma from CT scan images using ant colony optimization based feature selection,"Background Computer-aided diagnosis (CAD) systems for the detection of lung disorders play an important role in clinical decision making. CAD systems provide a second opinion to the physician in interpreting computed tomography (CT) images. In this work, a CAD system to diagnose pulmonary hamartoma nodules from chest CT images is proposed. Methods Segmentation of lung parenchyma from CT images is carried out using Otsu’s thresholding method. Nodules are considered to be the region of interests (ROIs) in this work. Texture, shape and run length based features are extracted from the ROIs. Cosine similarity measure (CSM) and rough dependency measure (RDM) are used independently as filter evaluation functions with ant colony optimization (ACO) to select two subsets of features. The selected subsets are used to train two classifiers namely support vector machine (SVM) and Naive Bayes (NB) classifiers using 10-fold cross validation. All the four trained classifiers are tested and the performance measures are estimated. Results CT slices of patients affected with pulmonary cancer and hamartoma are used for experimentation. From the lung parenchymal tissues of 300 CT slices, 390 nodules are extracted. The feature selection algorithms, ACO-CSM and ACO-RDM are run for different feature subset sizes. The selected features are used to train SVM and NB classifiers. From the results obtained, it is inferred that SVM classifier with the feature subsets chosen by ACO-RDM feature selection approach yielded a maximum classification accuracy of 94.36% with 38 features. Conclusion From the results, it can be clearly inferred that selecting relevant features to train the classifier has a definite impact on the performance of the classifier."
Computer-aided grading of gliomas based on local and global MRI features,"Background and objectives A computer-aided diagnosis (CAD) system based on quantitative magnetic resonance imaging (MRI) features was developed to evaluate the malignancy of diffuse gliomas, which are central nervous system tumors. Methods The acquired image database for the CAD performance evaluation was composed of 34 glioblastomas and 73 diffuse lower-grade gliomas. In each case, tissues enclosed in a delineated tumor area were analyzed according to their gray-scale intensities on MRI scans. Four histogram moment features describing the global gray-scale distributions of gliomas tissues and 14 textural features were used to interpret local correlations between adjacent pixel values. With a logistic regression model, the individual feature set and a combination of both feature sets were used to establish the malignancy prediction model. Results Performances of the CAD system using global, local, and the combination of both image feature sets achieved accuracies of 76%, 83%, and 88%, respectively. Compared to global features, the combined features had significantly better accuracy (p = 0.0213). With respect to the pathology results, the CAD classification obtained substantial agreement κ = 0.698, p < 0.001. Conclusions Numerous proposed image features were significant in distinguishing glioblastomas from lower-grade gliomas. Combining them further into a malignancy prediction model would be promising in providing diagnostic suggestions for clinical use."
Concept attribution: Explaining CNN decisions to physicians,"Deep learning explainability is often reached by gradient-based approaches that attribute the network output to perturbations of the input pixels. However, the relevance of input pixels may be difficult to relate to relevant image features in some applications, e.g. diagnostic measures in medical imaging. The framework described in this paper shifts the attribution focus from pixel values to user-defined concepts. By checking if certain diagnostic measures are present in the learned representations, experts can explain and entrust the network output. Being post-hoc, our method does not alter the network training and can be easily plugged into the latest state-of-the-art convolutional networks. This paper presents the main components of the framework for attribution to concepts, in addition to the introduction of a spatial pooling operation on top of the feature maps to obtain a solid interpretability analysis. Furthermore, regularized regression is analyzed as a solution to the regression overfitting in high-dimensionality latent spaces. The versatility of the proposed approach is shown by experiments on two medical applications, namely histopathology and retinopathy, and on one non-medical task, the task of handwritten digit classification. The obtained explanations are in line with clinicians’ guidelines and complementary to widely used visualization tools such as saliency maps."
Conceptual design of new data integration and process system for KSTAR data scheduling,"The KSTAR control and data acquisition systems mainly use data storage layer of MDSPlus for diagnostic data and channel archiver for EPICS-based control system data. In addition to these storage systems, KSTAR has various types of data such as user logs from Relational Database (RDB) and various types of logs from the control system. A large scientific machine like KSTAR is needed to implement various types of use cases for scheduling data and data analysis. The goal of a new data integration and process system is to design the KSTAR data scheduling on top of the Pulse Automation and Scheduling System (PASS) according to KSTAR events. The KSTAR Data Integration System (KDIS) is designed by using Big Data software infrastructures and frameworks. The KDIS handles events that are synchronized with the KSTAR EPICS events and other data sources such as the rest API and logs for integrating and processing data from different data sources and for visualizing data. In this paper, we explain the detailed design concept of KDIS and demonstrate a data scheduling use case with this system."
Condition monitoring of rotating machines under time-varying conditions based on adaptive canonical variate analysis,"Condition monitoring signals obtained from rotating machines often demonstrate a highly non-stationary and transient nature due to internal natural deterioration characteristics of their constituent components and external time-varying operational conditions. Traditional multivariate statistical monitoring approaches are based on the assumption that the underlying processes are linear and static and are apt to interpret the normal changes in operating conditions as faults, which would result in high false positive rates. On the other hand, the development of robust diagnostic techniques for the detection of incipient faults remains a challenge for researchers, given the difficulty of finding an appropriate trade-off between a low false positive ratio and early detection of emerging faults. To address these issues, this paper proposes a novel adaptive fault detection approach based on the canonical residuals (CR) induced by the combination of canonical variate analysis (CVA) and matrix perturbation theory for the monitoring of dynamic processes where variations in operating conditions are incurred. The canonical residuals are calculated based upon the distinctions between past and future measurements and are able to effectively detect emerging faults while still maintaining a low false positive rate. The effectiveness of the developed diagnostic model for the detection of abnormalities in industrial processes was demonstrated for slow involving faults in case studies of two operational industrial high-pressure pumps. In comparison with the variable-based and canonical correlation-based statistical monitoring approaches, the proposed canonical residuals-adaptive canonical variate analysis (CR-ACVA) fault detection method has demonstrated its superiorities by the detailed performance comparisons."
Congestive heart failure waveform classification based on short time-step analysis with recurrent network,"Congestive heart failure (CHF) is characterized by the heart's inability to pump blood adequately throughout the body without increased intracardiac pressure. Diverse approaches are used to treat CHF. These approaches, which include physical examination, echocardiography, and laboratory testing, require a high degree of competence to interpret findings and make diagnoses. Moreover, existing methods do not account for the relationships between variables and thus provide limited performance. Electrocardiogram (ECG), as a non-invasive test, may be used for CHF early diagnosis, which would require further examination to be referred. A previous study revealed a significant correlation between heart failure (HF) and ECG features. However, the method was only performed on small, balanced data; then, the features must be derived from trial and error. The current paper proposes deep-learning techniques—recurrent neural networks (RNNs) with long short-term memory (LSTM) architectures—to create a diagnostic algorithm that achieves high accuracy with limited information and automated feature extraction. The ECG signals used in this study were obtained from the public PhysioNet databases. We fine-tuned the hyperparameters of 24 LSTM models to obtain the best model. Moreover, ECG signal segmentation was compared among the first five and fifteen minutes as features. Out of the 24 LSTM models, the model with the first fifteen minutes of ECG signals (model 1) obtained the highest accuracy, sensitivity, specificity, precision, and F1-score (99.86%, 99.85%, 99.85%, 99.87%, and 99.86%, respectively). The first fifteen minutes of ECG signals performed well because the LSTM model learned an increasing number of features. In conclusion, the proposed LSTM model could give a clinician a preliminary CHF diagnosis for further medical attention. Deep learning can be a useful predictive method for increasing the number of identified CHF patients."
Considering the human operator cognitive process for the interpretation of diagnostic outcomes related to component failures and cyber security attacks,"In this work, we consider diagnostics of cyber attacks in Cyber-Physical Systems (CPSs), based on data analytics. For the first time to authors knowledge, the performance of such diagnosis is quantified considering the possible failure of the human operator cognitive process in interpreting and understanding the diagnosis support tool outcomes. A Non-Parametric CUmulative SUM (NP-CUSUM) approach is used for data-driven diagnostic, and the cognitive process of the human operator who interprets its outputs is modeled by a Bayesian Belief Network (BBN). The overall framework is applied on the digital controller of the Advanced Lead-cooled Fast Reactor European Demonstrator (ALFRED)."
Constructing Interpretable Classifiers to Diagnose Gastric Cancer Based on Breath Tests,"Quick, inexpensive and accurate diagnosis of gastric cancer is a necessity, but at this moment the available methods do not hold up. One of the most promising possibilities is breath test analysis, which is quick, relatively inexpensive and comfortable to the person tested. However, this method has not yet been well explored. Therefore in this article the authors propose using transparent classification models to explain diagnostic patterns and knowledge, which is acquired in the process. The models are induced using decision tree classification algorithms and RIPPER algorithm for decision rule induction. The accuracy of these models is compared to neural network accuracy."
"Convolutional neural networks: Ensemble modeling, fine-tuning and unsupervised semantic localization for neurosurgical CLE images","Confocal laser endomicroscopy (CLE) is an advanced optical fluorescence technology undergoing assessment for applications in brain tumor surgery. Many of the CLE images can be distorted and interpreted as nondiagnostic. However, just one neat CLE image might suffice for intraoperative diagnosis of the tumor. While manual examination of thousands of nondiagnostic images during surgery would be impractical, this creates an opportunity for a model to select diagnostic images for the pathologists or surgeons review. In this study, we sought to develop a deep learning model to automatically detect the diagnostic images. We explored the effect of training regimes and ensemble modeling and localized histological features from diagnostic CLE images. The developed model could achieve promising agreement with the ground truth. With the speed and precision of the proposed method, it has potential to be integrated into the operative workflow in the brain tumor surgery."
Cost estimation of an asteroid mining mission using partial least squares structural equation modelling (PLS-SEM),"One of the biggest challenges of a mining endeavour lies in the judgement of its economic feasibility such as the estimation of costs and the prediction of the market value of the designated product. Whereas the complexity of the latter originates mainly from the supply and demand of the economic situation, the complexity of the first derives from the lack of heritage data, the high degree of innovation and the uncertainties that are connected to any long-term project. In this paper, the authors apply a cause-effect analysis technique called structural equation modelling (SEM) using partial least squares (PLS) to estimate the costs of an asteroid mining mission concept called KaNaRiA. For data ascertainment, two expert interview rounds have been conducted involving international space engineers and scientists from different professional levels and areas. First, a qualitative method called the Delphi technique is used to identify the main cost drivers and quantify their influence on the overall costs. Second, the cost drivers were formulated as questions to enable the classification on a Likert-scale. The collected data is the input for the development of the cost prognosis model by applying PLS-SEM. The cost model is given and its usage explained. The resulting cost model allows a user-individual estimation of the mission costs depending on the individual judgement of the influence of the final, relevant cost drivers. An application of the model using the judgment of the authors’ is presented."
Co-trained convolutional neural networks for automated detection of prostate cancer in multi-parametric MRI,"Multi-parameter magnetic resonance imaging (mp-MRI) is increasingly popular for prostate cancer (PCa) detection and diagnosis. However, interpreting mp-MRI data which typically contains multiple unregistered 3D sequences, e.g. apparent diffusion coefficient (ADC) and T2-weighted (T2w) images, is time-consuming and demands special expertise, limiting its usage for large-scale PCa screening. Therefore, solutions to computer-aided detection of PCa in mp-MRI images are highly desirable. Most recent advances in automated methods for PCa detection employ a handcrafted feature based two-stage classification flow, i.e. voxel-level classification followed by a region-level classification. This work presents an automated PCa detection system which can concurrently identify the presence of PCa in an image and localize lesions based on deep convolutional neural network (CNN) features and a single-stage SVM classifier. Specifically, the developed co-trained CNNs consist of two parallel convolutional networks for ADC and T2w images respectively. Each network is trained using images of a single modality in a weakly-supervised manner by providing a set of prostate images with image-level labels indicating only the presence of PCa without priors of lesions’ locations. Discriminative visual patterns of lesions can be learned effectively from clutters of prostate and surrounding tissues. A cancer response map with each pixel indicating the likelihood to be cancerous is explicitly generated at the last convolutional layer of the network for each modality. A new back-propagated error E is defined to enforce both optimized classification results and consistent cancer response maps for different modalities, which help capture highly representative PCa-relevant features during the CNN feature learning process. The CNN features of each modality are concatenated and fed into a SVM classifier. For images which are classified to contain cancers, non-maximum suppression and adaptive thresholding are applied to the corresponding cancer response maps for PCa foci localization. Evaluation based on 160 patient data with 12-core systematic TRUS-guided prostate biopsy as the reference standard demonstrates that our system achieves a sensitivity of 0.46, 0.92 and 0.97 at 0.1, 1 and 10 false positives per normal/benign patient which is significantly superior to two state-of-the-art CNN-based methods (Oquab et al., 2015; Zhou et al., 2015) and 6-core systematic prostate biopsies."
Could removal of project-level knowledge flow obstacles contribute to software process improvement? A study of software engineer perceptions,"Context Software process improvement (SPI) is one type of innovation often formulated to address problems such as uncontrollable costs, schedule overruns, and poor end product quality. This study investigates SPI through the application of knowledge management (KM) at the software project level, viewing KM, when applied in software development, as complementary to SPI. Objective This study advances the use of KM in SPI by investigating impediments to the use and flow of knowledge within an individual software development project. Our proposition is that the removal of obstacles to project-level knowledge flow (K-flow) will enhance SPI. We investigate this proposition by exploring the problems associated with project K-flows as seen by software project team members. Method We conducted a descriptive case study of an industrial software development project, in which we collected data concerning project knowledge sources and K-flows using semi-structured interviews. Using this data, we constructed a diagnostic project knowledge map (K-map). The K-map was analyzed to identify K-flow obstacles and potential solutions. Questionnaires based on these obstacles and solutions were formulated to probe software engineers’ perceptions of the effect of the solutions on SPI. Results Findings from participant questionnaires reveal that software engineers perceive that the removal or mitigation of project-level K-flow obstacles generally reduces the time to perform their work, helps them to meet their deadlines, and improves their work quality, thus resulting in SPI. Conclusions This study provides support for the usefulness of project-level K-flow obstacle removal for SPI. It provides a unique project-level perspective, using input from the project's software engineers. It also explains and supports the use of K-mapping for the identification of project-level K-flow obstacles. With this approach, practitioners gain insight into SPI in “real time” as a project is executed. These insights may help to enhance their current and future SPI efforts."
COVID-19 diagnosis —A review of current methods,"A fast and accurate self-testing tool for COVID-19 diagnosis has become a prerequisite to comprehend the exact number of cases worldwide and to take medical and governmental actions accordingly. SARS-CoV-2 (formerly, 2019-nCoV) infection was first reported in Wuhan (China) in December 2019, and then it has rapidly spread around the world, causing ~14 million active cases with ~582,000 deaths as of July 2020. The diagnosis tools available so far have been based on a) viral gene detection, b) human antibody detection, and c) viral antigen detection, among which the viral gene detection by RT-PCR has been found as the most reliable technique. In this report, the current SARS-CoV-2 detection kits, exclusively the ones that were issued an “Emergency Use Authorization” from the U.S. Food and Drug Administration, were discussed. The key structural components of the virus were presented to provide the audience with an understanding of the scientific principles behind the testing tools. The methods that are still in the early research state were also reviewed in a subsection based on the reports available so far."
CWV-BANN-SVM ensemble learning classifier for an accurate diagnosis of breast cancer,"This paper presents a new data mining technique for an accurate prediction of breast cancer (BC), which is one of the major mortality causes among women around the globe. The main objective of our study is to expand an automatic expert system (ES) to provide an accurate diagnosis of BC. Both, Support Vector Machines (SVMs) and Artificial Neural Networks (ANNs) were applied to analyze BC data. The well-known Wisconsin Breast Cancer Dataset (WBCD), available in the UCI repository, was examined in our study. We first tested the SVM algorithm using various values of the C, ɛ and γ parameters. As a result of the first experiment, we were able to observe that the adjustment of these regularization parameters can greatly improve the performance of the traditional SVM algorithm applied for BC detection. The highest obtained accuracy at the first step was 99.71%. Then, we performed a new BC detection approach based on two ensemble learning techniques: the confidence-weighted voting method and the boosting ensemble technique. Our model, called CWV-BANNSVM, combines boosting ANNs (BANN) and two SVMs, using optimal parameters selected during the first experiment. The performance of the applied methods was evaluated using several popular metrics, such as specificity, sensitivity, precision, FPR, FNR, F1 score, AUC, Gini and accuracy. The proposed CWV-BANNSVM model was able to improve the performance of the traditional machine learning algorithms applied to BC detection, reaching the accuracy of 100%. To overcome the overfitting issue, we determined and used some appropriate parameter values of polynomial SVM. Our comparison with the existing studies dedicated to BC prediction suggests that the proposed CWV-BANN-SVM model provides one of the best prediction performances overall."
Daily energy consumption signatures and control charts for air-conditioned buildings,"Energy signatures for air conditioning systems can have characteristics which are not seen with heating systems. This paper explains and illustrates some of the characteristics that are specific to air conditioning systems and describes how energy signatures that take account of them can be applied to produce benchmarks, control charts and diagnostic information. It focusses on the use of energy signatures derived from measured daily system energy consumption. Daily energy signatures can generate more robust energy consumption benchmarks and provide additional insight into unusual energy demand patterns compared to monthly or weekly signatures, albeit requiring slightly more data. In particular, they distinguish between weekday and weekend consumptions. They can be used to generate benchmarks based on standardised annual consumption or standardised annual load factor. In addition they can be used to generate control charts to identify days of unusual consumption for individual systems. More sets of daily energy consumption data are needed to evaluate their diagnostic power."
Damage identification techniques via modal curvature analysis: Overview and comparison,"This paper aims to compare several damage identification methods based on the analysis of modal curvature and related quantities (natural frequencies and modal strain energy) by evaluating their performances on the same test case, a damaged Euler–Bernoulli beam. Damage is modelled as a localized and uniform reduction of stiffness so that closed-form expressions of the mode-shape curvatures can be analytically computed and data accuracy, which affects final results, can be controlled. The selected techniques belong to two categories: one includes several methods that need reference data for detecting structural modifications due to damage, the second group, including the modified Laplacian operator and the fractal dimension, avoids the knowledge of the undamaged behavior for issuing a damage diagnosis. To explain better the different performances of the methods, the mathematical formulation has been revised in some cases so as to fit into a common framework where the underlying hypotheses are clearly stated. Because the various damage indexes are calculated on ‘exact’ data, a sensitivity analysis has been carried out with respect to the number of points where curvature information is available, to the position of damage between adjacent points, to the modes involved in the index computation. In this way, this analysis intends to point out comparatively the capability of locating and estimating damage of each method along with some critical issues already present with noiseless data."
Data Streams Mining for Anomaly and Change Detection in Continuous Plant Operation,"Data streams are collected from the real time operation of complex machines, plants and other technological systems. The collected information could be further used for different purposes, such as performance evaluation, anomaly detection, change detection or fault diagnosis of the operating systems. The analysis of the data streams is done by a calculation tool for estimating the similarity level between pairs of given chunks of data from the data stream, considered as data clouds. One of these data cloud represents a prerecorded (previously known) abnormal operation of the system, while the other data cloud represents a current (still unknown) behavior of the system. Then the similarity analysis will show how close the two data clouds are. In this paper we propose a novel method for similarity analysis that uses two types of models called Data Cloud Model (DCM) and Window Cloud Model (WCM). The DCM is obtained from the data cloud that represents a previously known operation of the system, while the WCM is obtained from the newly collected data cloud from the data stream, called window data cloud. Both data clouds have an equal length (number of data). The algorithms for creating the DCM and WCM are explained in the paper. The DCM consists of Active Grid cells that represent approximately the data density within the known data cloud. The WCM estimates the data density at the same active grid cells, but based on the data from the new window data cloud. Both densities are represented as two Histograms that are compared to each other in order to calculate the similarity level as a value between 0.0 and 1.0. Another problem discussed in the paper is finding a plausible method for detection of significant changes in the data streams. Here the moving window technique is used for collecting series of subsequent data clouds. Then two procedures of moving windows are run in parallel, each of them with different window lengths, in order to calculate the center-of-gravity of the respective data in a real time. The difference between the results of the two moving windows is used as an estimate of the change in the process operation. Both technologies developed in this paper are explain in details in the paper and illustrated on the example of real data stream from a petrochemical plant."
Data-driven and association rule mining-based fault diagnosis and action mechanism analysis for building chillers,"Developing advanced fault detection and diagnosis (FDD) techniques for building chillers is becoming increasingly essential for building energy saving. Previous FDD studies have mainly concentrated on the model performance, while fewer studies have examined the chiller fault action mechanism. This paper, therefore, proposes a method that can conduct both fault diagnosis and fault action mechanism explanation of building chillers. The method is data-driven-based and can be trained by system operational data based on the classification based on association (CBA) algorithm. Also, it is qualitatively based because system operational rules can be extracted from the diagnostic model by association rule mining. The fault diagnosis process and the fault action mechanism on the chiller system can be then understood by rule interpretation. The experimental chiller data of ASHRAE RP-1043 is used to validate the effectiveness of the proposed method, and the results show that the CBA-based fault diagnosis model can well identify seven common chiller faults with an overall diagnostic accuracy of 90.15%. In this work, the key rules of each fault are extracted and visualized. The mined rules can be well interpreted by domain knowledge, and the action mechanisms of seven faults are concluded. Moreover, the discrepant rule analysis can provide a proper reference for multiple fault decoupling. The knowledge discovered from the fault diagnosis process is valuable for the development of FDD researches and shortcuts for field application."
Data-driven approach to study the polygonization of high-speed railway train wheel-sets using field data of China’s HSR train,"Environmental factors, like seasonality, have been proved to exert significant impact on reliability of China high-speed rail train wheels in this article. Most studies on polygonization of train wheels are based on physical models, mathematical models or simulation systems. Normally, characteristics and mechanisms of wheel polygonization are studied under ideal conditions without considering the impact of the environment. However, in practical use, there are many irregular wear wheels and irregular wear cannot be explained by theoretical models with assumptions of ideal conditions. We look at two possible factors in polygonization: seasonality and proximity to engines. Our analysis of field data shows the environmental factor has more impact on wheel polygonization than the mechanical factor. Based on the Bayesian models, the mean time to failure of the wheels under different operation conditions is conducted. A case study of China’s HSR train wheels is conducted to confirm the finding. The case study shows the degree of polygonal wear is much more severe in summer than other seasons. The finding may give a totally new research perspective on polygonization of train wheels. We use Bayesian analysis because this method is useful for small and incomplete data sets. We propose three Bayesian data-driven models."
Data-driven decision model based on dynamical classifier selection,"In the era of big data, large volumes of data have been accumulated in different fields. To help make decisions by using the accumulated data, this paper proposes a data-driven decision model based on dynamical classifier selection (DCS). Under the framework of multi-criteria decision making, historical data including individual assessments on criteria and overall assessments are collected and represented by interval-valued numbers. A set of base classifiers is selected and trained using historical data. For each new alternative, its similar historical alternatives are identified from historical data through using its prediction derived from each trained classifier. By using the predictive accuracy of each base classifier and the average similarity between the new alternative and its similar historical alternatives, a new DCS strategy is developed to select an appropriate classifier from base classifiers for the new alternative. The developed DCS strategy effectively avoids the subjective determination of the size of the local region in the field of DCS. Based on the similar historical alternatives determined by the selected base classifier, an optimization model is constructed to learn criterion weights from the individual assessments of the alternatives on criteria and the predicted overall assessments derived from the selected base classifier. Then, the explainable decisions are generated with the learned criterion weights. The proposed decision model is used to aid the diagnosis of thyroid nodules. Through its comparison with traditional decision model and three representative DCS methods, the advantage of the proposed decision model in balancing accuracy and interpretability is validated."
Data-efficient deep learning of radiological image data for outcome prediction after endovascular treatment of patients with acute ischemic stroke,"Treatment selection is becoming increasingly more important in acute ischemic stroke patient care. Clinical variables and radiological image biomarkers (old age, pre-stroke mRS, NIHSS, occlusion location, ASPECTS, among others) have an important role in treatment selection and prognosis. Radiological biomarkers require expert annotation and are subject to inter-observer variability. Recently, Deep Learning has been introduced to reproduce these radiological image biomarkers. Instead of reproducing these biomarkers, in this work, we investigated Deep Learning techniques for building models to directly predict good reperfusion after endovascular treatment (EVT) and good functional outcome using CT angiography images. These models do not require image annotation and are fast to compute. We compare the Deep Learning models to Machine Learning models using traditional radiological image biomarkers. We explored Residual Neural Network (ResNet) architectures, adapted them with Structured Receptive Fields (RFNN) and auto-encoders (AE) for network weight initialization. We further included model visualization techniques to provide insight into the network's decision-making process. We applied the methods on the MR CLEAN Registry dataset with 1301 patients. The Deep Learning models outperformed the models using traditional radiological image biomarkers in three out of four cross-validation folds for functional outcome (average AUC of 0.71) and for all folds for reperfusion (average AUC of 0.65). Model visualization showed that the arteries were relevant features for functional outcome prediction. The best results were obtained for the ResNet models with RFNN. Auto-encoder initialization often improved the results. We concluded that, in our dataset, automated image analysis with Deep Learning methods outperforms radiological image biomarkers for stroke outcome prediction and has the potential to improve treatment selection."
DBN based failure prognosis method considering the response of protective layers for the complex industrial systems,"In complex industrial systems, operating, regulating, maintenance activities and external incidents take place dynamically and multiple entities in same or different subsystems interact in a complex manner. Most of single faults have multiple propagation paths. Any local slight deviation is able to propagate, spread, accumulate and increase through system fault causal chains. It will finally result in system failure and unplanned outages or even catastrophic accidents. The key issues focus on both of how to reduce the probability of fault occurrence and decrease the loss of fault consequence. The implementation of such requirements can be studied in terms of the determination of the fault root causes, prediction of the possible consequence, and also estimation of the risk and timing of various maintenance activities, which are considered in a failure prognosis scheme. This study proposes a DBN based failure prognosis method for complex system. Not only the interaction between components, but also the influence of the layers of protection in the system is considered when the dynamic failure scenarios are analyzed. Therefore the proposed method considers multiple factors including degradation mechanism, parameter deviation, the response of the layers of protection and also the external environment. With this model, the dynamic influence diagram of the components' degradation trends can be calculated and be used to evaluate the different effects of the layers of protection quantitatively. Some key problems are also explained such as how to determine the new nodes in DBN representing the behavior of protective layers and how to update the CPT in the extended model. In the case study, the proposed method is tested on the flue gas energy recovery system (FGERS) which is widely used in the petrochemical industry to demonstrate its effectiveness. It makes a great help for early warning and optimization of the layers of protection in the complex industrial system."
Deep convolution network based emotion analysis towards mental health care,"Facial expressions play an important role during communications, allowing information regarding the emotional state of an individual to be conveyed and inferred. Research suggests that automatic facial expression recognition is a promising avenue of enquiry in mental healthcare, as facial expressions can also reflect an individual's mental state. In order to develop user-friendly, low-cost and effective facial expression analysis systems for mental health care, this paper presents a novel deep convolution network based emotion analysis framework to support mental state detection and diagnosis. The proposed system is able to process facial images and interpret the temporal evolution of emotions through a new solution in which deep features are extracted from the Fully Connected Layer 6 of the AlexNet, with a standard Linear Discriminant Analysis Classifier exploited to obtain the final classification outcome. It is tested against 5 benchmarking databases, including JAFFE, KDEF,CK+, and databases with the images obtained ‘in the wild’ such as FER2013 and AffectNet. Compared with the other state-of-the-art methods, we observe that our method has overall higher accuracy of facial expression recognition. Additionally, when compared to the state-of-the-art deep learning algorithms such as Vgg16, GoogleNet, ResNet and AlexNet, the proposed method demonstrated better efficiency and has less device requirements. The experiments presented in this paper demonstrate that the proposed method outperforms the other methods in terms of accuracy and efficiency which suggests it could act as a smart, low-cost, user-friendly cognitive aid to detect, monitor, and diagnose the mental health of a patient through automatic facial expression analysis."
Deep convolutional extreme learning machines: Filters combination and error model validation,"In recent years, deep convolutional neural network models have been increasingly used in various computer vision tasks, like plate number recognition, object recognition, automatic digit recognition, and medical applications supporting diagnosis by signals or images. A disadvantage of these networks is the long training time. It can take days to adjust weights with iterative methods based on gradient descent. This can be an obstacle in applications that need frequent training or in real time. Fast convolutional networks avoid gradient-based methods by efficiently defining filters in feature extraction and weights in classification. The issue is how to set the convolutional filter banks, since they are not learned by the backpropagation of gradients? In this work we propose a deep fast convolutional neural network based on extreme learning machine and a fixed bank of filters. We demonstrate that our model is feasible to be used in cost-effective non-specialized computer hardware, performing the training task faster than models running on GPUs. Results were generated on EMNIST dataset representing the widely studied problem of digit recognition. We provide a deep convolutional extreme learning machine (CELM) with two feature extraction stages and combinations of selected filters. For the proposed network, we find that the empirical generalization error is explained by the error model based on a theorem by Rahimi and Retch. In comparison to the state-of-the-art, the proposed network resulted in superior accuracy as well as competitive training time, even in relation to approaches that employ processing in GPUs."
Deep learning algorithms for rotating machinery intelligent diagnosis: An open source benchmark study,"Rotating machinery intelligent diagnosis based on deep learning (DL) has gone through tremendous progress, which can help reduce costly breakdowns. However, different datasets and hyper-parameters are recommended to be used, and few open source codes are publicly available, resulting in unfair comparisons and ineffective improvement. To address these issues, we perform a comprehensive evaluation of four models, including multi-layer perception (MLP), auto-encoder (AE), convolutional neural network (CNN), and recurrent neural network (RNN), with seven datasets to provide a benchmark study. We first gather nine publicly available datasets and give a comprehensive benchmark study of DL-based models with two data split strategies, five input formats, three normalization methods, and four augmentation methods. Second, we integrate the whole evaluation codes into a code library and release it to the public for better comparisons. Third, we use specific-designed cases to point out the existing issues, including class imbalance, generalization ability, interpretability, few-shot learning, and model selection. Finally, we release a unified code framework for comparing and testing models fairly and quickly, emphasize the importance of open source codes, provide the baseline accuracy (a lower bound), and discuss existing issues in this field. The code library is available at: https://github.com/ZhaoZhibin/DL-based-Intelligent-Diagnosis-Benchmark."
Deep learning for brain disorder diagnosis based on fMRI images,"In modern neuroscience and clinical study, neuroscientists and clinicians often use non-invasive imaging techniques to validate theories and computational models, observe brain activities and diagnose brain disorders. The functional Magnetic Resonance Imaging (fMRI) is one of the commonly-used imaging modalities that can be used to understand human brain mechanisms as well as the diagnosis and treatment of brain disorders. The advances in artificial intelligence and the emergence of deep learning techniques have shown promising results to better interpret fMRI data. Deep learning techniques have rapidly become the state of the art for analyzing fMRI data sets and resulted in performance improvements in diverse fMRI applications. Deep learning is normally presented as an end-to-end learning process and can alleviate feature engineering requirements and hence reduce domain knowledge requirements to some extent. Under the framework of deep learning, fMRI data can be considered as images, time series or images series. Hence, different deep learning models such as convolutional neural networks, recurrent neural network, or a combination of both, can be developed to process fMRI data for different tasks. In this review, we discussed the basics of deep learning methods and focused on its successful implementations for brain disorder diagnosis based on fMRI images. The goal is to provide a high-level overview of brain disorder diagnosis with fMRI images from the perspective of deep learning applications."
"Deep learning for prognostics and health management: State of the art, challenges, and opportunities","Improving the reliability of engineered systems is a crucial problem in many applications in various engineering fields, such as aerospace, nuclear energy, and water declination industries. This requires efficient and effective system health monitoring methods, including processing and analyzing massive machinery data to detect anomalies and performing diagnosis and prognosis. In recent years, deep learning has been a fast-growing field and has shown promising results for Prognostics and Health Management (PHM) in interpreting condition monitoring signals such as vibration, acoustic emission, and pressure due to its capacity to mine complex representations from raw data. This paper provides a systematic review of state-of-the-art deep learning-based PHM frameworks. It emphasizes on the most recent trends within the field and presents the benefits and potentials of state-of-the-art deep neural networks for system health management. In addition, limitations and challenges of the existing technologies are discussed, which leads to opportunities for future research."
Deep Learning Identifies Digital Biomarkers for Self-Reported Parkinson's Disease,"Large-scale population screening and in-home monitoring for patients with Parkinson's disease (PD) has so far been mainly carried out by traditional healthcare methods and systems. Development of mobile health may provide an independent, future method to detect PD. Current PD detection algorithms will benefit from better generalizability with data collected in real-world situations. In this paper, we report the top-performing smartphone-based method in the recent DREAM Parkinson's Disease Digital Biomarker Challenge for digital diagnosis of PD. Utilizing real-world accelerometer records, this approach differentiated PD from control subjects with an area under the receiver-operating characteristic curve of 0.87 by 3D augmentation of accelerometer records, a significant improvement over other state-of-the-art methods. This study paves the way for future at-home screening of PD and other neurodegenerative conditions affecting movement."
Deep Learning Identifies Digital Biomarkers for Self-Reported Parkinson's Disease,"Large-scale population screening and in-home monitoring for patients with Parkinson's disease (PD) has so far been mainly carried out by traditional healthcare methods and systems. Development of mobile health may provide an independent, future method to detect PD. Current PD detection algorithms will benefit from better generalizability with data collected in real-world situations. In this paper, we report the top-performing smartphone-based method in the recent DREAM Parkinson's Disease Digital Biomarker Challenge for digital diagnosis of PD. Utilizing real-world accelerometer records, this approach differentiated PD from control subjects with an area under the receiver-operating characteristic curve of 0.87 by 3D augmentation of accelerometer records, a significant improvement over other state-of-the-art methods. This study paves the way for future at-home screening of PD and other neurodegenerative conditions affecting movement."
"Deep learning, reusable and problem-based architectures for detection of consolidation on chest X-ray images","Background and objective In most patients presenting with respiratory symptoms, the findings of chest radiography play a key role in the diagnosis, management, and follow-up of the disease. Consolidation is a common term in radiology, which indicates focally increased lung density. When the alveolar structures become filled with pus, fluid, blood cells or protein subsequent to a pulmonary pathological process, it may result in different types of lung opacity in chest radiograph. This study aims at detecting consolidations in chest x-ray radiographs, with a certain precision, using artificial intelligence and especially Deep Convolutional Neural Networks to assist radiologist for better diagnosis. Methods Medical image datasets usually are relatively small to be used for training a Deep Convolutional Neural Network (DCNN), so transfer learning technique with well-known DCNNs pre-trained with ImageNet dataset are used to improve the accuracy of the models. ImageNet feature space is different from medical images and in the other side, the well-known DCNNs are designed to achieve the best performance on ImageNet. Therefore, they cannot show their best performance on medical images. To overcome this problem, we designed a problem-based architecture which preserves the information of images for detecting consolidation in Pediatric Chest X-ray dataset. We proposed a three-step pre-processing approach to enhance generalization of the models. To demonstrate the correctness of numerical results, an occlusion test is applied to visualize outputs of the model and localize the detected appropriate area. A different dataset as an extra validation is used in order to investigate the generalization of the proposed model. Results The best accuracy to detect consolidation is 94.67% obtained by our problem based architecture for the understudy dataset which outperforms the previous works and the other architectures. Conclusions The designed models can be employed as computer aided diagnosis tools in real practice. We critically discussed the datasets and the previous works based on them and show that without some considerations the results of them may be misleading. We believe, the output of AI should be only interpreted as focal consolidation. The clinical significance of the finding can not be interpreted without integration of clinical data."
Deep neural network ensemble for pneumonia localization from a large-scale chest x-ray database,"Pneumonia is a bacterial, viral, or fungal infection of one or both sides of the lungs that causes lung alveoli to fill up with fluid or pus, which is usually diagnosed with chest x-rays. This work investigates opportunities for applying machine learning solutions for automated detection and localization of pneumonia on chest x-ray images. We propose an ensemble of two convolutional neural networks, namely RetinaNet and Mask R-CNN for pneumonia detection and localization. We validated our solution on a recently released dataset of 26,684 images from Kaggle Pneumonia Detection Challenge and were score among the top 3% of submitted solutions. With 0.793 recall, we developed a reliable solution for automated pneumonia diagnosis and validated it on the largest clinical database publicity available to date. Some of the challenging cases were additionally examined by a team of physicians, who helped us to interpret the obtained results and confirm their practical applicability."
Deep neural network models for computational histopathology: A survey,"Histopathological images contain rich phenotypic information that can be used to monitor underlying mechanisms contributing to disease progression and patient survival outcomes. Recently, deep learning has become the mainstream methodological choice for analyzing and interpreting histology images. In this paper, we present a comprehensive review of state-of-the-art deep learning approaches that have been used in the context of histopathological image analysis. From the survey of over 130 papers, we review the field’s progress based on the methodological aspect of different machine learning strategies such as supervised, weakly supervised, unsupervised, transfer learning and various other sub-variants of these methods. We also provide an overview of deep learning based survival models that are applicable for disease-specific prognosis tasks. Finally, we summarize several existing open datasets and highlight critical challenges and limitations with current deep learning approaches, along with possible avenues for future research."
deExploit: Identifying misuses of input data to diagnose memory-corruption exploits at the binary level,"Memory-corruption exploits are one of the major threats to the Internet security. Once an exploit has been detected, exploit diagnosis techniques can be used to identify the unknown vulnerability and attack vector. In the security landscape, exploit diagnosis is always performed by third-party security experts who cannot access the source code. This makes binary-level exploit diagnosis a time-consuming and error-prone process. Despite considerable efforts to defend against exploits, automatic exploit diagnosis remains a significant challenge. In this paper, we propose a novel insight for detecting memory corruption at the binary level by identifying the misuses of input data and present an exploit diagnosis approach called deExploit. Our approach requires no knowledge of the source code or debugging information. For exploit diagnosis, deExploit is generic in terms of the detection of both control-flow-hijack and data-oriented exploits. In addition, deExploit automatically provides precise information regarding the corruption point, the memory operation that causes the corruption, and the key attack steps used to bypass existing defense mechanisms. We implement deExploit and perform it to diagnose multiple realistic exploits. The results show that deExploit is able to diagnose memory-corruption exploits."
"Definition of a SNOMED CT pathology subset and microglossary, based on 1.17 million biological samples from the Catalan Pathology Registry","SNOMED CT terminology is not backed by standard norms of encoding among pathologists. The vast number of concepts ordered in hierarchies and axes, together with the lack of rules of use, complicates the functionality of SNOMED CT for coding, extracting, and analyzing the data. Defining subgroups of SNOMED CT by discipline could increase its functionality. The challenge lies in how to choose the concepts to be included in a subset from a total of over 300,000. Besides, SNOMED CT does not cover daily need, as the clinical reality is dynamic and changing. To adapt SNOMED CT to needs in a flexible way, the possibility exists to create extensions. In Catalonia, most pathology departments have been migrating from SNOMED II to SNOMED CT in a bid to advance the development of the Catalan Pathology Registry, which was created in 2014 as a repository for all the pathological diagnoses. This article explains the methodology used to: (a) identify the clinico-pathological entities and the molecular diagnostic procedures not included in SNOMED CT; (b) define the theoretical subset and microglossary of pathology; (c) describe the SNOMED CT concepts used by pathologists of 1.17 million samples of the Catalan Pathology Registry; and (d) adapt the theoretical subset and the microglossary according to the actual use of SNOMED CT. Of the 328,365 concepts available for coding the diagnoses (326,732 in SNOMED CT and 1576 in Catalan extension), only 2% have been used. Combining two axes of SNOMED CT, body structure and clinical findings, has enabled coding most of the morphologies."
Denoising of low-dose CT images via low-rank tensor modeling and total variation regularization,"Low-dose Computed Tomography (CT) imaging is a most commonly used medical imaging modality. Though the reduction in dosage reduces the risk due to radiation, it leads to an increase in noise level. Hence, it is a mandatory requirement to include a noise reduction technique as a pre- and/or post-processing step for better disease diagnosis. The nuclear norm minimization has attracted a great deal of research interest in contemporary years. This paper proposes a low-rank approximation based approach for denoising of CT images by effectively utilizing the global spatial correlation and local smoothness properties. The tensor nuclear norm is used to describe the global properties and the tensor total variation is used to characterize the local smoothness as well as to improve global smoothness. The resulting optimization problem is solved by the Alternative Direction Method of Multipliers (ADMM) technique. Experimental results on simulated and real CT data prove that the proposed methods outperform the state-of-art works."
DENSA: An effective negative selection algorithm with flexible boundaries for self-space and dynamic number of detectors,"The negative selection algorithm is an anomaly detection technique inspired by the self-nonself discrimination behavior observed in the Biological Immune System. The most controversial question of these algorithms is their poor performance on real world applications. To overcome such limitation this research work focuses on generating more efficient detectors through a more flexible boundary for self-patterns. Rather than applying conventional affinity measures, the detectors are generated benefiting from a Gaussian Mixture Model (GMM) fitted on normal space. From the GMM capabilities the algorithm is able to dynamically determine efficient subsets of detectors. In order to evaluate the efficiency and robustness of the proposed algorithm, different data sets have been examined as benchmark, including 2D synthesis data sets. Furthermore, for evaluating the capability and effectiveness of the proposed algorithm on real-world problems, it has been performed and tested for detecting anomalies in archaeological sites located in Lorestan, Iran. The experimental results prove how the proposed approach helps the negative selection algorithm to improve its detection capability, because the detectors can be efficiently distributed into the non-self space. It is important to note how this research work presents also a first analysis of the anomaly detection capabilities in the field of archaeology, introducing a novel application method, which can be efficiently used by the archaeologists for interpreting their growing amount of data and draw valuable conclusions about the historical past. Finally, in order to analyse the convergence and the running time of the proposed algorithm, a study has been conducted using the classical Time-To-Target plots, which present a standard graphical methodology for data analysis based on the comparisons between the empirical and theoretical distributions."
Design and evaluation of a hybrid system for detection and prediction of faults in electrical transformers,"Transformers are the vital parts of an electrical grid system. A faulty transformer can destabilize the electrical supply along with the other devices of the transmission system. Due to its significant role in the system, a transformer has to be free from faults and irregularities. Dissolved Gas-in-oil Analysis (DGA) is a method that helps in diagnosing the faults present in an electrical transformer. This paper proposes a hybrid system based on Genetic Neural Computing (GNC) for analyzing and interpreting the data derived from the concentration of the dissolved gases. It is further analyzed and clustered into four subsets according to the standard C57.104 defined by IEEE using genetic algorithm (GA). The clustered data is fed to the neural network that is used to predict the different types of faults present in the transformers. The hybrid system generates the necessary decision rules to assist the system’s operator in identifying the exact fault in the transformer and its fault status. This analysis would then be helpful in performing the required maintenance check and plan for repairs."
"Design, integration and manufacturing of the MEBT and DPlate support frames for IFMIF LIPAc","The International Fusion Materials Irradiation Facility (IFMIF) [1] aims to provide an accelerator-based, D-Li neutron source to produce high energy neutrons at sufficient intensity and irradiation volume for DEMO materials qualification. As part of the Broader Approach (BA) agreement between Japan and EURATOM, the goal of the IFMIF/EVEDA project is to work on the engineering design of IFMIF and to validate the main technological challenges which, among a wide diversity of hardware includes the LIPAc (Linear IFMIF Prototype Accelerator) [2], a 125mA continuous wave deuteron accelerator up to 9MeV mainly designed and manufactured in Europe. The Medium Energy Beam Transport line (MEBT) [3], [4] is in charge of the beam transport at 5 MeV/125mA and matching beam parameters between two acceleration structures, the RadioFrequency Quadrupole (RFQ) and the Superconducting RF linear accelerator (SRF Linac), while the Diagnostic Plate (DPlate) [5] is a movable module with a set of diagnostics and instrumentations in charge of characterizing the beam in the different accelerator commissioning stages (RFQ commissioning, SRF Linac commissioning) and to provide accelerator operational parameters. Both beamline designs are state of art, being a real engineering (and mechanical) challenge, due to the compactness and alignment requirements from beam dynamics, and the seismic requirements from the accelerator site. An optimized design is critical in order to reduce beam losses and production of radiation at high power beam. The present paper summarizes the mechanical design and analysis of the MEBT and Diagnostic Plate support frames, as well as their manufacturing solutions and mechanical integration with the components installed both in the MEBT and Diagnostic Plate. The mechanical design and integration show the engineering development, adopted to fulfill the strict structural, seismic and alignment requirements."
Detecting time-evolving phenotypic topics via tensor factorization on electronic health records: Cardiovascular disease case study,"Objective Discovering subphenotypes of complex diseases can help characterize disease cohorts for investigative studies aimed at developing better diagnoses and treatments. Recent advances in unsupervised machine learning on electronic health record (EHR) data have enabled researchers to discover phenotypes without input from domain experts. However, most existing studies have ignored time and modeled diseases as discrete events. Uncovering the evolution of phenotypes – how they emerge, evolve and contribute to health outcomes – is essential to define more precise phenotypes and refine the understanding of disease progression. Our objective was to assess the benefits of an unsupervised approach that incorporates time to model diseases as dynamic processes in phenotype discovery. Methods In this study, we applied a constrained non-negative tensor-factorization approach to characterize the complexity of cardiovascular disease (CVD) patient cohort based on longitudinal EHR data. Through tensor-factorization, we identified a set of phenotypic topics (i.e., subphenotypes) that these patients established over the 10 years prior to the diagnosis of CVD, and showed the progress pattern. For each identified subphenotype, we examined its association with the risk for adverse cardiovascular outcomes estimated by the American College of Cardiology/American Heart Association Pooled Cohort Risk Equations, a conventional CVD-risk assessment tool frequently used in clinical practice. Furthermore, we compared the subsequent myocardial infarction (MI) rates among the six most prevalent subphenotypes using survival analysis. Results From a cohort of 12,380 adult CVD individuals with 1068 unique PheCodes, we successfully identified 14 subphenotypes. Through the association analysis with estimated CVD risk for each subtype, we found some phenotypic topics such as Vitamin D deficiency and depression, Urinary infections cannot be explained by the conventional risk factors. Through a survival analysis, we found markedly different risks of subsequent MI following the diagnosis of CVD among the six most prevalent topics (p < 0.0001), indicating these topics may capture clinically meaningful subphenotypes of CVD. Conclusion This study demonstrates the potential benefits of using tensor-decomposition to model diseases as dynamic processes from longitudinal EHR data. Our results suggest that this data-driven approach may potentially help researchers identify complex and chronic disease subphenotypes in precision medicine research."
Detection and classification of faults in pitch-regulated wind turbine generators using normal behaviour models based on performance curves,"The fast growing wind industry requires a more sophisticated fault detection approach in pitch-regulated wind turbine generators (WTG), particularly in the pitch system that has led to the highest failure frequency and downtime. Improved analysis of data from Supervisory Control and Data Acquisition (SCADA) systems can be used to generate alarms and signals that could provide earlier indication of WTG faults and allow operators to more effectively plan Operation and Maintenance (O&M) strategies prior to WTG failures. Several data-mining approaches, e.g. Artificial Neural Network (ANN), and Normal Behaviour Models (NBM) have been used for that purpose. However, practical applications are limited because of the SCADA data complexity and the lack of accuracy due to the use of SCADA data averaged over a period of 10 min for ANN training. This paper aims to propose a new pitch fault detection procedure using performance curve (PC) based NBMs. An advantage of the proposed approach is that the system consisting of NBMs and criteria, can be developed using technical specifications of studied WTGs. A second advantage is that training data is unnecessary prior to application of the system. In order to construct the proposed system, details of WTG operational states and PCs are studied. Power-generator speed (P-N) and pitch angle-generator speed (PA-N) curves are selected to set up NBMs due to the better fit between the measured data and theoretical PCs. Six case studies have been carried out to show the prognosis of WTG fault and to demonstrate the feasibility of the proposed method. The results illustrate that polluted slip rings and the pitch controller malfunctions could be detected by the proposed method 20 h and 13 h earlier than by the AI approaches investigated and the existing alarm system. In addition, the proposed approach is able to explain and visualize abnormal behaviour of WTGs during the fault conditions."
Detection and localization of spatially correlated point landmarks in medical images using an automatically learned conditional random field,"The automatic detection and accurate localization of landmarks is a crucial task in medical imaging. It is necessary for tasks like diagnosis, surgical planning, and post-operative assessment. A common approach to localize multiple landmarks is to combine multiple independent localizers for individual landmarks with a spatial regularizer, e.g., a conditional random field (CRF). Its configuration, e.g., the CRF topology and potential functions, often has to be manually specified w.r.t. the application. In this paper, we present a general framework to automatically learn the optimal configuration of a CRF for localizing multiple landmarks. Furthermore, we introduce a novel “missing” label for each landmark (node in the CRF). The key idea is to define a pool of potentials and optimize their CRF weights and the potential values for missing landmarks in a learning framework. Potentials with a low weight are removed, thus optimizing the graph topology. This allows to easily transfer our framework to new applications, and to integrate different localizers. Further advantages of our algorithm are its low test runtime, low amount of training data, and interpretability. We illustrate its feasibility in a detailed evaluation on three medical datasets featuring high degrees of pathologies and outliers."
Detection of respiratory diseases from chest X rays using Nesterov accelerated adaptive moment estimation,"Recent developments in the field of machine learning have led to drastic improvements in medical diagnosis. Identification of different medical conditions with high accuracy is possible through machine learning, specifically deep learning. Convolutional Neural Networks are a subset of deep neural networks, used in investigating visual images. In this study, a method to identify bacterial pneumonia, viral pneumonia and COVID-19 from chest X-rays is proposed using convolutional neural networks. Training accuracy of 0.9440 and validation accuracy of 0.9356 was obtained using this model. The test accuracy was found to be 0.8753. As a matter of fact, COVID-19 diagnosing precision and recall of the proposed method are 0.95 and 1.00 respectively. Significant improvements are seen when compared to other approaches."
Detection of transformer mechanical deformations by comparing different FRA connections,"Frequency response analysis (FRA) is an effective technique to detect mechanical faults inside a power transformer. The FRA interpretation is normally carried out by comparing an FRA trace corresponding to a specific connection scheme with a fingerprint of its type. This paper, by contrast, proposes a novel method based on comparing FRA traces of different connection schemes recorded from the same winding to detect the mechanical deformations. In this contribution, the theoretical background is explained, and the experimental results are addressed to support the functionality of the proposed algorithm. Furthermore, a circuit model of the windings is created, and the simulation results are also used to validate the experimental measurements. Finally, two case studies describe the application of the new method in the condition assessment of transformers to achieve a more reliable decision. Although more research is needed to support the new applications, the early potential of the method is shown in this contribution."
Determining spectral groups to distinguish oil emulsions from Sargassum over the Gulf of Mexico using an airborne imaging spectrometer,"During the weathering of marine-spilled oils, various types of oil pollution are formed that can harm marine and coastal environments. Thus, the remote detection, classification and quantification of spilled oils is important in marine environmental monitoring. Although multispectral images can be used to observe various spilled oils, due to confusion between the multispectral backscattered signals, distinguishing spilled oils from floating algae in the same image is challenging. The spectral features of carbon-hydrogen (-C-H) and oxygen-hydrogen (-O-H) groups, and pigments, are diagnostic absorption features and are different from the backscattering signal, they have not been used to improve detection independently. In this study, all the spectral features of the groups were clearly interpreted using reflectance spectra collected from an airborne visible infrared imaging spectrometer (AVIRIS). A reflectance peak-trough detection method to characterize the different spectral groups was used to determine the spectral features of Deepwater Horizon (DWH) oil emulsions and floating Sargassum in the Gulf of Mexico (GOM). The results show that the spilled oils and floating Sargassum can be clearly identified, and the various spilled oils (i.e., different oil emulsions and oil slicks) could also be determined from the differences in the spectral features of the above groups. Finally, we discuss the spectral requirements for the identification of these groups and we conclude that optical remote sensing, including imaging spectrometers, will play an increasingly important role in assessing marine oil spills."
Developing social-ecological system indicators using group model building,"In many coastal regions, activities of multiple users present a growing strain on the ecological state of the area. The necessity of using integrative system approaches to understand and solve coastal problems has become obvious in the last decades. Integrated management strategies for social-ecological systems (SESs) call for the development of SES indicators that help (i) to identify and link the states and processes of social, economic and ecological subsystems and (ii) to balance different stakeholder objectives over the long-term within natural limits. Here we use a system dynamics modeling approach called group model building (GMB) as a diagnostic participative tool for understanding the determinants of characteristic SES issues in the Dutch Wadden Sea region and exploring salient SES indicators for management. We used GMB in two separate workshops for two distinct cases: sustainable mussel fisheries and tourism development. Follow-up online questionnaires elicited relevant variables for deriving SES indicators. In both modeling cases participants identified and connected the variables that expressed fundamental SES dynamics driving each issue. In the mussel fisheries model the central part of the structure was the interaction between the model variables ‘extent of mussel habitat with high natural value’, ‘mussel cultivation efficiency’, and ‘market supply’. In the tourism model a key driving force for explaining tourist development was the reciprocal relation between the model variables ‘natural value’, ‘experience value’, and ‘number of tourists’. Application of GMB revealed SES issue complexity and explicitly identified key linkages and potential SES indicators for policy and management in the Dutch Wadden Sea area. As a tool for stakeholder involvement in integrated coastal management the approach enables the joint building of system understanding and the exchange of individual perspectives. Participants agreed with the jointly created models and highly appreciated the way the structured approach facilitated communication and learning about complex and contested issues."
Development and application of the entropy production diagnostic model to the cavitation flow of a pump-turbine in pump mode,"A novel mathematical model, entropy production diagnostic model (EPDM) with phase transition (EPDMS), was developed, which is practical to analyze the irreversible loss for cavitation flow in hydraulic machinery by including mass transfer and slip velocity. To model the corresponding flow, two-fluid model with main interphase forces was employed. Compared with the EPDM in homogeneous model, two extra contributions, diffusion and interface entropy production (DifE, IntE), are added in EPDMS. Then, EPDMS was validated by experiments of a pump-turbine in pump mode at three off-design conditions. The improved numerical model is more accurate in capturing the typical cavitation points, while the original one is obviously lower due to poor consideration of phase interaction. Therefore, the loss predicted by EPDMS is more rational. A detailed comparison among four overlapping items of EPDM and EPDMS was performed. It shows that these items merely interpret the accumulative effect of cavitation on the flow, while as a key part of EPDMS, IntE is closely related to the evolution of cavitation. Moreover, low-frequency excitation that caused by cavitation-induced vortex is captured by the probe at the edge of the cavity, which is consistent with the frequency-domain characteristics of IntE instead of other items."
Development and experimentation of a new digital communicating and intelligent stethoscope,"Introduction We have previously developed a communicating and intelligent electronic digital stethoscope. Part of the developments has been conducted in collaborative and institutional projects. These research projects resulted in the implementation of a Bluetooth electronic stethoscope (eStetho), together with signal processing and data communication techniques. Materials and methods The main objective of the present work was the validation of the technological and medical choices by healthcare professionals, in a university hospital setting. For that, eStetho has been compared with acoustic stethoscopes in two prospective studies: the PRI stéthoscope study, including more than 160 patients, and an education study, including 30 students. Results In this analysis, the eStetho system allowed us to have a correct diagnosis in 10% more cases than with an acoustic stethoscope. An improvement of good diagnoses (recognition of heart and respiratory sounds) between 20 to 30% is also observed with 30 students of the second cycle of their medical studies. Discussion Whilst conventional stethoscope auscultation is subjective, hardly sharable and interpreted by a single clinician, the characterization and identification of sounds by computer-based recording and analysis systems provide objective and early diagnostic help along with better sensitivity and reproducibility when interpreting findings. Conclusions The eStetho system allows for the optimization of auscultatory findings, hence achieving a correct diagnosis by physically characterizing sounds through recordings, visualization and automated analysis systems. Résumé Introduction Nous avons précédemment mis au point un stéthoscope électronique numérique communicant et intelligent. Une partie des développements a été menée dans des projets collaboratifs et institutionnels. Ces projets de recherche ont abouti à la mise au point d’un stéthoscope électronique Bluetooth (eStetho), associé à des techniques de traitement du signal et de la communication de données. Matériels et méthodes L’objectif principal du présent travail est la validation des choix technologiques et médicaux par des professionnels de la santé dans le cadre d’un centre hospitalo-universitaire. Pour cela, le system eStetho a été comparé à des stéthoscopes acoustiques dans deux études prospectives : l’étude PRI stéthoscope, incluant plus de 160 patients et une étude pour tester l’apport éducationnel, incluant 30 étudiants. Résultats Dans cette analyse, le système eStetho nous a permis d’avoir un diagnostic correct dans 10 % des cas de plus qu’avec les stéthoscopes acoustiques. Une amélioration entre 20 à 30 % de ces bons diagnostics (identification des sons cardiaques et pulmonaires) a également été observée chez 30 étudiants du deuxième cycle des études médicales. Discussion Alors que l’auscultation à l’aide d’un stéthoscope conventionnel est subjective, non partageable, et interprétée par un seul clinicien, la caractérisation et l’identification des sons, leur enregistrement et l’analyse par des systèmes informatiques fournissent une aide diagnostique objective et précoce ainsi qu’une meilleure sensibilité et reproductibilité lors de l’interprétation des résultats. Conclusions Le système eStetho permet une optimisation de l’auscultation et la réalisation d’un diagnostic correct en caractérisant physiquement les sons à travers des enregistrements, la visualisation et des systèmes d’analyse automatisée."
Development of a computer-interpretable clinical guideline model for decision support in the differential diagnosis of hyponatremia,"Introduction Hyponatremia is the most common type of electrolyte imbalance, occurring when serum sodium is below threshold levels, typically 135mmol/L. Electrolyte balance has been identified as one of the most challenging subjects for medical students, but also as one of the most relevant areas to learn about according to physicians and researchers. We present a computer-interpretable guideline (CIG) model that will be used for medical training to learn how to improve the diagnosis of hyponatremia applying an expert consensus document (ECDs). Methods We used the PROForma set of tools to develop the model, using an iterative process involving two knowledge engineers (a computer science Ph.D. and a preventive medicine specialist) and two expert endocrinologists. We also carried out an initial validation of the model and a qualitative post-analysis from the results of a retrospective study (N=65 patients), comparing the consensus diagnosis of two experts with the output of the tool. Results The model includes over two-hundred “for”, “against” and “neutral” arguments that are selectively triggered depending on the input value of more than forty patient-state variables. We share the methodology followed for the development process and the initial validation results, that achieved a high ratio of 61/65 agreements with the consensus diagnosis, having a kappa value of K=0.86 for overall agreement and K=0.80 for first-ranked agreement. Conclusion Hospital care professionals involved in the project showed high expectations of using this tool for training, but the process to follow for a successful diagnosis and application is not trivial, as reported in this manuscript. Secondary benefits of using these tools are associated to improving research knowledge and existing clinical practice guidelines (CPGs) or ECDs. Beyond point-of-care clinical decision support, knowledge-based decision support systems are very attractive as a training tool, to help selected professionals to better understand difficult diseases that are underdiagnosed and/or incorrectly managed."
Development of a diagnostic and prognostic tool for predictive maintenance in the railcar industry,"The use of a diagnostic and prognostic tool for predictive maintenance serves as a continuous inspection, detective and predictive tool for making important decisions on maintenance activities before failure occurs. The prediction of failure is important in the railway industry in reducing the maintenance and operating cost, minimizing interruptions, risk, unscheduled maintenance and accidents while enhancing higher productivity and component lifespan. In this study, a diagnostic and prognostic tool was developed to constantly monitor and predict the rate of degradation and Remaining Useful Life (RUL) of a railcar wheel bearing. The tool uses the envelope spectrum and kurtosis analysis, which employs the wheel acceleration data obtained from a primary source and the data was interpreted with the aid of statistical and computational methods. The input data was first pre-processed and important features are extracted in a MATLAB 2018b environment. The extracted features were thereafter integrated into the diagnostic and prognostic tool with a pre-set threshold value or feature for the wheel acceleration for predictive purpose. The results obtained indicate the suitability of the diagnosis and prognostic tool for the determination of the railcar wheel condition, prediction of the Mean Time to Failure (MTTF), as well as the remaining useful life of the railcar bearing."
Development of a Reinforcement Learning-based Evolutionary Fuzzy Rule-Based System for diabetes diagnosis,"The early diagnosis of disease is critical to preventing the occurrence of severe complications. Diabetes is a serious health problem. A variety of methods have been developed for diagnosing diabetes. The majority of these methods have been developed in a black-box manner, which cannot be used to explain the inference and diagnosis procedure. Therefore, it is essential to develop methods with high accuracy and interpretability. In this study, a Reinforcement Learning-based Evolutionary Fuzzy Rule-Based System (RLEFRBS) is developed for diabetes diagnosis. The proposed model involves the building of a Rule Base (RB) and rule optimization. The initial RB is constructed using numerical data without initial rules; after learning the rules, redundant rules are eliminated based on the confidence measure. Next, redundant conditions in the antecedent parts are pruned to yield simpler rules with higher interpretability. Finally, an appropriate subset of the rules is selected using a Genetic Algorithm (GA), and the RB is constructed. Evolutionary tuning of the membership functions and weight adjusting using Reinforcement Learning (RL) are used to improve the performance of RLEFRBS. Moreover, to deal with uncovered instances, it makes use of an efficient rule stretching method. The performance of RLEFRBS was examined using two common datasets: Pima Indian Diabetes (PID) and BioSat Diabetes Dataset (BDD). The experimental results show that the proposed model provides a more compact, interpretable and accurate RB that can be considered to be a promising alternative for diagnosis of diabetes."
Developments in the “profil cultural” method for an improved assessment of soil structure under no-till,"In France, agronomists have studied the effects of cropping systems on soil structure using a field method that is based on a visual description of the soil structure. This “profil cultural” method was designed as a field diagnostic tool to identify the effects of tillage and compaction on soil structure dynamics. It is of great benefit to agronomists seeking to improve crop management and preserve soil structure and fertility. However, the “profil cultural” method was developed and has mainly been used in conventional tillage systems with regular ploughing. As there has been an increase in the use of various forms of reduced, minimum and no-tillage systems in many parts of the world, it is necessary to re-evaluate this method’s ability to describe and interpret soil structure dynamics in no-till or reduced tillage. In these situations, changes in soil structure over time are mainly driven by compaction and by regeneration through natural agents (climatic conditions, root growth and macrofauna), therefore it is important to evaluate the effects of these natural processes on soil structure dynamics. These concerns have led to adaptations and amendments to the initial method based on field observations and experimental work in different cropping systems, soil types and climatic conditions. The description of crack types has been improved and a criterion of biological activity based on the visual examination of clods has been introduced. To test this modified method, a comparison with the initial method was undertaken and its ability to make diagnoses tested in five experiments in France, Brazil and Argentina. The adapted method allowed an improved assessment of the impact of cropping systems on soil functioning when natural processes were integrated into the description."
Diagnosability improvement of dynamic clustering through automatic learning of discrete event models,"This paper deals with the problem of improving data-based diagnosis of continuous systems taking advantage of the system control information represented as discrete event dynamics. The approach starts from dynamic clustering results and, combining the information about operational modes, automatically generates a discrete event system that improves clustering results interpretability for decision-making purposes and enhances fault detection capabilities by the inclusion of event related dynamics. The generated timed discrete event system is adaptive thanks to the dynamic nature of the clusterer from which it was learned, namely DyClee. The timed discrete event system brings valuable temporal information to distinguish behaviors that are non-diagnosable based solely on the clustering itself."
Diagnosis algorithms for indirect structural health monitoring of a bridge model via dimensionality reduction,"We present a data-driven approach based on physical insights to achieve damage diagnosis of bridges using only vibration signals collected on board the vehicles passing over the bridge. Though data-driven models have been shown to produce promising results in this context, they generally require labeled examples to fit the models (i.e., supervised learning) and make it difficult to interpret the physical mechanisms. We posit that these shortcomings can be alleviated by studying the physical relationship between damage and the distribution of the resulting acceleration signals, and then choosing an appropriate model to invert this process. To help guide the development of appropriate damage diagnosis algorithms, we first make use of the theoretical formulation of the vehicle-bridge interaction system in the frequency domain and conduct a finite element simulation of this system. From the derived numerical solution, we observe that not only is the trend of the acceleration signals of a passing vehicle with different damage severity non-linear, but also that both the low- and high-frequency responses of a passing vehicle contain information about damage severity. Guided by these observations, we use several dimensionality reduction methods to extract representative features from the vehicle’s vibration response. We then propose an unsupervised damage severity comparison model and a semi-supervised damage severity estimation model aiming at indirect monitoring of bridges. We apply the algorithms to diagnose changes that occur in a laboratory bridge model to which a concentrated mass of gradually changing magnitude is attached at mid-span. The experimental results of the damage severity comparison and estimation show that a non-convex and non-linear dimensionality reduction technique (stacked autoencoders) outperforms other linear and/or convex dimensionality reduction techniques. Overall, our results provide evidence for the applicability of indirect structural health monitoring in bridge models and suggest the feasibility of extending this approach to actual structures."
Diagnosis of autism through EEG processed by advanced computational algorithms: A pilot study,"Background. Multi-Scale Ranked Organizing Map coupled with Implicit Function as Squashing Time algorithm(MS-ROM/I-FAST) is a new, complex system based on Artificial Neural networks (ANNs) able to extract features of interest in computerized EEG through the analysis of few minutes of their EEG without any preliminary pre-processing. A proof of concept study previously published showed accuracy values ranging from 94%-98% in discerning subjects with Mild Cognitive Impairment and/or Alzheimer's Disease from healthy elderly people. The presence of deviant patterns in simple resting state EEG recordings in autism, consistent with the atypical organization of the cerebral cortex present, prompted us in applying this potent analytical systems in search of a EEG signature of the disease. Aim of the study. The aim of the study is to assess how effectively this methodology distinguishes subjects with autism from typically developing ones. Methods. Fifteen definite ASD subjects (13 males; 2 females; age range 7–14; mean value = 10.4) and ten typically developing subjects (4 males; 6 females; age range 7–12; mean value 9.2) were included in the study. Patients received Autism diagnoses according to DSM-V criteria, subsequently confirmed by the ADOS scale. A segment of artefact-free EEG lasting 60 seconds was used to compute input values for subsequent analyses. MS-ROM/I-FAST coupled with a well-documented evolutionary system able to select predictive features (TWIST) created an invariant features vector input of EEG on which supervised machine learning systems acted as blind classifiers. Results. The overall predictive capability of machine learning system in sorting out autistic cases from normal control amounted consistently to 100% with all kind of systems employed using training-testing protocol and to 84% - 92.8% using Leave One Out protocol. The similarities among the ANN weight matrixes measured with apposite algorithms were not affected by the age of the subjects. This suggests that the ANNs do not read age-related EEG patterns, but rather invariant features related to the brain's underlying disconnection signature. Conclusion. This pilot study seems to open up new avenues for the development of non-invasive diagnostic testing for the early detection of ASD."
Diagnosis of learner dropout based on learning styles for online distance learning,"The amount of data generated by computer systems in Online Distance Learning (ODL) contains rich information. One example of this information we define as the Learner Learning Trail (LLT), which is the sequence of interactions between the students and the virtual environment. Another example is the Learner Learning Style (LLS), which is associated with the student behavior and choices during the learning process. This information can be used to identify learner behavior and learning style. We perceived, after the study of related literature, that the research field of learner diagnosis for ODL does not apply the conjoint use of LLT and LLS. In this article, we propose a model capable of integrating data generated from the behavior of students in ODL with cognitive aspects of them, such as their Learning Styles, by crossing LLT with LLS. We also propose the CPAD method (Collect, Preprocessing, Analysis, Diagnosis), which is implemented by collecting the raw data regarding learning activities, preprocessing the data into structured time sequences, analyzing the sequences regarding the learning styles and using this analysis to diagnose the learner behavior. We selected the dropout to investigate, once the dropout rate in ODL is a real problem in universities around the world. In addition, the dropout is a student decision which can be associated with previous students behaviors. We performed a study with 202 learners to evaluate if learning styles are capable of explaining aspects of the student behavior. The results suggest that Sequential/Global learning style dimension is more capable of explaining the dropout than the other dimensions. Also, we performed four classification experiments to verify how the dimensions of Felder-Silverman Learning Style Model influence the learner diagnosis. We perceived that the Sequential/Global dimension could provide a higher accuracy average with lower variation independently of the diagnosis technique."
Diagnostic concordance between mobile interfaces and conventional workstations for emergency imaging assessment,"Introduction Mobile devices and software are now available with sufficient computing power, speed and complexity to allow for real-time interpretation of radiology exams. In this paper, we perform a multivariable user study that investigates concordance of image-based diagnoses provided using mobile devices on the one hand and conventional workstations on the other hand. Methods We performed a between-subjects task-analysis using CT, MRI and radiography datasets. Moreover, we investigated the adequacy of the screen size, image quality, usability and the availability of the tools necessary for the analysis. Radiologists, members of several teams, participated in the experiment under real work conditions. A total of 64 studies with 93 main diagnoses were analyzed. Results Our results showed that 56 cases were classified with complete concordance (87.69%), 5 cases with almost complete concordance (7.69%) and 1 case (1.56%) with partial concordance. Only 2 studies presented discordance between the reports (3.07%). The main reason to explain the cause of those disagreements was the lack of multiplanar reconstruction tool in the mobile viewer. Screen size and image quality had no direct impact on the mobile diagnosis process. Conclusion We concluded that for images from emergency modalities, a mobile interface provides accurate interpretation and swift response, which could benefit patients' healthcare."
Diagnostic spectroscopic tools for worn brake pad materials: A case study,"Polymeric composites represent an advantageous choice for brake pads materials, offering a good balance between costs and performances. In this work, composite brake pads based on barite, potassium titanate and aramid fibers embedded in a polymeric matrix of phenol formaldehyde resin have been wear-tested in laboratory against a class FC-250 cast iron using a car brake simulator and then characterize using different microscopic and spectroscopic techniques. Four main wear mechanisms were observed depending on brake pad composition, as follows: tribolayer formation, ceramic crystal pull-out, cleavage of the C–N bond in aramid pulp fibres, and dissolution of barite to produce either ammonium or potassium sulphate. In the presence of potassium titanate, both wear rate and tribolayer area coverage were clearly reduced. Spectroscopic techniques enabled us to resolve the physicochemical degradation mechanisms behind brake performance, suggesting their potential use as in situ probes."
Diagnostics for laser cutting efficiency using computational fluid dynamics,"In laser cutting processes, the air nozzle, used to evacuate the material molten by the laser, has a strong influence on the cutting quality and efficiency. The present paper proposes flow diagnostics to evaluate this efficiency based on the numerical evaluation of the flow aerodynamic around the nozzle and its interaction with the kerf. The computational domain includes the nozzle reservoir, where the pressure is imposed, the nozzle and the metal plate with its kerf. The flow diagnostics are based on Mach and momentum iso-contours, mass balancing and wall-shear stress on the kerf front. This allows discriminating between different nozzle configurations and to identify the most efficient aerodynamically. An application on four different nozzles is proposed, and results showed that the cut quality observed experimentally in real working conditions may be explained by the diagnostics put in place to evaluate the cutting efficiency."
Diagnostics of a spark-discharge coupled to laser aluminum plasma by optical emission spectroscopy and ion time-of-flight,"Optical emission and ion time-of-flight are measured for a spark-coupled laser aluminum plasma. A Q-switched Nd:YAG laser (wavelength λ = 1064 nm, pulse width τ ∼ 7 ns, pulse energy Ep ≤ 260 mJ, intensity I ≤ 15 × 109 W/cm2) generates the Al plasma, while a synchronized spark-discharge enhances the ion flux and charge state. Time-integrated, spatially-resolved optical spectra are used to obtain the plasma excitation temperature Te and density ne. The coupling of 2.4 J of spark-discharge energy to the laser plasma enhances the optical emission line intensity. The effective ion temperature Ti is calculated from a shifted Maxwell-Boltzmann distribution fit of the time-of-flight signal deconvolved for each ion charge. For I = 3.5 × 109 W/cm2, Ti is ∼15 eV. For a spark energy of 2.4 J coupled to the laser plasma, Ti increases to ∼50 eV and up to Al8+ is identified from the ion time-of-flight signal. The Ti obtained from the ion time-of-flight is much larger than Te obtained from optical spectroscopy, although the plasma is considered to be in local thermodynamic equilibrium. This result is explained in view of the temporal development of the ablation plume and the different plasma regions probed by the two methods."
Digital cardiotocography: What is the optimal sampling frequency?,"Cardiotocography (CTG) is the most popular prenatal diagnostic test for establishing fetal health and consists in simultaneous recording of fetal heart rate (FHR, bpm) and maternal uterine contraction (UC, mmHg) traces. Typically, FHR and UC traces are visually analyzed and interpreted by clinicians. Recently, software applications like CTG Analyzer have been developed to support visual CTG interpretation by making it more objective and independent from clinician’s experience. Automatic CTG analysis requires CTG-traces digitalization and thus assessment of a correct sampling frequency (SF). Thus, this paper aims to investigate dependency of automatic CTG analysis on SF in order to identify optimal SF (OSF) for FHR and UC traces that minimizes computational efforts without jeopardizing CTG interpretation. To this aim, the “CTU-CHB intra-partum CTG database” was considered and visually annotated by an expert gynecologist. FHR and UC traces, originally sampled at 4 Hz, were down sampled at 2 Hz, 1 Hz, 0.4 Hz and 0.2 Hz, and automatically analyzed using CTG Analyzer. Eventually, results obtained through automatic analysis were compared to visual annotations, which were taken as reference. A cumulative statistical index (CSI), ranging from 0.00% to 100.00%, was defined as a linear combination of positive-predictive value, sensitivity, false-positive rate and false-negative rate. OSF was defined as the one that maximizes CSI. If CSI was showing the same value for more than one SF, the lowest SF was selected as the optimal since minimizing computational efforts. Results indicate that OSF for FHR is 2 Hz (CSI ≥ 85.41%), whereas OSF for UC is 0.2 Hz (CSI = 75.21%)."
"Digital twin, physics-based model, and machine learning applied to damage detection in structures","This work is interested in digital twins, and the development of a simplified framework for them, in the context of dynamical systems. Digital twin is an ingenious concept that helps on organizing different areas of expertise aiming at supporting engineering decisions related to a specific asset; it articulates computational models, sensors, learning, real time analysis, diagnosis, prognosis, and so on. In this framework, and to leverage its capacity, we explore the integration of physics-based models with machine learning. A digital twin is constructed for a damaged structure, where a discrete physics-based computational model is employed to investigate several damage scenarios. A machine learning classifier, that serves as the digital twin, is trained with data taken from a stochastic computational model. This strategy allows the use of an interpretable model (physics-based) to build a fast digital twin (machine learning) that will be connected to the physical twin to support real time engineering decisions. Different classifiers (quadratic discriminant, support vector machines, etc) are tested, and different model parameters (number of sensors, level of noise, damage intensity, uncertainty, operational parameters, etc) are considered to construct datasets for the training. The accuracy of the digital twin depends on the scenario analyzed. Through the chosen application, we are able to emphasize each step of a digital twin construction, including the possibility of integrating physics-based models with machine learning. The different scenarios explored yield conclusions that might be helpful for a large range of applications."
Dimension reduction in nonparametric models of production,"It is well-known that the convergence rates of nonparametric efficiency estimators (e.g., free-disposal hull and data envelopment analysis estimators) become slower with increasing numbers of input and output quantities (i.e., dimensionality). Dimension reduction is often utilized in non-parametric density and regression where similar problems occur, but has been used in only a few instances in the context of efficiency estimation. This paper explains why the problem occurs in nonparametric models of production and proposes three diagnostics for when dimension reduction might lead to more accurate estimation of efficiency. Simulation results provide additional insight, and suggest that in many cases dimension reduction is advantageous in terms of reducing estimation error. The simulation results also suggest that when dimensionality is reduced, free-disposal hull estimators become an attractive, viable alternative to the more frequently used (and more restrictive) data envelopment analysis estimators. In the context of efficiency estimation, these results provide the first quantification of the tradeoff between information lost versus improvement in estimation error due to dimension reduction. Results from several papers in the literature are revisited to show what might be gained from reducing dimensionality and how interpretations might differ."
Domain-invariant interpretable fundus image quality assessment,"Objective and quantitative assessment of fundus image quality is essential for the diagnosis of retinal diseases. The major factors in fundus image quality assessment are image artifact, clarity, and field definition. Unfortunately, most of existing quality assessment methods focus on the quality of overall image, without interpretable quality feedback for real-time adjustment. Furthermore, these models are often sensitive to the specific imaging devices, and cannot generalize well under different imaging conditions. This paper presents a new multi-task domain adaptation framework to automatically assess fundus image quality. The proposed framework provides interpretable quality assessment with both quantitative scores and quality visualization for potential real-time image recapture with proper adjustment. In particular, the present approach can detect optic disc and fovea structures as landmarks, to assist the assessment through coarse-to-fine feature encoding. The framework also exploit semi-tied adversarial discriminative domain adaptation to make the model generalizable across different data sources. Experimental results demonstrated that the proposed algorithm outperforms different state-of-the-art approaches and achieves an area under the ROC curve of 0.9455 for the overall quality classification."
DR|GRADUATE: Uncertainty-aware deep learning-based diabetic retinopathy grading in eye fundus images,"Diabetic retinopathy (DR) grading is crucial in determining the adequate treatment and follow up of patient, but the screening process can be tiresome and prone to errors. Deep learning approaches have shown promising performance as computer-aided diagnosis (CAD) systems, but their black-box behaviour hinders clinical application. We propose DR|GRADUATE, a novel deep learning-based DR grading CAD system that supports its decision by providing a medically interpretable explanation and an estimation of how uncertain that prediction is, allowing the ophthalmologist to measure how much that decision should be trusted. We designed DR|GRADUATE taking into account the ordinal nature of the DR grading problem. A novel Gaussian-sampling approach built upon a Multiple Instance Learning framework allow DR|GRADUATE to infer an image grade associated with an explanation map and a prediction uncertainty while being trained only with image-wise labels. DR|GRADUATE was trained on the Kaggle DR detection training set and evaluated across multiple datasets. In DR grading, a quadratic-weighted Cohen’s kappa (κ) between 0.71 and 0.84 was achieved in five different datasets. We show that high κ values occur for images with low prediction uncertainty, thus indicating that this uncertainty is a valid measure of the predictions’ quality. Further, bad quality images are generally associated with higher uncertainties, showing that images not suitable for diagnosis indeed lead to less trustworthy predictions. Additionally, tests on unfamiliar medical image data types suggest that DR|GRADUATE allows outlier detection. The attention maps generally highlight regions of interest for diagnosis. These results show the great potential of DR|GRADUATE as a second-opinion system in DR severity grading."
Drug-induced cellular death dynamics monitored by a highly sensitive organic electrochemical system,"We propose and demonstrate a sensitive diagnostic device based on an Organic Electrochemical Transistor (OECT) for direct in-vitro monitoring cell death. The system efficiently monitors cell death dynamics, being able to detect signals related to specific death mechanisms, namely necrosis or early/late apoptosis, demonstrating a reproducible correlation between the OECT electrical response and the trends of standard cell death assays. The innovative design of the Twell-OECT system has been modeled to better correlate electrical signals with cell death dynamics. To qualify the device, we used a human lung adenocarcinoma cell line (A549) that was cultivated on the micro-porous membrane of a Transwell (Twell) support, and exposed to the anticancer drug doxorubicin. Time-dependent and dose-dependent dynamics of A549 cells exposed to doxorubicin are evaluated by monitoring cell death upon exposure to a range of doses and times that fully covers the protocols used in cancer treatment. The demonstrated ability to directly monitor cell stress and death dynamics upon drug exposure using simple electronic devices and, possibly, achieving selectivity to different cell dynamics is of great interest for several application fields, including toxicology, pharmacology, and therapeutics."
Dual-branch combination network (DCN): Towards accurate diagnosis and lesion segmentation of COVID-19 using CT images,"The recent global outbreak and spread of coronavirus disease (COVID-19) makes it an imperative to develop accurate and efficient diagnostic tools for the disease as medical resources are getting increasingly constrained. Artificial intelligence (AI)-aided tools have exhibited desirable potential; for example, chest computed tomography (CT) has been demonstrated to play a major role in the diagnosis and evaluation of COVID-19. However, developing a CT-based AI diagnostic system for the disease detection has faced considerable challenges, which is mainly due to the lack of adequate manually-delineated samples for training, as well as the requirement of sufficient sensitivity to subtle lesions in the early infection stages. In this study, we developed a dual-branch combination network (DCN) for COVID-19 diagnosis that can simultaneously achieve individual-level classification and lesion segmentation. To focus the classification branch more intensively on the lesion areas, a novel lesion attention module was developed to integrate the intermediate segmentation results. Furthermore, to manage the potential influence of different imaging parameters from individual facilities, a slice probability mapping method was proposed to learn the transformation from slice-level to individual-level classification. We conducted experiments on a large dataset of 1202 subjects from ten institutes in China. The results demonstrated that 1) the proposed DCN attained a classification accuracy of 96.74% on the internal dataset and 92.87% on the external validation dataset, thereby outperforming other models; 2) DCN obtained comparable performance with fewer samples and exhibited higher sensitivity, especially in subtle lesion detection; and 3) DCN provided good interpretability on the loci of infection compared to other deep models due to its classification guided by high-level semantic information. An online CT-based diagnostic platform for COVID-19 derived from our proposed framework is now available."
Dynamic visualization of multi-level molecular data: The Director package in R,"Background and objective: High-throughput measurement technologies have triggered a rise in large-scale cancer studies containing multiple levels of molecular data. While there are a number of efficient methods to analyze individual data types, there are far less that enhance data interpretation after analysis. We present the R package Director, a dynamic visualization approach to linking and interrogating multiple levels of molecular data after analysis for clinically meaningful, actionable insights. Methods: Sankey diagrams are traditionally used to represent quantitative flows through multiple, distinct events. Regulation can be interpreted as a flow of biological information through a series of molecular interactions. Functions in Director introduce novel drawing capabilities to make Sankey diagrams robust to a wide range of quantitative measures and to depict molecular interactions as regulatory cascades. The package streamlines creation of diagrams using as input quantitative measurements identifying nodes as molecules of interest and paths as the interaction strength between two molecules. Results: Director’s utility is demonstrated with quantitative measurements of candidate microRNA-gene networks identified in an ovarian cancer dataset. A recent study reported eight miRNAs as master regulators of signature genes in epithelial-mesenchymal transition (EMT). The Sankey diagrams generated with data from this study furthers interpretation of the miRNAs’ roles by revealing potential co-regulatory behavior in the extracellular matrix (ECM). An additional analysis identified 32 genes differentially expressed between good and poor prognosis patients in four significant pathways (FDR  ≤  0.1), three of which support a complementary role of the ECM in ovarian cancer. The resulting diagram created with Director suggest elevated levels of COL11A1, INHBA, and THBS2 — a signature feature of metastasis [1] — and decreased levels of their targeting miRNAs define poor prognosis. Conclusion: We have demonstrated a visualization approach suitable for implementation in an analysis workflow, linking multiple levels of molecular data to gain novel perspective on candidate biomarkers in a complex disease. The diagrams are dynamic, easily replicable, and rendered locally as HTML files to facilitate sharing. The R package Director is simple to use and widely available on all operating systems through Bioconductor (http://bioconductor.org/packages/Director) and GitHub (http://kzouchka.github.io/Director)."
EagleMine: Vision-guided Micro-clusters recognition and collective anomaly detection,"Given a graph with millions of nodes, what patterns exist in the distributions of node characteristics? How can we detect them and separate anomalous nodes in a way similar to human visual perception? More generally, how can we identify micro-clusters in a histogram and spot some interesting patterns? In this paper, we propose a vision-guided algorithm, EagleMine, to recognize and summarize node groups in a histogram constructed from some correlated features. EagleMine hierarchically discovers node groups, which form internally connected dense areas in the histogram, by utilizing a water-level tree with multiple resolutions according to the rule of the visual recognition. EagleMine uses the statistical hypothesis test to determine the optimal groups while exploring the tree and simultaneously performs vocabulary-based summarization. Moreover, EagleMine can identify anomalous micro-clusters, consisting of nodes that exhibit very similar and suspicious behavior, deviate away from the majority. Experiments on the real-world datasets show that our method can recognize intuitive node groups as human vision does; it achieves the best summarization performance compared to baselines. In terms of anomaly detection, EagleMine also outperforms the state-of-the-art graph-based methods with significantly improving accuracy in a micro-blog dataset. Moreover, EagleMine can be used for other applications, e.g., to detect the synchronized patterns in the temporal retweet event."
Early detection of network element outages based on customer trouble calls,"This paper deals with the issue of early detection of network element outages. Timeliness of outage detection as well as accuracy in finding outages on equipment in a telecommunication network depend on the monitoring system used and its performance. The intent of this paper is to investigate and propose a complementary solution to improve the performance of the existing systems in detecting faults earlier than it was able to do before. In developing our approach two constraints are given. The existing operational environment cannot be changed; threshold tuning and parameter changing cannot be done; furthermore no additional infrastructure investment has been planned. Hence, our approach relies on an alternative method based on a two-stage hybrid statistical and diagnostic detector which we designed in a way that exploits additional available data and avoids alarm monitoring system imperfections. The role of this detector is twofold: early detection of network element outages based on customer trouble calls and rule-based decision making for faulty-element isolation based on knowledge derived from fault and network management data. In this paper we present results of statistical analysis of trouble-reporting data. The analysis showed that the timing of customers' trouble reports and their content have information potential that can be utilized for early detection of outages. The detector is explained in detail and its accuracy and reduction delay is evaluated. The method presented can reduce the outage detection delay time by 2.33h on average observed in relation to the performance of an existing fault management process which was designed to detect outages solely on the basis of an alarm monitoring system, for the “difficulties in work” type of malfunction. We attained an overall probability of correct detection of 95.3%. Out of the total number of outages that hypothetically could be detected, by using this method we were able to detect 77.5% of cases 1h before the alarm was raised in the existing alarm system, while 23% of cases were detected 4h before the actual alarm. The approach has been tested on real telecommunication network data over the period of one year."
Ease of adoption of clinical natural language processing software: An evaluation of five systems,"Objective In recognition of potential barriers that may inhibit the widespread adoption of biomedical software, the 2014 i2b2 Challenge introduced a special track, Track 3 – Software Usability Assessment, in order to develop a better understanding of the adoption issues that might be associated with the state-of-the-art clinical NLP systems. This paper reports the ease of adoption assessment methods we developed for this track, and the results of evaluating five clinical NLP system submissions. Materials and methods A team of human evaluators performed a series of scripted adoptability test tasks with each of the participating systems. The evaluation team consisted of four “expert evaluators” with training in computer science, and eight “end user evaluators” with mixed backgrounds in medicine, nursing, pharmacy, and health informatics. We assessed how easy it is to adopt the submitted systems along the following three dimensions: communication effectiveness (i.e., how effective a system is in communicating its designed objectives to intended audience), effort required to install, and effort required to use. We used a formal software usability testing tool, TURF, to record the evaluators’ interactions with the systems and ‘think-aloud’ data revealing their thought processes when installing and using the systems and when resolving unexpected issues. Results Overall, the ease of adoption ratings that the five systems received are unsatisfactory. Installation of some of the systems proved to be rather difficult, and some systems failed to adequately communicate their designed objectives to intended adopters. Further, the average ratings provided by the end user evaluators on ease of use and ease of interpreting output are −0.35 and −0.53, respectively, indicating that this group of users generally deemed the systems extremely difficult to work with. While the ratings provided by the expert evaluators are higher, 0.6 and 0.45, respectively, these ratings are still low indicating that they also experienced considerable struggles. Discussion The results of the Track 3 evaluation show that the adoptability of the five participating clinical NLP systems has a great margin for improvement. Remedy strategies suggested by the evaluators included (1) more detailed and operation system specific use instructions; (2) provision of more pertinent onscreen feedback for easier diagnosis of problems; (3) including screen walk-throughs in use instructions so users know what to expect and what might have gone wrong; (4) avoiding jargon and acronyms in materials intended for end users; and (5) packaging prerequisites required within software distributions so that prospective adopters of the software do not have to obtain each of the third-party components on their own."
EasyMiner.eu: Web framework for interpretable machine learning based on rules and frequent itemsets,"EasyMiner (http://www.easyminer.eu) is a web-based system for interpretable machine learning based on frequent itemsets. It currently offers association rule learning (apriori, FP-Growth) and classification (CBA). EasyMiner offers a visual interface designed for interactivity, allowing the user to define a constraining pattern for the mining task. The CBA algorithm can also be used for pruning of the rule set, thus addressing the common problem of “too many rules” on the output, and the implementation supports automatic tuning of confidence and support thresholds. The development version additionally supports anomaly detection (FPI and its variations) and linked data mining (AMIE+). EasyMiner is dockerized, some of its components are available as open source R packages."
EasyMiner.eu: Web framework for interpretable machine learning based on rules and frequent itemsets,"EasyMiner (http://www.easyminer.eu) is a web-based system for interpretable machine learning based on frequent itemsets. It currently offers association rule learning (apriori, FP-Growth) and classification (CBA). EasyMiner offers a visual interface designed for interactivity, allowing the user to define a constraining pattern for the mining task. The CBA algorithm can also be used for pruning of the rule set, thus addressing the common problem of “too many rules” on the output, and the implementation supports automatic tuning of confidence and support thresholds. The development version additionally supports anomaly detection (FPI and its variations) and linked data mining (AMIE+). EasyMiner is dockerized, some of its components are available as open source R packages."
EasyMiner.eu: Web framework for interpretable machine learning based on rules and frequent itemsets,"EasyMiner (http://www.easyminer.eu) is a web-based system for interpretable machine learning based on frequent itemsets. It currently offers association rule learning (apriori, FP-Growth) and classification (CBA). EasyMiner offers a visual interface designed for interactivity, allowing the user to define a constraining pattern for the mining task. The CBA algorithm can also be used for pruning of the rule set, thus addressing the common problem of “too many rules” on the output, and the implementation supports automatic tuning of confidence and support thresholds. The development version additionally supports anomaly detection (FPI and its variations) and linked data mining (AMIE+). EasyMiner is dockerized, some of its components are available as open source R packages."
"Ebinformatics: Ebola fuzzy informatics systems on the diagnosis, prediction and recommendation of appropriate treatments for Ebola virus disease (EVD)","Ebola Virus Disease (EVD) also known as the Ebola hemorrhagic fever is a very deadly infectious disease to humankind. Therefore, a safer and complementary method of diagnosis is to employ the use of an expert system in order to initiate a platform for pre-clinical treatments, thus acting as a precursor to comprehensive medical diagnosis and treatments. This work presents a design and implementation of informatics software and a knowledge-based expert system for the diagnosis, and provision of recommendations on the appropriate type of recommended treatment to the Ebola Virus Disease (EVD). In this research an Ebola fuzzy informatics system was developed for the purpose of diagnosing and providing useful recommendations to the management of the EVD in West Africa and other affected regions of the world. It also acts as a supplementary resource in providing medical advice to individuals in Ebola – ravaged countries. This aim was achieved through the following objectives: (i) gathering of facts through the conduct of a comprehensive continental survey to determine the knowledge and perception level of the public about factors responsible for the transmission of the Ebola Virus Disease (ii) develop an informatics software based on information collated from health institutions on basic diagnosis of the Ebola Virus Disease-related symptoms (iii) adopting and marrying the knowledge of fuzzy logic and expert systems in developing the informatics software. Necessary requirements were collated from the review of existing expert systems, consultation of journals and articles, and internet sources. Online survey was conducted to determine the level at which individuals are aware of the factors responsible for the transmission of the Ebola Virus Disease (EVD). The expert system developed, was designed to use fuzzy logic as its inference mechanism along with a set of rules. A knowledge base was created to help provide diagnosis on the Ebola Virus Disease (EVD). The Root Sum Square (RSS) was adopted as a fuzzy inference method. The degree of participation of each input parameter was shown using the triangular membership function and the defuzzification technique used is the Center of Gravity (CoG). The resulting software produced a user-friendly desktop-based, Windows-based, application and the tools used were explained in the results section in three (3) separate phases. First, a comprehensive online survey was conducted over a period of about 3–9 months. 100 Participants participated in the survey on the perception and knowledge analysis of different individuals about Ebola Virus Disease (EVD) transmission factors. 31% of the participants didn't know that there is presently no cure for Ebola. 28% believed that there is presently a cure for Ebola. 43% agreed that Ebola is both air-borne and water-borne, while 33% disagreed, 24% do not know. 23% believed that insects and mosquitoes can help in transmitting the Ebola Virus Disease (EVD), while 30% were completely ignorant. We noticed that ignorance was a major limiting factor among some participants. Second, a test was conducted among 45 people. Results from a comprehensive testing of the Ebinformatics software by allowing users to operate and use the software, revealed that 60% of them were satisfied, while 16% were not satisfied with the software, while 24% were indifferent. 69% of the users were in agreement that Ebinformatics was supportive, 20% disagreed, while 11% were indifferent. 67% found the software easy to use, 13% disagreed, while 20% were indifferent. Third, the output of the software, showing the various diagnosis and recommendations interfaces were presented. Recommendations were also given with respect to how the system can be extended, and further improved upon."
EEG epochs with less alpha rhythm improve discrimination of mild Alzheimer's,"Background and objective Eyes-closed-awake electroencephalogram (EEG) is a useful tool in the diagnosis of Alzheimer's. However, there is eyes-closed-awake EEG with dominant or rare alpha rhythm. In this paper, we show that random selection of EEG epochs disregarding the alpha rhythm will lead to bias concerning EEG-based Alzheimer's Disease diagnosis. Methods We compared EEG epochs with more than 30% and with less than 30% alpha rhythm of mild Alzheimer's Disease patients and healthy elderly. We classified epochs as dominant alpha scenario and rare alpha scenario according to alpha rhythm (8–13 Hz) percentage in O1, O2 and Oz channels. Accordingly, we divided the probands into four groups: 17 dominant alpha scenario controls, 15 mild Alzheimer's patients with dominant alpha scenario epochs, 12 rare alpha scenario healthy elderly and 15 mild Alzheimer's Disease patients with rare alpha scenario epochs. We looked for group differences using one-way ANOVA tests followed by post-hoc multiple comparisons (p < 0.05) over normalized energy values (%) on the other four well-known frequency bands (delta, theta, beta and gamma) using two different electrode configurations (parieto-occipital and central). Results After carrying out post-hoc multiple comparisons, for both electrode configurations we found significant differences between mild Alzheimer's patients and healthy elderly on beta- and theta-energy (%) only for the rare alpha scenario. No differences were found for the dominant alpha scenario in any of the five frequency bands. Conclusions This is the first study of Alzheimer's awake-EEG reporting the influence of alpha rhythm on epoch selection, where our results revealed that, contrarily to what was most likely expected, less synchronized EEG epochs (rare alpha scenario) better discriminated mild Alzheimer's than those presenting abundant alpha (dominant alpha scenario). In addition, we find out that epoch selection is a very sensitive issue in qEEG research. Consequently, for Alzheimer's studies dealing with resting state EEG, we propose that epoch selection strategies should always be cautiously designed and thoroughly explained."
Effect of methane on pilot-fuel auto-ignition in dual-fuel engines,"The ignition behavior of n-dodecane micro-pilot spray in a lean-premixed methane/air charge was investigated in an optically accessible Rapid Compression-Expansion Machine at dual-fuel engine-like pressure/temperature conditions. The pilot fuel was admitted using a coaxial single-hole 100 µm injector mounted on the cylinder periphery. Optical diagnostics include combined high-speed CH2O-PLIF (10 kHz) and Schlieren (80 kHz) imaging for detection of the first-stage ignition, and simultaneous high-speed OH* chemiluminescence (40 kHz) imaging for high-temperature ignition. The aim of this study is to enhance the fundamental understanding of the interaction of methane with the auto-ignition process of short pilot-fuel injections. Addition of methane into the air charge considerably prolongs ignition delay of the pilot spray with an increasing effect at lower temperatures and with higher methane/air equivalence ratios. The temporal separation of the first CH2O detection and high-temperature ignition was found almost constant regardless of methane content. This was interpreted as methane mostly deferring the cool-flame reactivity. In order to understand the underlying mechanisms of this interaction, experimental investigations were complemented with 1D-flamelet simulations using detailed chemistry, confirming the chemical influence of methane deferring the reactivity in the pilot-fuel lean mixtures. This shifts the onset of first-stage reactivity towards the fuel-richer conditions. Consequently, the onset of the turbulent cool-flame is delayed, leading to an overall increased high-temperature ignition delay. Overall, the study reveals a complex interplay between entrainment, low T and high T chemistry and micro-mixing for dual-fuel auto-ignition processes for which the governing processes were identified."
Effect of post injections on mixture preparation and unburned hydrocarbon emissions in a heavy-duty diesel engine,"This work explores the mechanisms by which a post injection can reduce unburned hydrocarbon (UHC) emissions in heavy-duty diesel engines operating at low-temperature combustion conditions. Post injections, small, close-coupled injections of fuel after the main injection, have been shown to reduce UHC in the authors’ previous work. In this work, we analyze optical data from laser-induced fluorescence of both CH2O and OH and use chemical reactor modeling to better understand the mechanism by which post injections reduce UHC emissions. The results indicate that post-injection efficacy, or the extent to which a post injection reduces UHC emissions, is a strong function of the cylinder pressure variation during the post injection. However, the data and analysis indicate that the pressure and temperature rise from the post injection combustion cannot solely explain the UHC reduction measured by both engine-out and optical diagnostics. The fluid-mechanic, thermal, and chemical interaction of the post injection with the main-injection mixture is a key part of UHC reduction; the starting action of the post jet and the subsequent entrainment of surrounding gases are likely both important processes in reducing UHC with a post injection."
Effect of root morphology on the susceptibility of endodontically treated teeth to vertical root fracture: An ex-vivo model,"Vertical root fracture (VRF) of endodontically treated teeth is relatively common, and the involved teeth have a poor prognosis. Previous destructive methodologies applied force to the root in an uneven manner; thus, the associated experiments could not truly assess the mechanical behavior of VRF. This problem was resolved in the current study via the novel application of a bursting pressure methodology to endodontically treated maxillary central incisors and premolars. Hydrostatic pressure was applied inside the root canal through a cannula bonded to the coronal access cavity, and the apical foramen was sealed. VRFs were observed as water burst from the fractured root surface. Morphometric parameters were measured by staining and serially sectioning the roots. The bursting pressure was significantly lower in the premolars compared with that in the incisors (19.1±3.3MPa and 25.5. ±4.5MPa, respectively, p=0.001). Cracks in the roots appeared from the apex to the cement enamel junction (CEJ) (61%), apex to mid-root (26%) and mid-root to CEJ (13%), and they involved either two root surfaces (52%) or one root surface (48%) and closely resembled clinical VRF cases. Positive correlations were found between the bursting pressure and the proximal root wall thickness, whereas correlations were not observed between the bursting pressure and the buccal or lingual wall thicknesses. Statistical Analyses of Covariance (ANCOVA) models showed that the proximal wall thickness and an elliptically shaped root cross section were the variables that indicated the differences in strength between premolars, which are more prone to VRF, and maxillary central incisors, which are less prone to VRF."
Effects of carbon dioxide addition to fuel on soot evolution in ethylene and propane diffusion flames,"The influence of carbon dioxide addition to the fuel on soot evolution in ethylene and propane diffusion flames was studied by optical diagnostics. The mole fraction of CO2 addition ranged from 0 to 0.5, while the flow rate of the fuel gas was kept constant for these two sets of flames. Spatial distributions of polycyclic aromatic hydrocarbons (PAHs), temperature, as well as volume fraction, primary particle size and number density of soot were observed by the methods of laser-induced fluorescence (LIF), ratio pyrometry and laser-induced incandescence (LII), respectively. It was found that the flame height decreased for ethylene flames while it was nearly constant for propane flames with increasing addition of CO2. The measurements showed a temperature reduction in the lower part but an increase in the upper part in the ethylene-based flames. By contrast, a slight temperature decrease was observed in overall propane-based flames with the addition of CO2. Similar suppression effects were observed in the total soot/PAHs loading, percentage of carbon conversion to soot, and the total number of primary soot particles regardless of the fuel type. Comparison between the total loading of soot and PAHs indicated that addition of CO2 inhibited the conversion of PAHs to soot. The results also showed that the addition of CO2 in the fuel had a small effect on the specific growth rate of soot regardless of the fuel type. Relative changes of particle surface area could reasonably well explain the shift in the peak volume fraction from the wings to the centerline with the addition of CO2 to the ethylene flames."
Effects of high-amplitude low-frequency structural vibrations and machinery sound waves on ultrasonic guided wave propagation for health monitoring of composite aircraft primary structures,"A reliable damage diagnostic by ultrasonic guided wave (GW) based structural health monitoring (SHM) can only be achieved if the physical interactions between wave propagation, the SHM system and environmental factors are fully understood. The purpose of this research was to gain knowledge about the effects of high-amplitude low-frequency structural vibrations (HA-LFV) and audible sound waves (SW) on ultrasonic GW propagation. Measurements were performed on a stiffened panel of a full-scale composite torsion box containing barely visible impact damage. Time-domain analysis of the filtered GW signals revealed that the main effect of HA-LFV was the presence of coherent noise. This was interpreted as the consequence of superposition of multiple dispersive wave groups produced by mode conversion at the moment of reflection on the corrugated panel surfaces during propagation. It was also observed that the coherent noise amplitude depends on the amplitude of the HA-LFV, and on the ratio between the HA-LFV frequency and the ultrasonic excitation frequency. These relationships can potentially be explored for the development of a HA-LFV compensation mechanism to enable in-service GW based damage diagnostics. In contrast, GW signals in the cases with audible SW present were almost unaffected. It was concluded that there is strong evidence supporting the hypothesis that ultrasonic GW propagation with HA-LFV effects can be analysed under the assumption of a permanently corrugated structure."
Effects of Soret diffusion on turbulent non-premixed H2 jet flames,"Recent studies have shown that Soret diffusion (SD), driven by temperature gradients, could play an important role in both laminar and turbulent premixed H2 flames. However, comparatively little effort has been made to investigate SD effects on turbulent non-premixed H2 flames, in spite of the relevance of these flames in safety and their potential application in clean power and propulsion systems. To this end, the impact of SD on turbulent non-premixed H2 combustion is investigated numerically in this work by comparing two three-dimensional direct numerical simulations of temporally evolving turbulent jet flames. In one simulation, a mixture-averaged diffusion (MD) model is used to approximate multicomponent transport, while in the other the MD model is supplemented with a Soret term to consider SD effects. The emphasis is placed on examining and interpreting the impact of SD on flame structure, differential diffusion, and flame-tangential diffusion. It is found that H and OH mass fractions are significantly affected by SD, while SD has a negligible impact on temperature, heat release rate and H2 mass fraction. This is due to the fact that larger SD flux of H radical is strongly coupled with the main chemical reactions. However, for H2 its larger SD flux is located in the fuel-rich zone and decoupled from the main reactions. The difference between a conserved scalar (mixture fraction Z) and a non-conserved scalar (Bilger mixture fraction ZBilger) is employed as a diagnostic parameter to characterize differential diffusion, and the results show that the effects of SD on differential diffusion are reflected in two aspects: (i) increasing the absolute value of Z−ZBilger and (ii) increasing the degree of misalignment between the gradients of Z and ZBilger. Furthermore, the analysis of the contribution of SD to flame-tangential diffusion occurring in mixture fraction isosurfaces indicates that for H radical SD can augment the relative contribution of flame-tangential diffusion, especially in the region of high scalar dissipation rate. On the other hand, for H2, SD has a negligible impact on both flame-normal and flame-tangential diffusion. The present study contributes to providing insights into how SD affects turbulent non-premixed H2 flames and into modeling SD effects with flamelet-based combustion models."
Emerging medical informatics with case-based reasoning for aiding clinical decision in multi-agent system,"This research aims to depict the methodological steps and tools about the combined operation of case-based reasoning (CBR) and multi-agent system (MAS) to expose the ontological application in the field of clinical decision support. The multi-agent architecture works for the consideration of the whole cycle of clinical decision-making adaptable to many medical aspects such as the diagnosis, prognosis, treatment, therapeutic monitoring of gastric cancer. In the multi-agent architecture, the ontological agent type employs the domain knowledge to ease the extraction of similar clinical cases and provide treatment suggestions to patients and physicians. Ontological agent is used for the extension of domain hierarchy and the interpretation of input requests. Case-based reasoning memorizes and restores experience data for solving similar problems, with the help of matching approach and defined interfaces of ontologies. A typical case is developed to illustrate the implementation of the knowledge acquisition and restitution of medical experts."
Empowering Community Health Workers with Inkjet-printed Diagnostic Test Strips,"Community health worker (CHW) programs have emerged as an effective way to address the growing double burden of acute and chronic diseases in developing countries. Affordable and ruggedized biomedical devices can enable CHWs to provide diagnostic and screening services to their communities. However, low profit margins make such devices and markets unattractive to large diagnostics companies. Diagnostic test strips, created with inkjet printers by depositing biochemical reagents on paper, are both practical and cost-effective. Such test strips can rapidly detect pathogens and other abnormalities through readily-interpretable visual results, making them well-suited for use by CHWs in the field."
Enhancing fault tolerance of autonomous mobile robots,"Experience demonstrates that autonomous mobile robots running in the field in a dynamic environment often breakdown. Generally, mobile robots are not designed to efficiently manage faulty or unforeseen situations. Even if some research studies exist, there is a lack of a global approach that really integrates dependability and particularly fault tolerance into the mobile robot design. This paper presents an approach that aims to integrate fault tolerance principles into the design of a robot real-time control architecture. A failure mode analysis is firstly conducted to identify and characterize the most relevant faults. Then the fault detection and diagnosis mechanisms are explained. Fault detection is based on dedicated software components scanning faulty behaviors. Diagnosis is based on the residual principle and signature analysis to identify faulty software or hardware components and faulty behaviors. Finally, the recovery mechanism, based on the modality principle, proposes to adapt the robot’s control loop according to the context and current operational functions of the robot. This approach has been applied and implemented in the control architecture of a Pioneer 3DX mobile robot."
Enhancing the performance of predictive models for Hospital mortality by adding nursing data,"Background Mortality is the most considered outcome for assessing the quality of hospital care. However, hospital mortality depends on diverse patient characteristics; thus, complete risk stratification is crucial to correctly estimate a patient’s prognosis. Electronic health records include standard medical data; however, standard nursing data, such as nursing diagnoses (which were considered essential for a complete picture of the patient condition) are seldom included. Objective To explore the independent predictive power of nursing diagnoses on patient hospital mortality and to investigate whether the inclusion of this variable in addition to medical diagnostic data can enhance the performance of risk adjustment tools. Methods Prospective observational study in one Italian university hospital. Data were collected for six months from a clinical nursing information system and the hospital discharge register. The number of nursing diagnoses identified by nurses within 24 h after admission was used to express the nursing dependency index (NDI). Eight logistic regression models were tested to predict patient mortality, by adding to a first basic model considering patient’s age, sex, and modality of hospital admission, the level of comorbidity (CCI), and the nursing and medical condition as expressed by the NDI and the All Patient Refined-Diagnosis Related Group weight (APR-DRGw), respectively. Results Overall, 2301 patients were included. The addition of the NDI to the model increased the explained variance by 20%. The explained variance increased by 56% when the APR-DRGw, CCI, and NDI were included. Thus, the latter model was nearly highly accurate (c = 0.89, 95% confidence interval: 0.87–0.92). Conclusion Nursing diagnoses have an independent power in predicting hospital mortality. The explained variance in the predictive models improved when nursing data were included in addition to medical data. These findings strengthen the need to include standardized nursing data in electronic health records."
Ensemble learning model for diagnosing COVID-19 from routine blood tests,"Background and objectives The pandemic of novel coronavirus disease 2019 (COVID-19) has severely impacted human society with a massive death toll worldwide. There is an urgent need for early and reliable screening of COVID-19 patients to provide better and timely patient care and to combat the spread of the disease. In this context, recent studies have reported some key advantages of using routine blood tests for initial screening of COVID-19 patients. In this article, first we present a review of the emerging techniques for COVID-19 diagnosis using routine laboratory and/or clinical data. Then, we propose ERLX which is an ensemble learning model for COVID-19 diagnosis from routine blood tests. Method The proposed model uses three well-known diverse classifiers, extra trees, random forest and logistic regression, which have different architectures and learning characteristics at the first level, and then combines their predictions by using a second level extreme gradient boosting (XGBoost) classifier to achieve a better performance. For data preparation, the proposed methodology employs a KNNImputer algorithm to handle null values in the dataset, isolation forest (iForest) to remove outlier data, and a synthetic minority oversampling technique (SMOTE) to balance data distribution. For model interpretability, features importance are reported by using the SHapley Additive exPlanations (SHAP) technique. Results The proposed model was trained and evaluated by using a publicly available data set from Albert Einstein Hospital in Brazil, which consisted of 5644 data samples with 559 confirmed COVID-19 cases. The ensemble model achieved outstanding performance with an overall accuracy of 99.88% [95% CI: 99.6–100], AUC of 99.38% [95% CI: 97.5–100], a sensitivity of 98.72% [95% CI: 94.6–100] and a specificity of 99.99% [95% CI: 99.99–100]. Discussion The proposed model revealed better performance when compared against existing state-of-the-art studies (Banerjee et al., 2020; de Freitas Barbosa et al., 2020; de Moraes Batista et al., 2020; Soares et al., 2020) [3,22,56,71] for the same set of features employed by them. As compared to the best performing Bayes Net model (de Freitas Barbosa et al., 2020) [22] average accuracy of 95.159%, ERLX achieved an average accuracy of 99.94%. In comparison with AUC of 85% reported by the SVM model (de Moraes Batista et al., 2020) [56], ERLX obtained AUC of 99.77% in addition to improvements in sensitivity, and specificity. As compared with ER-COV model (Soares et al., 2020) [71] average sensitivity of 70.25% and specificity of 85.98%, ERLX model achieved sensitivity of 99.47% and specificity of 99.99%. The ERLX model obtained a considerably higher score as compared with ANN model (Banerjee et al., 2020) [3] in all performance metrics. Therefore, the model presented is robust and can be deployed for reliable early and rapid screening of COVID-19 patients."
Enzyme inhibition studies by integrated Michaelis–Menten equation considering simultaneous presence of two inhibitors when one of them is a reaction product,"To determine initial velocities of enzyme catalyzed reactions without theoretical errors it is necessary to consider the use of the integrated Michaelis–Menten equation. When the reaction product is an inhibitor, this approach is particularly important. Nevertheless, kinetic studies usually involved the evaluation of other inhibitors beyond the reaction product. The occurrence of these situations emphasizes the importance of extending the integrated Michaelis–Menten equation, assuming the simultaneous presence of more than one inhibitor because reaction product is always present. This methodology is illustrated with the reaction catalyzed by alkaline phosphatase inhibited by phosphate (reaction product, inhibitor 1) and urea (inhibitor 2). The approach is explained in a step by step manner using an Excel spreadsheet (available as a template in Appendix). Curve fitting by nonlinear regression was performed with the Solver add-in (Microsoft Office Excel). Discrimination of the kinetic models was carried out based on Akaike information criterion. This work presents a methodology that can be used to develop an automated process, to discriminate in real time the inhibition type and kinetic constants as data (product vs. time) are achieved by the spectrophotometer."
Enzyme–polymeric/inorganic metal oxide/hybrid nanoparticle bio-conjugates in the development of therapeutic and biosensing platforms,"Background Because enzymes can control several metabolic pathways and regulate the production of free radicals, their simultaneous use with nanoplatforms showing protective and combinational properties is of great interest in the development of therapeutic nano-based platforms. However, enzyme immobilization on nanomaterials is not straightforward due to the toxic and unpredictable properties of nanoparticles in medical practice. Aim of review In fact, because of the ability to load enzymes on nano-based supports and increase their renewability, scientific groups have been tempted to create potential therapeutic enzymes in this field. Therefore, this study not only pays attention to the therapeutic and diagnostic applications of diseases by enzyme–nanoparticle (NP) bio-conjugate (abbreviated as: ENB), but also considers the importance of nanoplatforms used based on their toxicity, ease of application and lack of significant adverse effects on loaded enzymes. In the following, based on the published reports, we explained that the immobilization of enzymes on polymers, inorganic metal oxide and hybrid compounds provide hopes for potential use of ENBs in medical activities. Then, the use of ENBs in bioassay activities such as paper-based or wearing biosensors and lab-on-chip/microfluidic biosensors were evaluated. Finally, this review addresses the current challenges and future perspective of ENBs in biomedical applications. Key scientific concepts of review This literature may provide useful information regarding the application of ENBs in biosensing and therapeutic platforms."
Estimating gene expression from high-dimensional DNA methylation levels in cancer data: A bimodal unsupervised dimension reduction algorithm,"Recent molecular and genetic studies have revealed the importance of DNA methylation, a key epigenetic mark, in regulating gene expression and the abnormal profiles of DNA methylation in various diseases including cancer. Here, unsupervised learning methods that are geared towards high-throughput DNA methylation analysis are used to extract useful information from high-dimensional genome wide methylation data in order to provide crucial insights for accurate early diagnosis and treatment of cancer. Herein, these methods are highly dependent on the performance of an earlier step of dimension reduction that aims to find the best subset of attributes to be retained for learning. Widely used algorithms in the literature commonly suffer from resulting in trivial cluster structures and failing to shed light on the relationship between DNA methylation and cancer types due to their myopic and arbitrary search mechanisms. Addressing this issue, we introduce a bimodal unsupervised dimension reduction algorithm (BOUNDER) that identifies the best subset of loci for downstream analysis considering the variability and redundancy across all the samples using bimodal modeling before it feeds into the learning method. BOUNDER models each locus as a bimodal representation using a piecewise linear function with two segments and filters the informative loci based on the fitted line characteristics. To the best of our knowledge, the work presented here is the first study that uses bimodal modeling in unsupervised learning in DNA methylation analysis. BOUNDER is tailored for DNA methylation analysis using a detailed parameter tuning analysis. The performance of BOUNDER is benchmarked against those of widely used conventional algorithms using real lung, breast, kidney, and urological cancer datasets obtained from Gene Expression Omnibus in terms of their accuracies in hierarchical clustering and k-means clustering. Computational experiments reveal that BOUNDER outperforms the PCA and filtering based approach by providing the highest accuracy in 6 out of 9 datasets while providing more interpretable results through a correlation analysis. The BOUNDER algorithm is also shown to be more robust when compared to multiple other conventional dimension reduction algorithms across different datasets."
Estimating the end-of-life of PEM fuel cells: Guidelines and metrics,"Prognostics applications on PEMFC are developing these last years. Indeed, taking decision to extend the lifetime of a PEMFC stack based on behavior and remaining useful life predictions is seen as a promising solution to tackle the too short life’s issue of PEMFCs. However, the development of prognostics shows some lacks in the literature. Indeed, performing prognostics requires health indicators that reflect the state of health of stack, while being able to interpret them in an industrial context. It is also important to propose criteria to set its end of life. Moreover, to trust any prognostics’ application, one should be able to evaluate the performance of its algorithms with respect to standards. To help launching a discussion on these subjects among scientific and industrial actors, this paper addresses some of the issues encountered when performing prognostics of a PEMFC stack. After showing the link between prognostics and decision, this paper proposes guidelines to set the limits of a prognostics approach. The definitions of healthy and degraded modes are discussed as well as how to choose the time instant to perform predictions. Then, three criteria based on the power produced by the stack are proposed as indicators of the state of health of the stack. The definition of the end of life of the stack is also discussed before proposing some criteria to assess the performance of any prognostics algorithm on a PEMFC. Some perspectives of works are also discussed before concluding."
Estimation of growth features and thermophysical properties of melanoma within 3-D human skin using genetic algorithm and simulated annealing,"A study has been performed on human skin model with the motivation to device an effective non-invasive modality to characterize the subsurface skin cancer features such as tumor diameter, penetration depth, blood perfusion and metabolic heat generation based on the thermal response of the skin surface obtained from the thermal images. The work presents the role of data mining algorithms to find the tumor features underneath the skin based on the surface temperature variations obtained from a 3-D model of human skin. The human skin is assumed to be subjected to combined radiative, convective, and evaporative heat flux boundary conditions. The study revealed that, the major variation in the thermal response of tumor is attributed to increase in the volume, blood perfusion and thermogenic capacity. The variations due to inter- and intra-patient variability of tumor properties and size are obvious, which could be explained by the retrieved multiple combinations of variables. Furthermore, the reconstructed surface thermal distributions associated with estimated variables are found to be in a good match with the actual maps. The error <10% in the measured thermal distribution tends to give accurate reconstruction. Present strategy or algorithm along with a thermal camera may prove to be a useful diagnostic tool for the characterization of subsurface skin cancer and reduce the unnecessary biopsy trials."
European Society of Biomechanics S.M. Perren Award 2018: Altered biomechanical stimulation of the developing hip joint in presence of hip dysplasia risk factors,"Fetal kicking and movements generate biomechanical stimulation in the fetal skeleton, which is important for prenatal musculoskeletal development, particularly joint shape. Developmental dysplasia of the hip (DDH) is the most common joint shape abnormality at birth, with many risk factors for the condition being associated with restricted fetal movement. In this study, we investigate the biomechanics of fetal movements in such situations, namely fetal breech position, oligohydramnios and primiparity (firstborn pregnancy). We also investigate twin pregnancies, which are not at greater risk of DDH incidence, despite the more restricted intra-uterine environment. We track fetal movements for each of these situations using cine-MRI technology, quantify the kick and muscle forces, and characterise the resulting stress and strain in the hip joint, testing the hypothesis that altered biomechanical stimuli may explain the link between certain intra-uterine conditions and risk of DDH. Kick force, stress and strain were found to be significantly lower in cases of breech position and oligohydramnios. Similarly, firstborn fetuses were found to generate significantly lower kick forces than non-firstborns. Interestingly, no significant difference was observed in twins compared to singletons. This research represents the first evidence of a link between the biomechanics of fetal movements and the risk of DDH, potentially informing the development of future preventative measures and enhanced diagnosis. Our results emphasise the importance of ultrasound screening for breech position and oligohydramnios, particularly later in pregnancy, and suggest that earlier intervention to correct breech position through external cephalic version could reduce the risk of hip dysplasia."
Evaluating temperature and fuel stratification for heat-release rate control in a reactivity-controlled compression-ignition engine using optical diagnostics and chemical kinetics modeling,"The combustion process in a dual-fuel, reactivity-controlled compression-ignition (RCCI) engine is investigated using a combination of optical diagnostics and chemical kinetics modeling to explain the role of equivalence ratio, temperature, and fuel reactivity stratification for heat-release rate control. An optically accessible engine is operated in the RCCI combustion mode using gasoline primary reference fuels (PRF). A well-mixed charge of iso-octane (PRF=100) is created by injecting fuel into the engine cylinder during the intake stroke using a gasoline-type direct injector. Later in the cycle, n-heptane (PRF=0) is delivered through a centrally mounted diesel-type common-rail injector. This injection strategy generates stratification in equivalence ratio, fuel blend, and temperature. The first part of this study uses a high-speed camera to image the injection events and record high-temperature combustion chemiluminescence. The chemiluminescence imaging showed that, at the operating condition studied in the present work, mixtures in the squish region ignite first, and the reaction zone proceeds inward toward the center of the combustion chamber. The second part of this study investigates the charge preparation of the RCCI strategy using planar laser-induced fluorescence (PLIF) of a fuel tracer under non-reacting conditions to quantify fuel concentration distributions prior to ignition. The fuel-tracer PLIF data show that the combustion event proceeds down gradients in the n-heptane distribution. The third part of the study uses chemical kinetics modeling over a range of mixtures spanning the distributions observed from the fuel-tracer fluorescence imaging to isolate the roles of temperature, equivalence ratio, and PRF number stratification. The simulations predict that PRF number stratification is the dominant factor controlling the ignition location and growth rate of the reaction zone. Equivalence ratio has a smaller, but still significant, influence. Temperature stratification had a negligible influence due to the NTC behavior of the PRF mixtures."
Evolution of flow characteristics through finite-sized wind farms and influence of turbine arrangement,"Evolution of flow characteristics through finite-sized wind farms and the influence of the wind-farm configuration on modulating this evolution is explored through numerical simulations. The principal aim for the study is to identify regions of flow-adjustment and flow equilibrium within the wind farm. Towards this aim, a suite of five large-eddy simulations (LES) of the neutral atmospheric boundary layer with extremely long streamwise domains are performed with embedded finite-sized wind farms of different streamwise and spanwise spacing. Three diagnostic variables, namely, the wind-farm induced effective surface roughness, the wake viscosity and the wake-expansion coefficient are computed using the LES-generated database and are used to characterize the flow. Computation of the diagnostic variables is relevant to the wind-energy community in different contexts ranging from parametrization of wind farms in weather and climate models, to wind-farm design and optimization based on wake-models and eddy-viscosity type Reynolds-averaged Navier-Stokes solvers. Results show that flow equilibrium is achieved in the ‘most dense’ configuration of sx≈8D,sy≈5D at approximately the 19th row. Results also indicate that the streamwise spacing plays a dominant role determining the rate at which flow-adjustment is achieved within the wind farm."
Examination of spacecraft anomalies provides insight into complex space environment,"Spacecraft operations are affected by a variety of natural and manmade features that create significant ambiguity as to root cause determination for many anomalies and failures of satellites. The natural space environment comprises dynamic radiation, energetic atomic particles, and particulates (micrometeoroids and orbital debris) that vary temporally and spatially across relevant Earth orbits. Some of the failure mechanisms are further obfuscated by intricate local interactions, the fact that failures are often the result of more than one environmental effect, and lack of diagnostic sensors onboard spacecraft. At the same time, manmade influences on spacecraft anomalies and failures include design, manufacture, integration/installation, parts quality, testing completeness, and operations. These manmade aspects of the anomaly/failure attribution process are equally daunting as much of the relevant information is either not collected or not widely distributed for a variety of reasons. This paper details these dimensions of the anomaly/failure attribution process and provides data from a variety of operational examples to illustrate quantitative and specific actions to enhance the anomaly/failure attribution process short-term and long-term."
Experimental and theoretical evidence for the temperature-determined evolution of PAH functional groups,"Elucidating the chemical evolution of various functional groups in polycyclic aromatic hydrocarbons (PAH) and soot aids in understanding soot formation chemistry. In this work, the chemical evolution of various functional groups, including aromatic CH, aliphatic CH, C=O, COH and COC bonds, was experimentally investigated online, rather than with offline diagnostics. Oxidation was performed in a jet-stirred reactor (JSR), fueled with benzene/C2H2/air/N2 and benzene/phenol/C2H2/N2 for a temperature range of 600-1400 K. Kinetic modelling, including ab initio quantum chemistry calculations, reaction rate coefficient calculations and JSR simulations, were conducted to interpret the experimental data and the evolutionary chemistry of the various functional groups. Results show that the formation of functional groups on PAH and oxygenated PAH (OPAH) are highly sensitive to temperature. Aliphatic CH bonds survive mainly in the form of CCH2C, CCH2CH2C or CCH functional groups above 1200 K, and exist in the CHCH2 functional group below 1000 K. For the OPAH, the COC functional group presents stronger thermal stability than COH and C=O functional groups. Simulation results indicate that HACA-like pathway (hydrogen abstraction carbon addition), in which C2H2 attacks the O atom, followed by cyclization and H-atom elimination reactions, qualitatively describe the formation of OPAH with the CO-C functional group at different temperatures. The addition reaction involving PAH radical and C2H4 / C2H3 captures the evolution of PAH with the CHCH2 functional group, but fails to explain the formation of CCH2C and CCH2CH2C functional groups."
Experimental investigation of the auto-ignition of a transient propane Jet-in-Hot-Coflow,"Auto-ignition is a complex process which is extremely sensitive to boundary conditions such as local temperature, mixture or strain rate and occurs on very short time-scales. Therefore, measurement techniques with high spatio-temporal resolution have to be applied to test cases with well-defined boundary conditions in order to generate high-quality validation data for numerical simulations. In the current paper, the auto-ignition of a transient propane jet-in-hot coflow was studied with high-speed OH* chemiluminescence imaging and high-speed Rayleigh scattering for the simultaneous determination of mixture fraction, mixture temperature and scalar dissipation rate immediately prior to the onset of auto-ignition. A variation of the coflow temperature showed a pronounced temperature dependence of the auto-ignition location and time, and the temperature sensitivity was higher than for a comparable methane test case from the literature. This is explained by the lower sensitivity of propane ignition delay times to the local strain rate in comparison to methane. The Rayleigh measurements however showed that the formation mechanism of auto-ignition kernels is similar for propane and methane. Ignition kernels were found to form upstream of bulges of the inflowing jet at locations with locally low scalar dissipation rate."
Experimental research into the response of segmental barrel vaults to repetitive static and dynamic loads,"The article briefly recalls the issue of historical buildings located in seismically active areas. In the introduction of the paper, the issue of the change of the dynamic response of the structure as a method for determining the possible damage of the structure, which can be difficult to detect by other diagnostic procedures, is discussed. Furthermore, the article outlines the procedure for determining some significant characteristics (such as bending stiffness etc.) by means of the dynamic response of the structure, and simplified relationships and recommendations for diagnosing arch structures based on its dynamic response. The focus of the paper is on the general analysis of the results of experimental tests of five segmental barrel vaults, of which two were reinforced with CFRP strips subjected to alternating static and dynamic loads. Due to the structure of the vault - single-layer masonry with a thickness of 150 mm - a loading mode was required. In the article conclusion the test results are interpreted with regard to the above mentioned facts (extent of experimental loading, specific vault structure) and it is stated that they have demonstrated the possibility of using a change of dynamic characteristics as a possible diagnostic method of the origin and development of defects on the vault structure"
Experimental studies of autoignition events in unsteady hydrogen–air flames,"An experimental study is presented of unsteady N2-in-H2 jet flames in a co-flow of hot combustion products from lean premixed hydrogen combustion for investigation of the statistical likelihood of autoignition events in the mixing region. The unsteady jet flame is characterized by rapid ignition followed by a gradual blowout of the flame. Audio recordings and Schlieren imaging high speed videos are used in investigating the unsteady flame. The frequency of the blowout re-ignition event is investigated as a function of nitrogen dilution mole fraction (YN2=0.180–0.566), co-flow equivalence ratio (Φcf=0.20–0.27) and jet velocity (Vjet=300–500m/s). The results from the audio recordings and Schlieren imaging indicate that autoignition dominates the re-ignition. The frequency of ignition increase with increasing nitrogen dilution until a maximum is reached after which it decreases with further nitrogen dilution. For increasing equivalence ratios a higher nitrogen dilution is needed in the jet for the flame to become unsteady. The effect of the nitrogen dilution is explained primarily through a reduction in reaction rates and increased jet momentum. Furthermore, the results suggest that the re-ignition rates are controlled by both chemistry and turbulent mixing. The results from the audio recordings and the Schlieren imaging videos correspond well which validates the use of audio recordings as a diagnostic for studying of unsteady hydrogen jet flames."
Experimental study of heat transfer performance of compact wavy channel with nanofluids as the coolant medium: Real time non-intrusive measurements,"Heat transfer performance of nanofluids in the context of corrugated compact channels has been experimentally investigated through real-time, non-intrusive diagnostic technique. The corrugated channel (dimensions: 50 mm (length) × 10 mm (width) × 4 mm (depth); hydraulic diameter of 5.7 mm) comprises of two horizontal wavy plates aligned in an in-phase configuration. Amplitude and period of the wavy plates have been kept constant. Experiments have been performed with varying concentrations of Al2O3/water-based nanofluids for Re = 350–1000. For reference, experiments have also been conducted using water as the base fluid and a plane channel with the same hydraulic diameter as that of the corrugated channel. Convective field has been mapped using a Mach-Zehnder interferometer. Results have been qualitatively as well as quantitatively interpreted to characterize the heat transfer performance of nanofluids as the coolant medium. Interferograms recorded under infinite fringe setting mode captured the phenomena of flow detachment and re-attachment at the wavy sections of the corrugated channel and their dependence on nanofluids concentration. Thermal boundary layer profiles at such sections are seen to be locally perturbed due to flow detachment and its subsequent attachment to the principal wall of the channel. Quantitative data retrieved through the wedge fringe setting interferograms revealed a clear enhancement in heat transfer coefficient for the combination of nanofluids and corrugated channel in comparison with that achieved through water in the plane and/or corrugated channel. For any given Re and channel configuration, the heat transfer coefficient is seen to increase with increasing concentration of dilute nanofluids. For the wavy channel, an enhancement of ≈37% in heat transfer rates with 0.02% volume concentration of nanofluids is achieved at Re = 350 in comparison to the plane channel at the similar levels of nanoparticles volume concentration."
Experimental Study of Results Obtained from the Interaction with Softwares Motion-based Touchless Created for Habilitation-rehabilitation in users with Diagnosis of Autism Spectrum Disorders,"The research that we present sheds a light on a possible opportunity to provide an innovative method of rehabilitation with touchless software dedicated to users with special needs. To date our research focused on the investigation of the possible benefits in Autism Spectrum Disorders (ASD) and issues related to it such as, motor difficulties, relationship difficulties, social gender, and cognitive problems. In the study we examined five different age groups of users and the variables in relation to solicitation proposals, for 3 months. As to date there are a limited number of studies that investigate the application of software-based touchless motion and their use in practice habilitation-rehabilitation. This paper attempts to provide a dual contribution: explain an innovative therapeutic modality for lavish treatments using softwares dedicated and highly customizable, with supervision, and present the clinical experience conducted on the subjects examined. Overall, our research confirms the potential benefit that the use of such software provides dedicated, showing improvement learning variables in ASD children and adults."
Experimental validation of DXA-based finite element models for prediction of femoral strength,"Osteoporotic fractures are a major clinical problem and current diagnostic tools have an accuracy of only 50%. The aim of this study was to validate dual energy X-rays absorptiometry (DXA)-based finite element (FE) models to predict femoral strength in two loading configurations. Thirty-six pairs of fresh frozen human proximal femora were scanned with DXA and quantitative computed tomography (QCT). For each pair one femur was tested until failure in a one-legged standing configuration (STANCE) and one by replicating the position of the femur in a fall onto the greater trochanter (SIDE). Subject-specific 2D DXA-based linear FE models and 3D QCT-based nonlinear FE models were generated for each specimen and used to predict the measured femoral strength. The outcomes of the models were compared to standard DXA-based areal bone mineral density (aBMD) measurements. For the STANCE configuration the DXA-based FE models (R2=0.74, SEE=1473N) outperformed the best densitometric predictor (Neck_aBMD, R2=0.66, SEE=1687N) but not the QCT-based FE models (R2=0.80, SEE=1314N). For the SIDE configuration both QCT-based FE models (R2=0.85, SEE=455N) and DXA neck aBMD (R2=0.80, SEE=502N) outperformed DXA-based FE models (R2=0.77, SEE=529N). In both configurations the DXA-based FE model provided a good 1:1 agreement with the experimental data (CC=0.87 for SIDE and CC=0.86 for STANCE), with proper optimization of the failure criteria. In conclusion we found that the DXA-based FE models are a good predictor of femoral strength as compared with experimental data ex vivo. However, it remains to be investigated whether this novel approach can provide good predictions of the risk of fracture in vivo."
Experiments on pool boiling regimes and bubble departure characteristics of single vapor bubble under subcooled bulk conditions,"An experimental study for understanding the effects of subcooling of bulk fluid on the phenomenon of nucleate boiling heat transfer was carried out. The whole field experimental data was recorded in a purely non-intrusive manner using rainbow schlieren deflectometry as the optical diagnostic tool. Boiling experiments were conducted over a range of heat flux conditions (1⩽q′′⩽40 kW/m2) and for varying levels of subcooling (0 K ≤ ΔTsubcooling ≤ 10 K). The recorded images were first been interpreted to understand the effect of subcooling on various regimes of boiling phenomenon, which include convection, isolated nucleate boiling and vertical coalescence of the departing bubbles. Observations on convection regime showed an increasing influence of natural convection heat transfer. Based on the formation of vapor bubble and its departure characteristics, the isolated nucleate boiling regime of the classical boiling curve was further segmented into three sub-regimes namely, stable vapor bubbles, oscillating vapor bubbles and oscillating-departing vapor bubbles. A mechanism explaining the oscillations of vapor bubbles under subcooled condition was proposed that takes into account the relative dominance of condensing apex region and evaporating microlayer region. Various bubble dynamic parameters such as equivalent bubble diameter, departure frequency, aspect ratio and oscillation frequency were determined quantitatively to understand the effect of subcooling on the bubble departure characteristics. Based on the experimental results, an empirical correlation for predicting the bubble departure diameter and evaporative heat flux was developed and proposed."
Expert system supporting an early prediction of the bronchopulmonary dysplasia,"This work presents a decision support system which uses machine learning to support early prediction of bronchopulmonary dysplasia (BPD) for extremely premature infants after their first week of life. For that purpose a knowledge database was created based on the historical data gathered including data on 109 patients with birth weight less than or equal to 1500g. The core of the database consists of support vector machine and logit regression classification results calculated specifically for that system, and obtained by considering 214 different combinations of 14 risk factors. Based on the results obtained and user demands, the system recommends the best methods and the most suitable parameter subset among those currently available to the user. The program is also able to estimate the accuracy, sensitivity and specificity together with their standard deviations. The user is also given information on which additional parameter it is worth adding to his measurement system most and what an increase in prediction efficiency it is expected to trigger. The BPD can be predicted by the system with the accuracy reaching up to 83.25% in the best-case scenario, i.e. higher than for most of the models presented in the literature. This work presents a set of examples illustrating the difficulties in obtaining one single model that can be widely used, and thus explaining why an expert system approach is much more useful in day-to-day clinical practice. In addition, the work discusses the significance of the parameters used and the impact of a chosen method on the sensitivity and specificity."
Explainable artificial intelligence for breast cancer: A visual case-based reasoning approach,"Case-Based Reasoning (CBR) is a form of analogical reasoning in which the solution for a (new) query case is determined using a database of previous known cases with their solutions. Cases similar to the query are retrieved from the database, and then their solutions are adapted to the query. In medicine, a case usually corresponds to a patient and the problem consists of classifying the patient in a class of diagnostic or therapy. Compared to “black box” algorithms such as deep learning, the responses of CBR systems can be justified easily using the similar cases as examples. However, this possibility is often under-exploited and the explanations provided by most CBR systems are limited to the display of the similar cases. In this paper, we propose a CBR method that can be both executed automatically as an algorithm and presented visually in a user interface for providing visual explanations or for visual reasoning. After retrieving similar cases, a visual interface displays quantitative and qualitative similarities between the query and the similar cases, so as one can easily classify the query through visual reasoning, in a fully explainable manner. It combines a quantitative approach (visualized by a scatter plot based on Multidimensional Scaling in polar coordinates, preserving distances involving the query) and a qualitative approach (set visualization using rainbow boxes). We applied this method to breast cancer management. We showed on three public datasets that our qualitative method has a classification accuracy comparable to k-Nearest Neighbors algorithms, but is better explainable. We also tested the proposed interface during a small user study. Finally, we apply the proposed approach to a real dataset in breast cancer. Medical experts found the visual approach interesting as it explains why cases are similar through the visualization of shared patient characteristics."
Explainable classifier for improving the accountability in decision-making for colorectal cancer diagnosis from histopathological images,"Pathologists are responsible for cancer type diagnoses from histopathological cancer tissues. However, it is known that microscopic examination is tedious and time-consuming. In recent years, a long list of machine learning approaches to image classification and whole-slide segmentation has been developed to support pathologists. Although many showed exceptional performances, the majority of them are not able to rationalize their decisions. In this study, we developed an explainable classifier to support decision making for medical diagnoses. The proposed model does not provide an explanation about the causality between the input and the decisions, but offers a human-friendly explanation about the plausibility of the decision. Cumulative Fuzzy Class Membership Criterion (CFCMC) explains its decisions in three ways: through a semantical explanation about the possibilities of misclassification, showing the training sample responsible for a certain prediction and showing training samples from conflicting classes. In this paper, we explain about the mathematical structure of the classifier, which is not designed to be used as a fully automated diagnosis tool but as a support system for medical experts. We also report on the accuracy of the classifier against real world histopathological data for colorectal cancer. We also tested the acceptability of the system through clinical trials by 14 pathologists. We show that the proposed classifier is comparable to state of the art neural networks in accuracy, but more importantly it is more acceptable to be used by human experts as a diagnosis tool in the medical domain."
Explainable classifier for improving the accountability in decision-making for colorectal cancer diagnosis from histopathological images,"Pathologists are responsible for cancer type diagnoses from histopathological cancer tissues. However, it is known that microscopic examination is tedious and time-consuming. In recent years, a long list of machine learning approaches to image classification and whole-slide segmentation has been developed to support pathologists. Although many showed exceptional performances, the majority of them are not able to rationalize their decisions. In this study, we developed an explainable classifier to support decision making for medical diagnoses. The proposed model does not provide an explanation about the causality between the input and the decisions, but offers a human-friendly explanation about the plausibility of the decision. Cumulative Fuzzy Class Membership Criterion (CFCMC) explains its decisions in three ways: through a semantical explanation about the possibilities of misclassification, showing the training sample responsible for a certain prediction and showing training samples from conflicting classes. In this paper, we explain about the mathematical structure of the classifier, which is not designed to be used as a fully automated diagnosis tool but as a support system for medical experts. We also report on the accuracy of the classifier against real world histopathological data for colorectal cancer. We also tested the acceptability of the system through clinical trials by 14 pathologists. We show that the proposed classifier is comparable to state of the art neural networks in accuracy, but more importantly it is more acceptable to be used by human experts as a diagnosis tool in the medical domain."
Explainable Convolutional Neural Network for Gearbox Fault Diagnosis,"The rapid advancement of data analytics has opened up new opportunities for improving the life cycle of engineered products and enhancing sustainability by intelligent monitoring and fault diagnosis of the related manufacturing processes and systems. Recently, Deep Neural Networks (DNNs) have demonstrated improved accuracy and robustness in classifying machine fault types and severities, when compared with conventional machine learning techniques. A major constraint of DNNs is that they operate as ‘black boxes’, which do not provide insight into how fault classification decisions are made. This not only raises questions on the trustworthiness of the decisions themselves, but also limits further improvement of DNNs for adaptation to a broader range of applications. This paper presents an explainable Deep Convolutional Neural Network (DCNN), which has been developed on the basis of Layer-wise Relevance Propagation (LRP), for fault diagnosis of gearboxes. Vibration signals as time series data are first converted to time-frequency spectra images through wavelet transform, which are then classified by a DCNN. To explain the rationale for classification decision, LRP decomposes contributions from local regions in the spectra images to the classification results, and determines which time-frequency points in the spectra image contribute the most to fault type and severity identification. Results of the analysis are then cross-checked with the time-frequency analysis. The effectiveness of the developed explainable DCNN is evaluated by experiments on a gearbox testbed, where gears with different types and degrees of faults are evaluated. LRP results have revealed that a trained DCNN is selective to different frequency bands in the time-frequency spectra for classification of gearbox fault type and severity."
Explainable deep learning based medical diagnostic system,"Recently, many researchers have conducted data mining over medical data to uncover hidden patterns and use them to learn prediction models for clinical decision making and personalized medicine. While such healthcare learning models can achieve encouraging results, they seldom incorporate existing expert knowledge into their frameworks and hence prediction accuracy for individual patients can still be improved. However, expert knowledge spans across various websites and multiple databases with heterogeneous representations and hence is difficult to harness for improving learning models. In addition, patients' queries at medical consult websites are often ambiguous in their specified terms and hence the returned responses may not contain the information they seek. To tackle these problems, we first design a knowledge extraction framework that can generate an aggregated dataset to characterize diseases by integrating heterogeneous medical data sources. Then, based on the integrated dataset, we propose an end-to-end deep learning based medical diagnosis system (DL-MDS) to provide disease diagnosis for authorized users. We also provide explanations for the diagnose results. Evaluations on real-world data demonstrate that our proposed system achieves good performance on diseases diagnosis with a diverse set of patients’ queries."
Explainable deep learning based medical diagnostic system,"Recently, many researchers have conducted data mining over medical data to uncover hidden patterns and use them to learn prediction models for clinical decision making and personalized medicine. While such healthcare learning models can achieve encouraging results, they seldom incorporate existing expert knowledge into their frameworks and hence prediction accuracy for individual patients can still be improved. However, expert knowledge spans across various websites and multiple databases with heterogeneous representations and hence is difficult to harness for improving learning models. In addition, patients' queries at medical consult websites are often ambiguous in their specified terms and hence the returned responses may not contain the information they seek. To tackle these problems, we first design a knowledge extraction framework that can generate an aggregated dataset to characterize diseases by integrating heterogeneous medical data sources. Then, based on the integrated dataset, we propose an end-to-end deep learning based medical diagnosis system (DL-MDS) to provide disease diagnosis for authorized users. We also provide explanations for the diagnose results. Evaluations on real-world data demonstrate that our proposed system achieves good performance on diseases diagnosis with a diverse set of patients’ queries."
Explainable deep learning based medical diagnostic system,"Recently, many researchers have conducted data mining over medical data to uncover hidden patterns and use them to learn prediction models for clinical decision making and personalized medicine. While such healthcare learning models can achieve encouraging results, they seldom incorporate existing expert knowledge into their frameworks and hence prediction accuracy for individual patients can still be improved. However, expert knowledge spans across various websites and multiple databases with heterogeneous representations and hence is difficult to harness for improving learning models. In addition, patients' queries at medical consult websites are often ambiguous in their specified terms and hence the returned responses may not contain the information they seek. To tackle these problems, we first design a knowledge extraction framework that can generate an aggregated dataset to characterize diseases by integrating heterogeneous medical data sources. Then, based on the integrated dataset, we propose an end-to-end deep learning based medical diagnosis system (DL-MDS) to provide disease diagnosis for authorized users. We also provide explanations for the diagnose results. Evaluations on real-world data demonstrate that our proposed system achieves good performance on diseases diagnosis with a diverse set of patients’ queries."
Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays,"Background and Objective: Coronavirus disease (COVID-19) is an infectious disease caused by a new virus never identified before in humans. This virus causes respiratory disease (for instance, flu) with symptoms such as cough, fever and, in severe cases, pneumonia. The test to detect the presence of this virus in humans is performed on sputum or blood samples and the outcome is generally available within a few hours or, at most, days. Analysing biomedical imaging the patient shows signs of pneumonia. In this paper, with the aim of providing a fully automatic and faster diagnosis, we propose the adoption of deep learning for COVID-19 detection from X-rays. Method: In particular, we propose an approach composed by three phases: the first one to detect if in a chest X-ray there is the presence of a pneumonia. The second one to discern between COVID-19 and pneumonia. The last step is aimed to localise the areas in the X-ray symptomatic of the COVID-19 presence. Results and Conclusion: Experimental analysis on 6,523 chest X-rays belonging to different institutions demonstrated the effectiveness of the proposed approach, with an average time for COVID-19 detection of approximately 2.5 seconds and an average accuracy equal to 0.97."
Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays,"Background and Objective: Coronavirus disease (COVID-19) is an infectious disease caused by a new virus never identified before in humans. This virus causes respiratory disease (for instance, flu) with symptoms such as cough, fever and, in severe cases, pneumonia. The test to detect the presence of this virus in humans is performed on sputum or blood samples and the outcome is generally available within a few hours or, at most, days. Analysing biomedical imaging the patient shows signs of pneumonia. In this paper, with the aim of providing a fully automatic and faster diagnosis, we propose the adoption of deep learning for COVID-19 detection from X-rays. Method: In particular, we propose an approach composed by three phases: the first one to detect if in a chest X-ray there is the presence of a pneumonia. The second one to discern between COVID-19 and pneumonia. The last step is aimed to localise the areas in the X-ray symptomatic of the COVID-19 presence. Results and Conclusion: Experimental analysis on 6,523 chest X-rays belonging to different institutions demonstrated the effectiveness of the proposed approach, with an average time for COVID-19 detection of approximately 2.5 seconds and an average accuracy equal to 0.97."
Explainable skin lesion diagnosis using taxonomies,"Deep neural networks have rapidly become an indispensable tool in many classification applications. However, the inclusion of deep learning methods in medical diagnostic systems has come at the cost of diminishing their explainability. This significantly reduces the safety of a diagnostic system, since the physician is unable to interpret and validate the output. Therefore, in this work we aim to address this major limitation and improve the explainability of a skin cancer diagnostic system. We propose to leverage two sources of information: (i) medical knowledge, in particular the taxonomic organization of skin lesions, which will be used to develop a hierarchical neural network; and (ii) recent advances in channel and spatial attention modules, which can identify interpretable features and regions in dermoscopy images. We demonstrate that the proposed approach achieves competitive results in two dermoscopy data sets (ISIC 2017 and 2018) and provides insightful information about its decisions, thus increasing the safety of the model."
Explaining data with descriptions,"With the advent of Big Data, it is impossible for a human user to properly inspect and understand data at a glance. In this paper, we introduce the problem of generating data descriptions: a set of compact, readable and insightful formulas of boolean predicates that represents a set of data records. Unfortunately, finding the best description for a dataset is both NP-hard and task-specific. Therefore, we introduce a dynamic programming approach which, in concert with a set of heuristics, allows us not only to generate descriptions at interactive speed but also to accommodate diverse user needs—from anomaly detection to data exploration. Using real datasets, we evaluate our approach both quantitatively and qualitatively, and prove that descriptions are indeed a viable and powerful tool for supporting data enthusiasts and practitioners in gaining insights from data."
Exploiting complex medical data with interpretable deep learning for adverse drug event prediction,"A variety of deep learning architectures have been developed for the goal of predictive modelling and knowledge extraction from medical records. Several models have placed strong emphasis on temporal attention mechanisms and decay factors as a means to include highly temporally relevant information regarding the recency of medical event occurrence while facilitating medical code-level interpretability. In this study we utilise such models with a large Electronic Patient Record (EPR) data set consisting of diagnoses, medication, and clinical text data for the purpose of adverse drug event (ADE) prediction. The first contribution of this work is an empirical evaluation of two state-of-the-art medical-code based models in terms of objective performance metrics for ADE prediction on diagnosis and medication data. Secondly, as an extension of previous work, we augment an interpretable deep learning architecture to permit numerical risk and clinical text features and demonstrate how this approach yields improved predictive performance compared to the other baselines. Finally, we assess the importance of attention mechanisms in regards to their usefulness for medical code-level and text-level interpretability, which may facilitate novel insights pertaining to the nature of ADE occurrence within the health care domain."
Exploiting complex medical data with interpretable deep learning for adverse drug event prediction,"A variety of deep learning architectures have been developed for the goal of predictive modelling and knowledge extraction from medical records. Several models have placed strong emphasis on temporal attention mechanisms and decay factors as a means to include highly temporally relevant information regarding the recency of medical event occurrence while facilitating medical code-level interpretability. In this study we utilise such models with a large Electronic Patient Record (EPR) data set consisting of diagnoses, medication, and clinical text data for the purpose of adverse drug event (ADE) prediction. The first contribution of this work is an empirical evaluation of two state-of-the-art medical-code based models in terms of objective performance metrics for ADE prediction on diagnosis and medication data. Secondly, as an extension of previous work, we augment an interpretable deep learning architecture to permit numerical risk and clinical text features and demonstrate how this approach yields improved predictive performance compared to the other baselines. Finally, we assess the importance of attention mechanisms in regards to their usefulness for medical code-level and text-level interpretability, which may facilitate novel insights pertaining to the nature of ADE occurrence within the health care domain."
Exploiting complex medical data with interpretable deep learning for adverse drug event prediction,"A variety of deep learning architectures have been developed for the goal of predictive modelling and knowledge extraction from medical records. Several models have placed strong emphasis on temporal attention mechanisms and decay factors as a means to include highly temporally relevant information regarding the recency of medical event occurrence while facilitating medical code-level interpretability. In this study we utilise such models with a large Electronic Patient Record (EPR) data set consisting of diagnoses, medication, and clinical text data for the purpose of adverse drug event (ADE) prediction. The first contribution of this work is an empirical evaluation of two state-of-the-art medical-code based models in terms of objective performance metrics for ADE prediction on diagnosis and medication data. Secondly, as an extension of previous work, we augment an interpretable deep learning architecture to permit numerical risk and clinical text features and demonstrate how this approach yields improved predictive performance compared to the other baselines. Finally, we assess the importance of attention mechanisms in regards to their usefulness for medical code-level and text-level interpretability, which may facilitate novel insights pertaining to the nature of ADE occurrence within the health care domain."
Exploration of face-perceptual ability by EEG induced deep learning algorithm,"Face perception essentially refers to an individual's ability to understand and interpret a familiar face. This paper attempts to quantify the face perceptual ability of human subjects using their memory responses, acquired by an electroencephalographic (EEG) device, during engagement of the human subjects in a face recognition task. The entire experimental protocol is designed in the settings of pattern recognition, comprising five main steps, namely artifact removal, feature extraction, classifier training and testing, and face-perception ability measurement. The primary objective of the paper is to design a deep neural network model that utilizes both the temporal and spatial EEG features to categorize the familiar face (FF) and unfamiliar face (UF) responses of human subjects from their brain signals. To extract the spatial information, the acquired raw EEG data is transformed into multi-spectral 2-dimensional images by a deep learning approach. The spatial information, along with the EEG time-frequency domain (temporal) features are then utilized to train the proposed deep neural network, which can preserve the spatial as well as temporal EEG features that are more robust and less sensitive to the variations along each dimension. The proposed deep neural network paradigm demonstrates promising results in classifying the EEG responses for FF and UF recognition with high classification accuracy. Biological underpinning of the cortical mechanism using sLORETA and event related potential (ERP) analysis in perceiving FFs and UFs by both the healthy controls and prosopagnosic patients is also a significant inclusion in the paper. The first and foremost advantage of the proposed work is that it provides a technological means to assess the variations in face perceptual ability of individuals. It can also be used as a biomarker for early prosopagnosia diagnosis."
Exploring operational profiles and anomalies in computer performance logs,"Operational/functional problems in computer systems can be identified by monitoring and exploring performance metrics. These metrics can also be used to evaluate system activity profiles and manage relevant infrastructure (hardware and software). The critical point is finding features that make it possible to distinguish normal from abnormal system behaviour and to reveal emerging trends. This paper proposes a systematic methodology for deriving such features based on diverse observation perspectives in time (direct and aggregated) and defined specific data objects. We introduce a novel data model which combines collected samples into higher level observation objects (pulses and their compositions). This model is supported with original analysis algorithms for evaluating system behaviour. These provide useful sample/object statistics significantly enhanced with derived correlation formulas and periodicity properties. Compared with classical approaches, our model assures deeper and more accurate insight into system operation under real workload conditions as it uses new evaluation metrics and a wider scope of observation features (e.g. those related to pulse objects). In the paper, we perform exploratory studies covering real performance logs from several university servers. The results are interpreted in light of various statistical data views, including multidimensional correlation findings covering performance, event and process logs."
Fatigue damage evaluation of carbon fiber reinforced composites using nonlinear resonance spectroscopy,"The present work explores the use of nonlinear resonance spectroscopy to detect early fatigue damage in carbon fiber-epoxy composite laminates. The objective is to test composite samples at mid to high cycle fatigue (>10∧3 cycles), so that the damage evolution could be tracked. The nondestructive nature of the testing method ensures that the sample can be loaded and tested till failure, i.e. interrupted fatigue testing. The acoustic nonlinear response tracked over the fatigue life of the sample includes, the nonlinear frequency shift and modal damping ratio. Additionally, slow dynamic diagnostics was also carried out to investigate the recovery rate and recovery time. The samples that were tested were all quasi-isotropic laminates under 4-point bending load. These included a pristine sample, and two samples with impact damage to introduce variability. Tracking the nonlinear parameters over fatigue life showed a non-monotonic increase in nonlinearity. Conjectures based on acoustic response to microdamage and different stages of fatigue were used to explain the observed variation."
Fault analysis of the wear fault development in rolling bearings,"Signal processing methods are required to extract the features related to the wear process and how to track its evolution. Several signal processing methods are commonly applied in the experimental and real field tests. The generated signals of these tests are quite complex due to the dynamic nature of wear process, i.e., interaction among different wear mechanisms. Therefore, a dynamic model is required to explain the physical phenomena behind the detected signals. However, the current dynamic models in the literature lack to model the dynamic response under wear deterioration process over the whole lifetime, due to the complexity. Therefore, the purpose of this paper is to illustrate the evolution of the fault features with respect to the wear evolution process. It utilities a newly developed dynamic model and applies different commonly used signal processing methods to extract the diagnostic features of the whole wear evolution progress. The statistical time domain parameters and spectrum analysis are used in this study. Numerical results illustrate several issues related to wear evolution i.e., capabilities, weaknesses and indicators. The results show the extracted fault features and how they change with respect to the wear evolution process i.e., how the topological and tribological changes influence the extracted defect features. In this sense, the study helps to justify the experimental results in literature. The study provides a better understanding of the capability of different signal processing methods and highlights future enhancement."
Fault diagnosis based on variable-weighted separability-oriented subclass discriminant analysis,"Fisher discriminant analysis (FDA) is a classical method for discriminative dimensionality reduction, which has been extensively studied in the pattern classification field and widely used for fault diagnosis in industrial process monitoring. The classification performance of original FDA may degrade in overlapping, non-Gaussian or nonlinear data, and may be unable to extract sufficient features to interpret the data when the number of classes is small. To overcome these limitations, this paper proposes a combined variable weighting (VW) and separability-oriented subclass discriminant analysis (SSDA) scheme (denoted as VW-SSDA) for fault diagnosis. VW-SSDA utilizes K-means clustering to divide a class into subclasses using a separability oriented criterion, and all fault data are weighted by the corresponding weight vectors before applying FDA optimisation using re-defined scatter matrices. The proposed approach is applied to the Tennessee Eastman process. The results demonstrate that VW-SSDA shows better fault diagnosis performance than the several improved variants of the conventional FDA."
Fault diagnosis for underdetermined multistage assembly processes via an enhanced Bayesian hierarchical model,"Previous works have shown that only relying on measurement data is generally insufficient to identify root causes of the dimensional variation for the complex manufacturing system. It is also well known that for underdetermined multistage assembly processes (MAPs), the number of measurements is less than that of process errors so that the traditional methods are not available for the variation source identification. Therefore, there exists a substantial challenge for fault diagnosis of underdetermined MAPs, especially for the case with multiple fault patterns. To tackle this problem, a novel approach that integrates statistical analysis with domain knowledge is proposed in this paper. First, the variation propagation model is employed to reveal the inherent relationship between key control characteristics and key product characteristics, and then the corresponding variance model is constructed to interpret process faults by means of the abnormal variance of process errors. Considering that the probability of less process faults is far higher than that of more process faults in MAPs, the problem of fault diagnosis is further transformed into searching the sparse solution of abnormal variance changes for process faults. Afterwards, based on the non-negative property of the covariance matrix, an enhanced Bayesian hierarchical model is developed to favor sparse estimation of the variance for the underdetermined system in MAPs. Finally, experimental results of both the numeric simulation and practical case study demonstrate the proposed diagnosis methodology can effectively identify the process faults for different patterns with system noise."
Fault diagnosis of rolling bearing of wind turbines based on the Variational Mode Decomposition and Deep Convolutional Neural Networks,"Machine learning techniques have been successfully applied in intelligent fault diagnosis of rolling bearings in recent years. However, in the real world industrial application, the dissimilarity of data due to changes in the working conditions and data acquisition environment often cause a poor performance of the existing fault diagnosis methods. Consequently, to address these inadequacies, this paper developed a novel method by integrating the Convolutional Neural Networks (CNNs) with the Variational Mode Decomposition (VMD) algorithms. Named as “Variational Mode Decomposition with Deep Convolutional Neural Networks (VMD-DCNNs)”, the method, in an end-to-end way, directly processes raw vibration signals without artificial experiences and manual intervention to realize the fault diagnosis of rolling bearings. In addition, the CNN technique is used to extract features from each Intrinsic Mode Function (IMF) in order to address the deficiency in extracting features from a single source and to achieve an effective and efficient fault diagnosis of rolling bearings under different environments and states. The value of parameter K of the VMD-DCNNs model is optimized by considering time complexity and generalization ability of the model. Lastly, bearing experiments are conducted to verify the superiority of the VMD-DCNNs in diagnosing fault under different conditions. The visualizations of the signals in the convolutional layer explain the reasonability in selecting the value of parameter K and they also indicate that the translational invariances in a raw IMF component have been learned by the VMD-DCNNs model."
Fault diagnosis of wind turbine with SCADA alarms based multidimensional information processing method,"This paper presents a first attempt to use Dempster-Shafer (D-S) evidence theory for the fault diagnosis of wind turbine (WT) on SCADA alarm data. As two important elements in D-S evidence theory, identification framework (IF) and Basic Probability Assignment (BPA) are derived from WT maintenance records and SCADA alarm data. A procedure of multi-dimensional information fusion for WT fault diagnosis is presented. The diagnosis accuracy using BPAs obtained from a sample WT and from the wind farm are compared and evaluated. The result shows that D-S evidence theory as a multidimensional information processing method is useful for WT fault diagnosis. Compared to previous SCADA alarms processing methods, the approach proposed predominates at aspects of simple calculation, superior capability on dealing with large volume of alarms through quantifying fault probabilities. It has the advantages of being easy to perform, low cost and explainable, which make it ideal for online application. A self-BPA-generating procedure for future online application with this approach is also provided in this paper. It is concluded that D-S evidence theory applied to SCADA alarm analysis is a valuable approach to intelligent wind farm management."
Feasibility of using thermal response methods for nonintrusive compressed air flow measurement,"Robust, low-cost nonintrusive flow meters are of interest in many industries. In particular, a reliable nonintrusive flow measurement for the diagnosis of air leaks in compressed air systems is desirable. Measurement of the air flow due to leaks in the system ensures an accurate estimation of potential cost and energy savings. This study evaluates a novel method of using thermal responses to nonintrusively measure leakage rates in compressed air lines. The method uses heat and the resulting thermal response to calculate the flow rate inside the compressed air line. Compared to the current methods for flow measurement, this method can simplify flow measurement while decreasing the sensitivity to errors when measuring flow rates. In this study, the methodology of the proposed method is explained along with the potential advantages to the design. Two approaches are evaluated: a dynamic step response and sinusoidal frequency response. Simulated tests evaluate the feasibility of the proposed methods, followed by experiments that validate the simulation results. A clear correlation between the thermal step response and the flow rate indicate viability of the proposed method in simulation. Experimental results yielded similar results, confirming the validity of the proposed method. The results of a field test in an industrial environment demonstrate the capability of the approach to other flow rate measurement techniques."
Feature Selection for Anomaly Detection Using Optical Emission Spectroscopy,"To maintain the pace of development set by Moore’s law, production processes in semiconductor manufacturing are becoming more and more complex. The development of efficient and interpretable anomaly detection systems is fundamental to keeping production costs low. As the dimension of process monitoring data can become extremely high anomaly detection systems are impacted by the curse of dimensionality, hence dimensionality reduction plays an important role. Classical dimensionality reduction approaches, such as Principal Component Analysis, generally involve transformations that seek to maximize the explained variance. In datasets with several clusters of correlated variables the contributions of isolated variables to explained variance may be insignificant, with the result that they may not be included in the reduced data representation. It is then not possible to detect an anomaly if it is only reflected in such isolated variables. In this paper we present a new dimensionality reduction technique that takes account of such isolated variables and demonstrate how it can be used to build an interpretable and robust anomaly detection system for Optical Emission Spectroscopy data."
Feature selection using ant colony optimization with tandem-run recruitment to diagnose bronchitis from CT scan images,"Background and objectives Computer-aided diagnosis (CAD) plays a vital role in the routine clinical activity for the detection of lung disorders using computed tomography (CT) images. It serves as a source of second opinion that radiologists may consider in order to interpret CT images. In this work, the purpose of CAD is to improve the diagnostic accuracy of pulmonary bronchitis from CT images of the lung. Methods Left and right lung fields are segmented using optimal thresholding from the lung CT images. Texture and shape features are extracted from the pathology bearing regions. A hybrid feature selection approach based on ant colony optimization (ACO) combining cosine similarity and support vector machine (SVM) classifier is used to select relevant features. Additionally, tandem run recruitment strategy is included in the selection activity to choose the promising features. The SVM classifier is trained using the selected features and the performance of the trained classifier is evaluated using trivial performance evaluation measures. Results The training and testing datasets used in building the classifier model are disjoint and contains 200 CT slices affected with bronchitis, 50 normal slices and 300 slices with cancer. Out of 100 features extracted from each CT slice, a subset of 60 features is used for classification. ACO with tandem run strategy yielded 81.66% of accuracy whereas ACO without tandem run yielded an accuracy of 77.52%. When all the features are used for classifier training without feature selection algorithm, an accuracy of 75.14% is achieved. Conclusion From the results, it is inferred that identifying relevant features to train the classifier has a definite impact on the classifier performance."
Fine-tuning Pre-trained Convolutional Neural Networks for Gastric Precancerous Disease Classification on Magnification Narrow-band Imaging Images,"Gastric cancer(GC) is the fourth leading cause of cancer death worldwide. To prevent the occurrence of advanced GCs, there is a need for immediate detection and treatment of gastric precancerous and early cancerous lesions. Magnification endoscopy with narrow-band imaging (M-NBI) system as an advanced diagnostic imaging technology is widely used in evaluating gastric lesion types, which can interpret gastric lesion characteristics by enhancing contrasts between vessels and mucosal surfaces. Based on microvascular morphologies presented on M-NBI images, physicians can manually diagnose gastric lesions; but this is a tough work for unexperienced doctors and it is lacking of objectivity. In this study, we propose a transfer learning framework by fine-tuning pre-trained convolutional neural networks (CNNs) to classify gastric M-NBI images into three classes: chronic gastritis (CGT), low grade neoplasia (LGN) and early gastric cancer (EGC). The method we choose is used to compare with three kinds of traditional handcraft texture feature extraction methods and CNN models trained directly by our dataset. Results show that the performance of fine-tuned CNNs outperforms traditional handcraft features and trained CNNs. Experiments also illustrate that ResNet50 can achieve 0.96 accuracy, 0.92, 0.91 and 0.99 f1-scores for classifying M-NBI images into CGT, LGN and EGC. In conclusion, the proposed framework is suit for multi-classification tasks of gastric M-NBI images."
Finite element analysis of uncommonly large renal arteriovenous malformation—Adjacent renal cyst complex,"Background Renal arteriovenous malformation (RAVM) represents abnormal communication between the intrarenal arterial and venous system. The purpose of this study was to investigate hemodynamics and biomechanics quantities which may influence the instability of RAVM and imply clinical complications. Methods A detailed 3D reconstruction of RAVM was obtained from the patient CT scans, aortic inlet flow was measured by color-flow Doppler ultrasound, while material characteristics were adopted from the literature. A numerical finite element analysis (FEA) of the blood flow was performed by solving the governing equations for the viscous incompressible flow. The physical quantities calculated at the systolic and diastolic peak moment were velocity, pressure, shear stress and drag forces. Results We reported a case of a 50-year-old patient with a large RAVM and adjacent renal cyst, who unsuccessfully underwent two attempts of embolization that resulted in the consequent nephrectomy. FEA showed that the cyst had a very low pressure intensity and velocity field (with unstable flow in diastolic peak). For both systolic and diastolic moments, increased values of wall shear stress were found on the places with intensive wall calcification. Unusually high values of drag force which would likely explain the presence of pressure in the cystic formation were found on the infero-medial side where the cyst wall was the thinnest and where the flow streamlines converged. Conclusions FEA showed that the hemodynamics of the cyst-RAVM complex was unstable making it prone to rupture. Clinically established diagnosis of imminent rupture together with unfavorable hemodynamics of the lesion consequently made additional attempts of embolization risky and unsuccessful leading to total nephrectomy."
FLAGS: A methodology for adaptive anomaly detection and root cause analysis on sensor data streams by fusing expert knowledge with machine learning,"Anomalies and faults can be detected, and their causes verified, using both data-driven and knowledge-driven techniques. Data-driven techniques can adapt their internal functioning based on the raw input data but fail to explain the manifestation of any detection. Knowledge-driven techniques inherently deliver the cause of the faults that were detected but require too much human effort to set up. In this paper, we introduce FLAGS, the Fused-AI interpretabLe Anomaly Generation System, and combine both techniques in one methodology to overcome their limitations and optimize them based on limited user feedback. Semantic knowledge is incorporated in a machine learning technique to enhance expressivity. At the same time, feedback about the faults and anomalies that occurred is provided as input to increase adaptiveness using semantic rule mining methods. This new methodology is evaluated on a predictive maintenance case for trains. We show that our method reduces their downtime and provides more insight into frequently occurring problems."
Flexible vacuum vessel bolometer camera design in ITER to adapt to the final position of the gaps between Blanket Modules,"Bolometer cameras in ITER will be mounted, among others, on the Vacuum Vessel (VV) wall behind Blanket Modules (BMs). For the first assembly phase the platform (Cable fixations and the lower part of the internal signal chain) of VV cameras has to be delivered to fix the cables and protect their termination. During First Plasma, the as-built magnetic axis and magnetic flux surface will be measured and the BMs fixation will be adjusted to align them to the shape of the magnetic flux surfaces. Adjustments of the BMs of up to ±13,3 mm in toroidal and poloidal direction and up to ±20,3 mm in radial direction are foreseen. Accordingly, VV cameras need adjustments, too, to assure a proper view of the plasma. Calculations have been performed to define the impact of the possible BM movements onto the bolometer viewing cones. The toroidal and poloidal movements can be followed by shifting the collimator and the sensor along with the BMs. The radial movement of BMs was transferred to an additional poloidal shift of the sensor and collimator. The resulting concept for camera design with its complex space envelope will be presented. It provides an alignment flexibility of the cameras of ±31 mm poloidally and ±16 mm toroidally. Because of the short period between magnetic measurements and second assembly phase, the number of customizable parts is kept as low as possible."
Flow field characterization of pressurized sooting swirl flames and relation to soot distributions,"Mean and instantaneous flow fields were derived for sooting pressurized swirl flames, operated with ethylene/air in an aero-engine model combustor. Stereo particle image velocimetry served to deduce three velocity components and to identify locations of soot based on soot scattering. The measurements complement those of other quantities in the same flames published recently. Flow fields determined for cold and reactive conditions confirm conclusions drawn from application of other laser-based diagnostics: soot is mainly formed in the inner recirculation zone which recirculates reactive, hot unburnt reaction products, and partly transported into the high-velocity in-flow regions. Oxidation air injected after two thirds of the combustor forms a stagnation zone close to the combustor axis and splits into a portion flowing downstream toward the combustor exit and one transported upstream thereby affecting the local gas composition and temperatures in the inner recirculation zone. Analysis of the instantaneous images by proper orthogonal decomposition reveals the existence of a precessing vortex core which impacts the soot distribution. Presence of soot in high-velocity/high strain rate regions where soot formation is unlikely to occur can be explained as a result of transport. Flow field characterization and the correlation with soot presence, in complement of existing data, are expected to provide a valuable contribution to soot model validation."
Follow-up study of clinical and chest CT scans in confirmed COVID-19 patients,"Objective To analyze the CT imaging results of patients with COVID-19 who previously received several follow-up visits and to explain the changes in pulmonary inflammation. Methods Cases of 15 patients with COVID-19 were retrospectively analyzed: their epidemiology, clinical history, laboratory tests, and multiple CT chest scans obtained during the disease period were studied. Results The CT scans of the 15 patients showed different results. Four patients had no abnormal findings in their chest CT scans. The first scan of 1 patient revealed right lower lobe inflammation, while the lesion had been completely absorbed in follow-up. Two patients showed bilateral pulmonary inflammation in the first scan which had been absorbed by follow-up but the last examination showed extensive fibrosis. Two patients had no abnormalities in their first CT scans, while pulmonary inflammation was found in the second scan and this had not been completely absorbed by the last follow-up. One patient had pulmonary interstitial lesions with no evidence of National Cochlear Implant Programme (NCIP) on the first and second CT scans. NCIP was found at the third scan, and pulmonary inflammation was not completely absorbed at the last follow-up. Three patients were in the early stage of inflammation at the first scan, and the lesions were absorbed and repaired at the last follow-up. However, the lesions were not completely absorbed. One patient was in the advanced stage at the first scan, and the last follow-up pulmonary lesions were not completely absorbed. The first CT scan of 1 patient revealed large ground-glass opacity in the lungs involving the inner and middle bands. After follow-up, the disease progressed, and this condition was consistent with severe manifestations. Conclusion The follow-up of chest CT can reflect the change process of NCIP and the treatment effect. The first CT scan of lung lesions has a certain predictive effect on the outcome and prognosis of patients."
Forecasting remaining useful life: Interpretable deep learning approach via variational Bayesian inferences,"Predicting the remaining useful life of machinery, infrastructure, or other equipment can facilitate preemptive maintenance decisions, whereby a failure is prevented through timely repair or replacement. This allows for a better decision support by considering the anticipated time-to-failure and thus promises to reduce costs. Here a common baseline may be derived by fitting a probability density function to past lifetimes and then utilizing the (conditional) expected remaining useful life as a prognostic. This approach finds widespread use in practice because of its high explanatory power. A more accurate alternative is promised by machine learning, where forecasts incorporate deterioration processes and environmental variables through sensor data. However, machine learning largely functions as a black-box method and its forecasts thus forfeit most of the desired interpretability. As our primary contribution, we propose a structured-effect neural network for predicting the remaining useful life which combines the favorable properties of both approaches: its key innovation is that it offers both a high accountability and the flexibility of deep learning. The parameters are estimated via variational Bayesian inferences. The different approaches are compared based on the actual time-to-failure for aircraft engines. This demonstrates the performance and superior interpretability of our method, while we finally discuss implications for decision support."
Forecasting remaining useful life: Interpretable deep learning approach via variational Bayesian inferences,"Predicting the remaining useful life of machinery, infrastructure, or other equipment can facilitate preemptive maintenance decisions, whereby a failure is prevented through timely repair or replacement. This allows for a better decision support by considering the anticipated time-to-failure and thus promises to reduce costs. Here a common baseline may be derived by fitting a probability density function to past lifetimes and then utilizing the (conditional) expected remaining useful life as a prognostic. This approach finds widespread use in practice because of its high explanatory power. A more accurate alternative is promised by machine learning, where forecasts incorporate deterioration processes and environmental variables through sensor data. However, machine learning largely functions as a black-box method and its forecasts thus forfeit most of the desired interpretability. As our primary contribution, we propose a structured-effect neural network for predicting the remaining useful life which combines the favorable properties of both approaches: its key innovation is that it offers both a high accountability and the flexibility of deep learning. The parameters are estimated via variational Bayesian inferences. The different approaches are compared based on the actual time-to-failure for aircraft engines. This demonstrates the performance and superior interpretability of our method, while we finally discuss implications for decision support."
Forecasting remaining useful life: Interpretable deep learning approach via variational Bayesian inferences,"Predicting the remaining useful life of machinery, infrastructure, or other equipment can facilitate preemptive maintenance decisions, whereby a failure is prevented through timely repair or replacement. This allows for a better decision support by considering the anticipated time-to-failure and thus promises to reduce costs. Here a common baseline may be derived by fitting a probability density function to past lifetimes and then utilizing the (conditional) expected remaining useful life as a prognostic. This approach finds widespread use in practice because of its high explanatory power. A more accurate alternative is promised by machine learning, where forecasts incorporate deterioration processes and environmental variables through sensor data. However, machine learning largely functions as a black-box method and its forecasts thus forfeit most of the desired interpretability. As our primary contribution, we propose a structured-effect neural network for predicting the remaining useful life which combines the favorable properties of both approaches: its key innovation is that it offers both a high accountability and the flexibility of deep learning. The parameters are estimated via variational Bayesian inferences. The different approaches are compared based on the actual time-to-failure for aircraft engines. This demonstrates the performance and superior interpretability of our method, while we finally discuss implications for decision support."
Fully automatic 3D reconstruction of the placenta and its peripheral vasculature in intrauterine fetal MRI,"Recent advances in fetal magnetic resonance imaging (MRI) open the door to improved detection and characterization of fetal and placental abnormalities. Since interpreting MRI data can be complex and ambiguous, there is a need for robust computational methods able to quantify placental anatomy (including its vasculature) and function. In this work, we propose a novel fully-automated method to segment the placenta and its peripheral blood vessels from fetal MRI. First, a super-resolution reconstruction of the uterus is generated by combining axial, sagittal and coronal views. The placenta is then segmented using 3D Gabor filters, texture features and Support Vector Machines. A uterus edge-based instance selection is proposed to identify the support vectors defining the placenta boundary. Subsequently, peripheral blood vessels are extracted through a curvature-based corner detector. Our approach is validated on a rich set of 44 control and pathological cases: singleton and (normal / monochorionic) twin pregnancies between 25–37 weeks of gestation. Dice coefficients of 0.82  ±  0.02 and 0.81  ±  0.08 are achieved for placenta and its vasculature segmentation, respectively. A comparative analysis with state of the art convolutional neural networks (CNN), namely, 3D U-Net, V-Net, DeepMedic, Holistic3D Net, HighRes3D Net and Dense V-Net is also conducted for placenta localization, with our method outperforming all CNN approaches. Results suggest that our methodology can aid the diagnosis and surgical planning of severe fetal disorders."
Game theoretic interpretability for learning based preoperative gliomas grading,"Gliomas are the most common primary tumors occurring in the central nervous system. Accurate gliomas grading is crucial for prognosis assessment and optimal treatment on the part of the patient. This study aims to develop and validate a pretreatment MRI-based noninvasive machine learning radiomics model for preoperatively grading glioma, and to simultaneously interpret the radiomics features used in the model. Firstly, wavelet transform and Laplacian of Gaussian (LoG) filtering are used during image preprocessing and a total of 1024 quantitative features are extracted from the region of interest (ROI) of the tumor, which is manually delineated on the largest slice of MRI images. Then, feature selection is performed by Pearson correlation coefficient and the least absolute shrinkage and selection operator (LASSO). Finally, extreme gradient boosting (XGBoost) is built to carry out the glioma grading, and Shapley value is used to quantitatively interpret and reveal the important features contributing to grading. Experimental results on a benchmark dataset demonstrate that XGBoost is effective and efficient for the diagnosis of glioma. Accuracy, sensitivity, specificity, and AUC are 0.83, 0.86, 0.81, 0.86, respectively. These results add evidence of the important role of radiomics model based on only one representative MRI image in preoperatively grading glioma. The quantitative analysis and interpretation may assist clinicians to better understand the disease and select appropriate treatment for improving clinical outcomes."
Gated temporal convolutional neural network and expert features for diagnosing and explaining physiological time series: A case study on heart rates,"Background and Objective: Physiological time series are common data sources in many health applications. Mining data from physiological time series is crucial for promoting healthy living and reducing governmental medical expenditure. Recently, research and applications of deep learning methods on physiological time series have developed rapidly because such data can be continuously recorded by smart wristbands or smartwatches. However, existing deep learning methods suffer from excessive model complexity and a lack of explanation. This paper aims to handle these issues. Methods: We propose TEG-net, which is a novel deep learning method for accurately diagnosing and explaining physiological time series. TEG-net constructs T-net (a multi-scale bi-directional temporal convolutional neural network) to model physiological time series directly, E-net (personalized linear model) to model expert features extracted from physiological time series, and G-net (gating neural network) to combine T-net and E-net for diagnosis. The combination of T-net and E-net through G-net improves diagnosis accuracy and E-net can be utilized for explanation. Results: Experimental results demonstrate that TEG-net outperforms the second-best baseline by 13.68% in terms of area under the receiver operating characteristic curve and 11.49% in terms of area under the precision-recall curve. Additionally, intuitive justifications can be provided to explain model predictions. Conclusions: This paper develops an ensemble method to combine expert features and deep learning method for modeling physiological time series. Improvements in diagnostic accuracy and explanation make TEG-net applicable to many real-world health applications."
Generation of Signed Directed Graphs Using Functional Models⁎⁎This work is supported by the Danish Hydrocarbon Research and Technology Centre.,"Intelligent fault diagnosis systems can be a major aid to human operators charged with the high-level control of industrial plants. Such systems aim for high diagnostic accuracy while retaining the ability to produce results that can be interpreted by human experts on site. Signed directed graphs have been shown to be a viable method for plant-wide diagnosis that can incorporate both quantitative information about the process condition as well as qualitative information about the system topology and the functions of its components. Their range of application in industrial settings has been limited due to difficulties regarding the interpretation of results and consistent graph generation. This contribution addresses these issues by proposing an automated generation of signed directed graphs of industrial processes in the chemical, petroleum and nuclear industries using Multilevel Flow Modeling; a functional modeling method designed for operator support. The approach is demonstrated through a case study conducted on the Tennessee Eastman Process, showing that Multilevel Flow Modeling can facilitate a consistent modeling process for signed directed graphs. Finally, the resulting benefits regarding qualitative reasoning for plant-wide diagnosis are discussed."
Geometric precision analysis for Additive Manufacturing processes: A comparative study,"Additive Manufacturing (AM) has recently attracted increasing attention among manufacturing industries. This class of technologies is capable of creating parts with complex shapes and intricate structures. However, the poor geometric quality of the parts they produced is a major constraint in wide industrial adoption. Currently available analytical techniques based on classic measurement equipment could fail in analyzing the process parameters based on AM-created parts because of the layer-by-layer fabrication process. In this article, we introduce a novel three-dimensional point-cloud-based analytical toolset, volumetric data analysis (VDA), for AM-oriented metrological and experimental analysis. Each step of the VDA is discussed in detail. A high dimensional hypothesis testing procedure is proposed to compare the geometric precision of the part samples from two printing settings. New visualization tools for deviation diagnostics are presented to aid in interpreting and comparing the process outputs. The proposed methods are illustrated with a real experiment to compare the effects of different layer thicknesses in a filament deposition modeling printing process."
GEV-NN: A deep neural network architecture for class imbalance problem in binary classification,"Class imbalance is a common issue in many applications such as medical diagnosis, fraud detection, web advertising, etc. Although standard deep learning method has achieved remarkably high-performance on datasets with balanced classes, its ability to classify imbalanced dataset is still limited. This paper proposes a novel end-to-end deep neural network architecture and adopts Gumbel distribution as an activation function in neural networks for class imbalance problem in the application of binary classification. Our proposed architecture, named GEV-NN, consists of three components: the first component serves to score input variables to determine a set of suitable input, the second component is an auto-encoder that learns efficient explanatory features for the minority class, and in the last component, the combination of the scored input and extracted features are then used to make the final prediction. We jointly optimize these components in an end-to-end training. Extensive experiments using real-world imbalanced datasets showed that GEV-NN significantly outperforms the state-of-the-art baselines by around 2% at most. In addition, the GEV-NN gives a beneficial advantage to interpret variable importance. We find key risk factors for hypertension, which are consistent with other scientific researches, using the first component of GEV-NN."
Ground-penetrating radar method used for the characterisation of ornamental stone quarries,"In the work non-destructive ground-penetrating radar (GPR) was used to probe the texture and presence of anisotropies in different types of carbonated rocks intended for ornamental use. First, GPR on the different facies of a marble quarry in Macael (Almería, Spain) was applied where alternating layers of marble and mica schists can be found. GPR allows for the differentiation of the marble units from the mica schist units, which makes GPR a good tool for indirectly evaluating the reserves in a deposit. In addition, it allows for the detection of the different anisotropies in the marble units (holes, fractures), and this information could be utilised to formulate a work-plan design. Antennae of different frequencies were employed (100, 250 and 800MHz) for these purposes, and the 250MHz antenna had the most effective probing capacity for obtaining an accurate depth resolution. Second, a GPR study was conducted in three types of rock known commercially as Macael Marble, Crema Marfil and Red Travertine before the block-cutting process. For this configuration, the 800MHz antenna was used to differentiate the textures of each type of rock as well as the location of a variety of anisotropies, and the results showed that GPR is an effective tool for evaluating the block quality, determining whether resins must be injected to consolidate the block and estimating the orientation of the cutting process. In the work, the use of a supervised two-dimensional (i.e., radargram) probabilistic latent component analysis (PLCA) approach is proposed to highlight only the information from target objects (i.e., marble anisotropy) provided by the radargram, which facilitates the data interpretation by the user. This approach can search activations of the georadar pulse across both dimensions. Once the analysis has been performed, a variable-gain compensation process is proposed to outperform the energy losses that result from the reflections of the signal when a different material is found. The results have shown that the GPR method can be utilised as a tool for the diagnosis of stone materials prior to their use in artistic work or monuments; therefore, GPR may be considered a technique for material selection. The presence of discontinuities (sometimes visible to the naked eye) explain many of the phenomena and typologies of the stone after its eventual alteration, and such discontinuities could be avoided by means of this pre-emptive work."
Group reduced kernel extreme learning machine for fault diagnosis of aircraft engine,"The original kernel extreme learning machine (KELM) employs all training samples to construct hidden layer, thus avoiding the performance fluctuations caused by the ELM randomly assigning weights. However, excessive nodes will inevitably lead to structural redundancy, which hinders its application in systems with high real-time performance requirements but limited onboard storage and computing capacity. Considering the well interpretability of sparse learning, this study introduces the group sparse structure for KELM to resolve its limitation of structural redundancy. Specifically, the proposed novel method introduces a special norm to reformulate the dual optimization problem of KELM to realize group sparse structure in output weights. As a result, nodes with large weights can be selected as the significant nodes, while nodes with small weights will be regarded as the redundant nodes and neglected directly. In addition, we have also devised an alternating iterative optimization algorithm and deduced the complete proof of convergence to solve the non-smoothness optimization problem in proposed method. Then, the validity and feasibility of the proposed method are verified by extensive experiments on benchmark datasets. More importantly, tests of fault diagnosis for an aircraft engine show that the proposed approach can maintain the competitive recognition performance with much faster testing speed."
Guaranteeing robustness of structural condition monitoring to environmental variability,"Advances in sensor deployment and computational modeling have allowed significant strides to be recently made in the field of Structural Health Monitoring (SHM). One widely used SHM strategy is to perform a vibration analysis where a model of the structure's pristine (undamaged) condition is compared with vibration response data collected from the physical structure. Discrepancies between model predictions and monitoring data can be interpreted as structural damage. Unfortunately, multiple sources of uncertainty must also be considered in the analysis, including environmental variability, unknown model functional forms, and unknown values of model parameters. Not accounting for these sources of uncertainty can lead to false-positives or false-negatives in the structural condition assessment. To manage the uncertainty, we propose a robust SHM methodology that combines three technologies. A time series algorithm is trained using “baseline” data to predict the vibration response, compare predictions to actual measurements collected on a potentially damaged structure, and calculate a user-defined damage indicator. The second technology handles the uncertainty present in the problem. An analysis of robustness is performed to propagate this uncertainty through the time series algorithm and obtain the corresponding bounds of variation of the damage indicator. The uncertainty description and robustness analysis are both inspired by the theory of info-gap decision-making. Lastly, an appropriate “size” of the uncertainty space is determined through physical experiments performed in laboratory conditions. Our hypothesis is that examining how the uncertainty space changes throughout time might lead to superior diagnostics of structural damage as compared to only monitoring the damage indicator. This methodology is applied to a portal frame structure to assess if the strategy holds promise for robust SHM. (Publication approved for unlimited, public release on October-28-2015, LA-UR-15-28442, unclassified.)"
Health Management Design Considerations for an All Electric Aircraft,"This paper explains the On-board IVHM system for a State-Of-the-Art “All electric aircraft” and explores implementing practices for analysis based design, illustrations and development of IVHM capabilities. On implementing the system as an on board system will carry out fault detection and isolation, recommend maintenance action, provides prognostic capabilities to highest possible problems before these became critical. The vehicle Condition Based Maintenance (CBM) and adaptive control algorithm development based on an open architecture system which allow “Plug in and Plug off” various systems in a more efficient and flexible way. The scope of the IVHM design included consideration of data collection and communication from the continuous monitoring of aircraft systems, observation of current system states, and processing of this data to support proper maintenance and repair actions. Legacy commercial platforms and HM applications for various subsystems of these aircraft were identified. The list of possible applications was down-selected to a reduced number that offer the highest value using a QFD matrix based on the cost benefit analysis. Requirements, designs and system architectures were developed for these applications. The application areas considered included engine, tires and brakes, pneumatics and air conditioning, generator, and structures. IVHM design program included identification of application sensors, functions and interfaces; IVHM system architecture, descriptions of certification requirements and approaches; the results of a cost/benefit analyses and recommended standards and technology gaps. The work concluded with observations on nature of HM, the technologies, and the approaches and challenges to its integration into the current avionics, support system and business infrastructure. The IVHM design for All Electric Hybrid Wing Body (HWB) Aircraft has a challenging task of addressing and resolving the shortfalls in the legacy IVHM framework. The challenges like sensor battery maintenance, handling big data from SHM, On-Ground Data transfer by light, Extraction of required features at sensor nodes/RDCUs, ECAM/EICAS Interfaces, issues of certification of wireless SHM network has been addressed in this paper. Automatic Deployable Flight Data recorders are used in the design of HWB aircraft in which critical flight parameters are recorded. The component selection of IVHM system including software and hardware have been based on the COTS technology. The design emphasis on high levels of reliability and maintainability. The above systems are employed using IMA and integrated on AFDX data bus. The design activities has to pass through design reviews on systematic basis and the overall approach has been to make system highly lighter, effective “All weather” compatible and modular. It is concluded from the study of advancement in IVHM capabilities and new service offerings that IVHM technology is emerging as well as challenging. With the inclusion of adaptive control, vehicle condition based maintenance and pilot fatigue monitoring, IVHM evolved as a more proactively involved on-board system."
HealthXAI: Collaborative and explainable AI for supporting early diagnosis of cognitive decline,"Our aging society claims for innovative tools to early detect symptoms of cognitive decline. Several research efforts are being made to exploit sensorized smart-homes and artificial intelligence (AI) methods to detect a decline of the cognitive functions of the elderly in order to promptly alert practitioners. Even though those tools may provide accurate predictions, they currently provide limited support to clinicians in making a diagnosis. Indeed, most AI systems do not provide any explanation of the reason why a given prediction was computed. Other systems are based on a set of rules that are easy to interpret by a human. However, those rule-based systems can cope with a limited number of abnormal situations, and are not flexible enough to adapt to different users and contextual situations. In this paper, we tackle this challenging problem by proposing a flexible AI system to recognize early symptoms of cognitive decline in smart-homes, which is able to explain the reason of predictions at a fine-grained level. Our method relies on well known clinical indicators that consider subtle and overt behavioral anomalies, as well as spatial disorientation and wandering behaviors. In order to adapt to different individuals and situations, anomalies are recognized using a collaborative approach. We experimented our approach with a large set of real world subjects, including people with MCI and people with dementia. We also implemented a dashboard to allow clinicians to inspect anomalies together with the explanations of predictions. Results show that our system’s predictions are significantly correlated to the person’s actual diagnosis. Moreover, a preliminary user study with clinicians suggests that the explanation capabilities of our system are useful to improve the task performance and to increase trust. To the best of our knowledge, this is the first work that explores data-driven explainable AI for supporting the diagnosis of cognitive decline."
HealthXAI: Collaborative and explainable AI for supporting early diagnosis of cognitive decline,"Our aging society claims for innovative tools to early detect symptoms of cognitive decline. Several research efforts are being made to exploit sensorized smart-homes and artificial intelligence (AI) methods to detect a decline of the cognitive functions of the elderly in order to promptly alert practitioners. Even though those tools may provide accurate predictions, they currently provide limited support to clinicians in making a diagnosis. Indeed, most AI systems do not provide any explanation of the reason why a given prediction was computed. Other systems are based on a set of rules that are easy to interpret by a human. However, those rule-based systems can cope with a limited number of abnormal situations, and are not flexible enough to adapt to different users and contextual situations. In this paper, we tackle this challenging problem by proposing a flexible AI system to recognize early symptoms of cognitive decline in smart-homes, which is able to explain the reason of predictions at a fine-grained level. Our method relies on well known clinical indicators that consider subtle and overt behavioral anomalies, as well as spatial disorientation and wandering behaviors. In order to adapt to different individuals and situations, anomalies are recognized using a collaborative approach. We experimented our approach with a large set of real world subjects, including people with MCI and people with dementia. We also implemented a dashboard to allow clinicians to inspect anomalies together with the explanations of predictions. Results show that our system’s predictions are significantly correlated to the person’s actual diagnosis. Moreover, a preliminary user study with clinicians suggests that the explanation capabilities of our system are useful to improve the task performance and to increase trust. To the best of our knowledge, this is the first work that explores data-driven explainable AI for supporting the diagnosis of cognitive decline."
Heat transfer and instability characteristics of a loop thermosyphon with wide range of filling ratios,"A series of experiment using flow visualization technique and simultaneous measurement of pressure and temperature signals, are conducted to thoroughly investigate the heat transfer and instability characteristics in a loop thermosyphon with filling ratios ranging widely from 38% to 87% over a broad range of input heat flux from 35 to 395 W cm−2. The filling ratios are classified into three groups based on their distinctive heat transfer and instability behavior. The effects of filling ratio, input heat flux and inclination angle on the temperature nonuniformity, thermal resistance and instability features are examined in detail. The results show that, under the high filling ratios, the heat transfer capacity is seriously limited by the phase-change suppression and the consequent pressure soaring, while is advantageous in avoiding the evaporator dryout. Under the moderate filling ratios, the heat transfer capacity could be significantly higher, but the geyser boiling instability is obvious for a relatively large range of input heat flux, inducing the fluctuation of pressure and temperature. Under the low filling ratios, the heat transfer capacity is the highest; however, the transient local “Near-dryout” is easier to occur, causing large temperature overshoot and oscillation. The different variation trends of thermal resistance and temperature nonuniformity with filling ratios and input heat flux can generally be explained by the variation of the portions of latent and sensible heat transfer. Under the moderate and high filling ratios, the loop thermal resistance decreases somewhat with the decrease of inclination angle due to the elongation of the phase-change surfaces. A thermodynamic state diagram is drawn, indicating the measured average pressure - temperature of the working fluid at the evaporator for the three groups of filling ratios, with distinctly different instability issues, along with the saturation line to aid in the mechanism analysis and trouble diagnosis. A stability map in terms of the phase change number, Npch, and the filling ratio is presented, in which the boundaries dividing the different types of instability and the stable conditions are marked as a guidance for practical design and operation."
HeTROPY: Explainable learning diagnostics via heterogeneous maximum-entropy and multi-spatial knowledge representation,"Autonomous learning diagnostics, where the students’ strengths and weaknesses are disclosed from their observed performance data, is a challenging task in e-learning systems. Current student knowledge models can alleviate some of the problems in learning (i.e. predicting student performance) but they neglect learning diagnostics, which is based on causal reasoning. To this end, we propose a novel heterogeneous attention interpreter with a maximum entropy regularizer on top of a student knowledge model to achieve explainable learning diagnostics. Our model segregates the impact of the homogeneous knowledge points, while promoting the heterogeneous relatives by maximizing their chance to contribute to the prediction. We also propose a multi-spatial knowledge representation that is readily generalizable to other data-driven educational tasks. Extensive experiments on real-world datasets reveal that the proposed method is able to enhance the model’s explanatory power, hence increases the trustworthiness towards learning diagnostics. It also brings notable improvement in accuracy in the student performance prediction task. The findings in this paper are adoptable to various types of e-learning systems to assist teachers to gain insights into student learning states and diagnose learning problems."
Hidden factors and handling strategies on virtual in-situ sensor calibration in building energy systems: Prior information and cancellation effect,"Sensor errors greatly affect the performance of control, diagnosis, and optimization systems within building energy systems, negatively impacting energy efficiency. Virtual in-situ sensor calibration (VIC), a Bayesian theory based method, can improve building energy performance by calibrating erroneous sensors in working building energy systems on a large scale. Working sensors do not need to be removed nor will reference sensors need to be added, as is done in a conventional calibration. To improve the calibration accuracy, hidden factors and their negative effects on the accuracy of a VIC must be addressed properly. In this study, we define (1) prior information and (2) cancellation effects as the negative effects. The suggested VIC method is applied to a single energy system component and to a LiBr-H2O absorption refrigeration system, respectively, to discuss the two primary effects (mentioned above). In addition to adding data sets, two strategies—inclusion of local calibration and conducting repetitive prior updates—are proposed to solve the hidden factors’ issue. The case study (1) shows that the proposed local calibration with the prior updates can solve the two negative effects, thus suggesting the high calibration accuracy and (2) demonstrates that the calibrated measurements improve the accuracy of energy performance analysis for a building energy system (up to 17.82%)."
Hierarchical hyper-Laplacian prior for weak fault feature enhancement,"Sparsity-assisted methods are one of the most effective fault feature extraction methods which have been widely studied recently. However, no one has explained or discussed the choice of a suitable sparse prior from the perspective of the probability theory. In this paper, we define a hierarchical hyper-Laplacian prior induced model (HHLP) through maximizing the posterior probability for bearing fault diagnosis. In the proposed model, we conclude that the hyper-Laplacian prior can better model coefficients of fault feature than the Laplacian prior. Furthermore, we introduce a hierarchical hyper-Laplacian prior which embeds the physical characteristics to discriminate the harmonic interference. The main insight of this paper is that we provide a new way to model the sparse prior from the perspective of maximizing the posterior probability. Numerical simulations and an experimental application are performed to demonstrate performance of HHLP. Meanwhile, comparisons with the kurtosis based weighted sparse model, the generalized minimax-concave regularization inducing model, and the spectral kurtosis also verify the effectiveness of HHLP."
HMV: A medical decision support framework using multi-layer classifiers for disease prediction,"Decision support is a crucial function for decision makers in many industries. Typically, Decision Support Systems (DSS) help decision-makers to gather and interpret information and build a foundation for decision-making. Medical Decision Support Systems (MDSS) play an increasingly important role in medical practice. By assisting doctors with making clinical decisions, DSS are expected to improve the quality of medical care. Conventional clinical decision support systems are based on individual classifiers or a simple combination of these classifiers which tend to show moderate performance. In this research, a multi-layer classifier ensemble framework is proposed based on the optimal combination of heterogeneous classifiers. The proposed model named “HMV” overcomes the limitations of conventional performance bottlenecks by utilizing an ensemble of seven heterogeneous classifiers. The framework is evaluated on two different heart disease datasets, two breast cancer datasets, two diabetes datasets, two liver disease datasets, one Parkinson's disease dataset and one hepatitis dataset obtained from public repositories. Effectiveness of the proposed ensemble is investigated by comparison of results with several well-known classifiers as well as ensemble techniques. The experimental evaluation shows that the proposed framework dealt with all types of attributes and achieved high diagnosis accuracy. A case study is also presented based on a real time medical dataset in order to show the high performance and effectiveness of the proposed model."
Hot surface ignition of n-hexane in air,"An experimental investigation is conducted to analyze hot-surface ignition of n-hexane-air mixtures. The experimental setup, equipped with temperature diagnostics and schlieren imaging, utilizes a glow plug to initiate ignition in a flammable mixture. The hot-surface temperature at the point of ignition is measured for equivalence ratios ranging from 0.6 to 3 and chamber pressures varying from 25 to 100 kPa. The hot-surface temperature resulting in ignition is found to be weakly sensitive to equivalence ratio with a mean value of 980 K for mixtures with equivalence ratios between 0.75 and 3 at 100 kPa. Chamber pressure has a stronger influence with ignition temperature increasing to about 1140 K at 25 kPa. The experimental trends were reproduced in numerical simulations utilizing detailed chemistry of n-heptane as a surrogate for n-hexane given their similar ignition and flame propagation characteristics. The simulations further predict a two-stage ignition process resulting from an initial breakdown of the fuel with a small increase in temperature followed by a main ignition event accompanied by fuel depletion. Reaction rate analysis of the sequence of events leading to ignition conducted using a reduced order kinetic model suggests that the second-stage ignition event is caused primarily by the decomposition of hydrogen peroxide which occurs at temperatures above 900 K. The two-stage ignition process observed here is significantly different from that observed in previous studies due to the presence of convective and diffusive processes as well as the continuous increase in hot-surface temperature. These arguments are used to explain the insensitivity of ignition temperature to equivalence ratio, its decrease with increasing chamber pressure, and the location of the ignition kernel observed in experiments and simulations."
How effectively can spreadsheet anomalies be detected: An empirical study,"While spreadsheets are widely used, they have been found to be error-prone. Various techniques have been proposed to detect anomalies in spreadsheets, with varying scopes and effectiveness. Nevertheless, there is no empirical study comparing these techniques’ practical usefulness and effectiveness. In this work, we conducted a large-scale empirical study of three state-of-the-art techniques on their effectiveness in detecting spreadsheet anomalies. Our study focused on the precision, recall rate, efficiency and scope. We found that one technique outperforms the other two in precision and recall rate of spreadsheet anomaly detection. Efficiency of the three techniques is acceptable for most spreadsheets, but they may not be scalable to large spreadsheets with complex formulas. Besides, they have different scopes for detecting different spreadsheet anomalies, thus complementing to each other. We also discussed limitations of these three techniques. Based on our findings, we give suggestions for future spreadsheet research."
Human blood pressure measurement and medical care based on wireless sensors,"The mercury sphygmomanometer is increasingly acknowledged as the ""gold standard"" measurement of blood pressure at work, and the ban on mercury equipment appears to decrease its role in the world of the office and hospital. Blood Pressure (BP) measurements are the accurate diagnosis, and management for hypertension treatment is critical. This study provides an updated scientific statement from both the American Heart Association on assessing human blood pressure. Many machines have been evaluated in an educational environment to evaluate oscillometric blood pressure accuracy while reducing the method is most effective with auscultator human error. This results in a surge mercury device, and (perhaps never) to change the blood pressure measurement mode is preferably provided in clinics and hospitals. The essential methods to measure blood pressure and clinical measurements related to professional difficulties are explained in this section. Currently, available devices have raised an essential source of their hospitals and clinics and measurement errors. Practical advice on whether should use different equipment and measurement techniques are given. Under other circumstances, blood pressure measurement is being discussed in professional groups, Infants, adolescents, pregnant women, the elderly and the obese, among sure."
I see it in your eyes: Training the shallowest-possible CNN to recognise emotions and pain from muted web-assisted in-the-wild video-chats in real-time,"A robust value- and time-continuous emotion recognition has enormous potential benefits within healthcare. For example, within mental health, a real-time patient monitoring system capable of accurately inferring a patient’s emotional state could help doctors make an appropriate diagnosis and treatment plan. Such interventions could be vital in terms of ensuring a higher quality of life for the patient involved. To make such tools a reality, the associated machine learning systems need to be fast, robust and generalisable. In this regard, we present herein, a novel emotion recognition system consisting of the shallowest realisable Convolutional Neural Network (CNN) architecture. We draw insights from visualisations of the trained filter weights and the facial action unit (FAU) activations, i. e. the inputs to the model, of the participants featured in the in-the-wild, spontaneous video-chat sessions of the SEWA corpus. Further, we demonstrate the generalisablity of this approach on the German, Hungarian, and Chinese cultures available in this corpus. The obtained cross-cultural performance is a testimony to the universality of FAUs in expression and understanding of the human affective behaviours. These learnings were moderately consistent with the human perception of emotional expression. The practicality of the proposed approach is also demonstrated in another key healthcare applications; pain intensity prediction. Key results from these experiments highlight the transparency of the shallow CNN structure. As FAU can be extracted in near real-time, and because the models we developed are exceptionally shallow, this study paves the way for a robust, cross-cultural, end-to-end, in-the-wild, explainable real-time affect and pain prediction, that is value- and time-continuous."
I see it in your eyes: Training the shallowest-possible CNN to recognise emotions and pain from muted web-assisted in-the-wild video-chats in real-time,"A robust value- and time-continuous emotion recognition has enormous potential benefits within healthcare. For example, within mental health, a real-time patient monitoring system capable of accurately inferring a patient’s emotional state could help doctors make an appropriate diagnosis and treatment plan. Such interventions could be vital in terms of ensuring a higher quality of life for the patient involved. To make such tools a reality, the associated machine learning systems need to be fast, robust and generalisable. In this regard, we present herein, a novel emotion recognition system consisting of the shallowest realisable Convolutional Neural Network (CNN) architecture. We draw insights from visualisations of the trained filter weights and the facial action unit (FAU) activations, i. e. the inputs to the model, of the participants featured in the in-the-wild, spontaneous video-chat sessions of the SEWA corpus. Further, we demonstrate the generalisablity of this approach on the German, Hungarian, and Chinese cultures available in this corpus. The obtained cross-cultural performance is a testimony to the universality of FAUs in expression and understanding of the human affective behaviours. These learnings were moderately consistent with the human perception of emotional expression. The practicality of the proposed approach is also demonstrated in another key healthcare applications; pain intensity prediction. Key results from these experiments highlight the transparency of the shallow CNN structure. As FAU can be extracted in near real-time, and because the models we developed are exceptionally shallow, this study paves the way for a robust, cross-cultural, end-to-end, in-the-wild, explainable real-time affect and pain prediction, that is value- and time-continuous."
Identification and analysis of behavioral phenotypes in autism spectrum disorder via unsupervised machine learning,"Background and objective Autism spectrum disorder (ASD) is a heterogeneous disorder. Research has explored potential ASD subgroups with preliminary evidence supporting the existence of behaviorally and genetically distinct subgroups; however, research has yet to leverage machine learning to identify phenotypes on a scale large enough to robustly examine treatment response across such subgroups. The purpose of the present study was to apply Gaussian Mixture Models and Hierarchical Clustering to identify behavioral phenotypes of ASD and examine treatment response across the learned phenotypes. Materials and methods The present study included a sample of children with ASD (N = 2400), the largest of its kind to date. Unsupervised machine learning was applied to model ASD subgroups as well as their taxonomic relationships. Retrospective treatment data were available for a portion of the sample (n  = 1034). Treatment response was examined within each subgroup via regression. Results The application of a Gaussian Mixture Model revealed 16 subgroups. Further examination of the subgroups through Hierarchical Agglomerative Clustering suggested 2 overlying behavioral phenotypes with unique deficit profiles each composed of subgroups that differed in severity of those deficits. Furthermore, differentiated response to treatment was found across subtypes, with a substantially higher amount of variance accounted for due to the homogenization effect of the clustering. Discussion The high amount of variance explained by the regression models indicates that clustering provides a basis for homogenization, and thus an opportunity to tailor treatment based on cluster memberships. These findings have significant implications on prognosis and targeted treatment of ASD, and pave the way for personalized intervention based on unsupervised machine learning."
Identifying brain areas correlated with ADOS raw scores by studying altered dynamic functional connectivity patterns,"Altered functional connectivity patterns play an important role in explaining autism spectrum disorder related impairments. In order to examine such connectivity, resting state functional MRI is the most commonly used technique. To date, the majority of works in this area examine a whole time series of brain activation as a discrete stationary process. This study proposes a more detailed analysis of how functional connectivity fluctuates over time and how it is used to quantify instances demonstrating overconnectivity or underconnectivity. Non-parametric surrogates test identifies the areas where underconnectivity or overconnectivity correlate with the Autism Diagnosis Observation Schedule. In addition, this study shows how the areas identified affect the subjects behaviors. Our ultimate goal is a personalized autism diagnosis and treatment CAD system, where each subject impairments are distinctly mapped so they can be addressed with targeted treatments."
Identifying characteristic back shapes from anatomical scans of wheelchair users to improve seating design,"Spinal deformities are common in people who require the use of a wheelchair for mobility as a result of spinal cord injuries and other disabilities. Sitting positions vary between individuals with disabilities who use wheelchairs and individuals without disabilities. In individuals with spinal cord injury, spinal deformities can result in the development of back contours that deviate from the shape of standard rigid back support shells. The purpose of this study was to distinguish and classify various back contours of wheelchair users by utilizing digital anatomic scanning technology in order to inform the future development of back supports that would enhance postural support for those with spinal deformities. The three dimensional (3D) locations of bony landmarks were digitized when participants were in position, using a mechanical wand linked to the FastScantm system commonly used to measure surface contours. Raw FastScantm data were transformed according to bony landmarks. A total of 129 individuals participated in this study. A wide range of back contours were identified and categorized. Although participant characteristics (e.g., gender, diagnosis) were similar amongst the contour groups; no one characteristic explained the contours. Participants who were seated in a forward lean position had a higher amount of pelvic obliquity compared to those seated in an upright position; however, participants’ back contour was not correlated with pelvic obliquity. In conclusion, an array of different back shapes were classified in our cohort through 3D laser scanning technology. The methods and technology applied in this study could be replicated in future studies to categorize ranges of back shapes in larger populations of people with spinal cord injuries. Preliminary evidence indicates that customized postural support may be warranted to optimize positioning and posture when a standard rigid shell does not align with contours of a person's back. To optimize positioning, a range of contoured rigid backrests as well as height and angle adjustability are likely needed."
Image classification by using a reduced set of features in the TJ-II Thomson Scattering diagnostic,"Machine learning has been increasingly applied for developing pattern recognition systems in massive thermonuclear fusion databases. Several solutions can be found in the literature for fast retrieval of information, classification and forecasting of different types of waveforms. Images in fusion are not the exception, there are some data-driven models that have been successfully implemented to classify Thomson Scattering images in the TJ-II stellerator. Most of these image classifiers were developed by using techniques such as neural networks and support vector machines. One advantage of these techniques is that they only require a set of images and their corresponding classes to learn a decision function that provides the class to a new image. However, in general, this decision functions are commonly called black box models, because although they can achieve high success rates, it is difficult to explain why the classifier gives a particular response to a set of inputs. This work proposes the use of boosting algorithms to build data-driven models that use simple if-then rules and a small fraction of the original data to perform image classification of the TJ-II Thomson Scattering diagnostic."
Image recognition of CT diagnosis for cholangiocarcinoma treatment based on FPGA processor and neural network,"Imaging tests can help doctors identify abnormal internal organs that may indicate cholangiocarcinoma. Diagnostic techniques include Computed Tomography (CT) scan. For selected patients with early-stage bile duct cancer, liver transplantation is a viable treatment option. By optimizing radiotherapy and chemotherapy results before surgery, and tests confirmed by abdominal surgery without metastasis. Biliary tract cancer Cubic Centimeter (CC) is the second most common primary hepatobiliary cancer incidence has been rising. CC imaging characteristics, behavior, and treatment strategies depending on the form and location of the tumor and very different. In the cross-sectional images, the CC may (mass formation, infiltration, and catheter tube circumference) and according to the growth pattern positions (liver, hepatic portal around, extrahepatic/distal) classification. CC poor prognosis, surgery is the only treatment option. Biliary bile duct cancer diagnosis usually involves an emergency from diagnosis to treatment. Fatal cancer is attributable to the portion of the support challenges during treatment therapy. This article provides an overview of the intrahepatic and extrahepatic bile duct, including identifying risk factors, different treatments, symptom relief, and an in-depth understanding of common problems. Such as Neural Networks (NN) and the like have been used to interpret the image and Field-Programmable Gate Array (FPGA) diagnosis of Hepatocellular Carcinoma (HCC) and liver block. NN is a machine learning algorithm similar to the depth of learning. It demonstrated its ability to identify the specific function that can detect pathological lesions. Application of Neural Network to assess liver cancer and liver tumor imaging examination to diagnose cancer and evaluate neural networks' accuracy and performance."
Impact of coolant temperature on piston wall-wetting and smoke generation in a stratified-charge DISI engine operated on E30 fuel,"A late-injection strategy is typically adopted in stratified-charge direct injection spark ignition (DISI) engines to improve combustion stability for lean operation, but this may induce wall wetting on the piston surface and result in high soot emissions. E30 fuel, i.e., gasoline with 30% ethanol, is a potential alternative fuel that can offer a high Research Octane Number. However, the relatively high ethanol content increases the heat of vaporization, potentially exacerbating wall-wetting issues in DISI engines. In this study, the Refractive Index Matching (RIM) technique is used to measure fuel wall films in the piston bowl. The RIM implementation uses a novel LED illumination, integrated in the piston assembly and providing side illumination of the piston-bowl window. This RIM diagnostics in combination with high-speed imaging was used to investigate the impact of coolant temperature on the characteristics of wall wetting and combustion in an optical DISI engine fueled with E30. The experiments reveal that the smoke emissions increase drastically from 0.068 FSN to 1.14 FSN when the coolant temperature is reduced from 90 °C to 45 °C. Consistent with this finding, natural flame luminosity imaging reveals elevated soot incandescence with a reduction of the coolant temperature, indicative of pool fires. The RIM diagnostics show that a lower coolant temperature also leads to increased fuel film thickness, area, and volume, explaining the onset of pool fires and smoke."
Impact of the primary particle polydispersity on the radiative properties of soot aggregates,"Combustion generated soot appears as fractal aggregates formed by polydisperse nearly spherical primary particles. Knowledge of their radiative properties is a prerequisite for laser based diagnostics of soot. In this parametric study, the effect of primary particle polydispersity on soot aggregate absorption and scattering properties is investigated numerically. Two series of fractal aggregates formed by normal and lognormal distributed primary particles of different levels of standard deviation were numerically generated for typical flame soot with a fractal dimension and prefactor fixed to Df=1.73 and kf ≈ 1.5, respectively. Three aggregate sizes consisting of Np=15, 50 and 150 monomers per aggregate were investigated. Due to the uncertainty in soot refractive index, radiative properties were calculated by considering two different refractive indices at λ ≈ 532 nm recommended in the literature using the Discrete Dipoles Approximation and the Generalized Multiparticle Mie method. The results are interpreted in terms of correction factors to the Rayleigh–Debye–Gans theory for fractal aggregates (RDG-FA) for the forward scattering cross section A and for the absorption cross section h. It is shown that differential cross section for vertically polarized incident light, total scattering and absorption cross sections are well predicted by the RDG-FA theory for all considered aggregates formed by normally (σ/dp¯≤ 30%) and lognormally (σgeo ≤  1.6) distributed primary particles. The refractive index is found to be of greater impact than primary particle polydispersity on the importance of multiple scattering. The radiative force per unit laser power experienced by the soot aggregates was found primarily determined by the aggregate volume, regardless of the level of primary particle polydispersity."
Improved fault detection and diagnosis using sparse global-local preserving projections,"A new sparse dimensionality reduction method named sparse global-local preserving projections (SGLPP) is proposed. The SGLPP has two advantages. First, SGLPP can preserve both global and local structures of the data set. Second, SGLPP extracts sparse transformation vectors from the data set. The extracted sparse transformation vectors are able to reveal meaningful correlations between variables, which significantly improves the interpretability of SGLPP. These two advantages make SGLPP well suitable for fault detection and diagnosis in industrial processes. Therefore, a SGLPP-based process monitoring method is developed to improve the interpretability and the fault detection capability of monitoring models and to enhance the fault diagnosis capability. A full SGLPP model is combined with a set of partial SGLPP models to improve the fault sensitivity and to track the propagation of faults between process variables. In addition, three-level contribution plots, i.e., the variable-wise, group-wise, and group-variable-wise contribution plots, are constructed for fault evaluation and fault diagnosis. The effectiveness and advantages of proposed methods are illustrated with an industrial case study. The results indicate that the SGLPP models reveal real process mechanisms and control loops between process variables, and thus produces interpretable monitoring results. Moreover, the SGLPP-based method has better fault detection capability than conventional monitoring methods. Three-level contribution plots well show the effects of faults on process variables and produce reliable fault diagnosis results."
Improved Signal Characterization via Empirical Mode Decomposition to Enhance in-line Quality Monitoring,"The machine tool industry is facing the need to increase the sensorization of production systems to meet evolving market demands. This leads to the increasing interest for in-process monitoring tools that allow a fast detection of faults and unnatural process behaviours during the process itself. Nevertheless, the analysis of sensor signals implies several challenges. One major challenge consists of the complexity of signal patterns, which often exhibit a multiscale content, i.e., a superimposition of both stationary and non-stationary fluctuations on different time-frequency levels. Among time-frequency techniques, Empirical Mode Decomposition (EMD) is a powerful method to decompose any signal into its embedded oscillatory modes in a fully data-driven way, without any ex-ante basis selection. Because of this, it might be used effectively for automated monitoring and diagnosis of manufacturing processes. Unfortunately, it usually yields an over-decomposition, with single oscillation modes that can be split into more than one scale (this effect is also known as “mode mixing”). The literature lacks effective strategies to automatically synthetize the decomposition into a minimal number of physically relevant and interpretable components. This paper proposes a novel approach to achieve a synthetic decomposition of complex signals through the EMD procedure. A new criterion is proposed to group together multiple components associated to a common time-frequency pattern, aimed at summarizing the information content into a minimal number of modes, which may be easier to interpret. A real case study in waterjet cutting is presented, to demonstrate the benefits and the critical issues of the proposed approach."
Improvements in physics models of AFSI-ASCOT-based synthetic neutron diagnostics at JET,"New development steps of AFSI-ASCOT based synthetic neutron diagnostics and validation at JET are reported in this contribution. Synthetic neutron diagnostics are important not only in existing tokamaks, where they are used to interpret experimental data, but also in the design of future reactors including DEMO and beyond, where neutron detectors are one of the few diagnostics available. Thus, development and validation of realistic synthetic diagnostics is necessary for increasing confidence in existing models and future diagnostic designs. Recent development in AFSI includes physical corrections such as implementation of plasma rotation and reduction of the fast particle contribution in thermal reactant distribution. The rotation typically changes the beam-thermal reaction rates by 1–5%, while accounting for the fast particle density consistently reduces the neutron deficit (widely known inequality of the measured and calculated neutron rates) by up to 15% depending on the discharge. Further developments include implementation of angular dependence of DD differential fusion cross sections and accounting for finite Larmor radius effect, which is important for high-energy particles such as ICRH. Additionally, the role of data based analysis in synthetic diagnostics development with the help of JETPEAK database is discussed."
Improving prostate cancer classification in H&E tissue micro arrays using Ki67 and P63 histopathology,"Histopathology of Hematoxylin and Eosin (H&E)-stained tissue obtained from biopsy is commonly used in prostate cancer (PCa) diagnosis. Automatic PCa classification of digitized H&E slides has been developed before, but no attempts have been made to classify PCa using additional tissue stains registered to H&E. In this paper, we demonstrate that using H&E, Ki67 and p63-stained (3-stain) tissue improves PCa classification relative to H&E alone. We also show that we can infer PCa-relevant Ki67 and p63 information from the H&E slides alone, and use it to achieve H&E-based PCa classification that is comparable to the 3-stain classification. Reported improvements apply to classifying benign vs. malignant tissue, and low grade (Gleason group 2) vs. high grade (Gleason groups 3,4,5) cancer. Specifically, we conducted four classification tasks using 333 tissue samples extracted from 231 radical prostatectomy patients: regression tree-based classification using either (i) 3-stain features, with a benign vs malignant area under the curve (AUC = 92.9%), or (ii) real H&E features and H&E features learned from Ki67 and p63 stains (AUC = 92.4%), as well as deep learning classification using either (iii) real 3-stain tissue patches (AUC = 94.3%) and (iv) real H&E patches and generated Ki67 and p63 patches (AUC = 93.0%) using a deep convolutional generative adversarial network. Classification performance was assessed with Monte Carlo cross validation and quantified in terms of the Area Under the Curve, Brier score, sensitivity, and specificity. Our results are interpretable and indicate that the standard H&E classification could be improved by mimicking other stain types."
Incorporating message format into user evaluation of microblog information credibility: A nonlinear perspective,"The spreading of misinformation and disinformation is a great problem on microblogs, leading user evaluation of information credibility a critical issue. This study incorporates two message format factors related to multimedia usage on microblogs (vividness and multimedia diagnosticity) with two well-discussed factors for information credibility (i.e., argument quality and source credibility) as a holistic framework to investigate user evaluation of microblog information credibility. Further, the study draws on two-factor theory and its variant three-factor lens to explain the nonlinear effects of the above factors on microblog information credibility. An online survey was conducted to test the proposed framework by collecting data from microblog users. The research findings reveal that for the effects on microblog information credibility: (1) argument quality (a hygiene factor) exerts a decreasing incremental effect; (2) source credibility (a bivalent factor) exerts only a linear effect; and (3) multimedia diagnosticity (a motivating factor) exerts an increasing incremental effect. This study adds to current knowledge about information credibility by proposing an insightful framework to understand the key predictors of microblog information credibility and further examining the nonlinear effects of these predictors."
Individual variability analysis of fluorescence parameters measured in skin with different levels of nutritive blood flow,"Fluorescence spectroscopy has recently become more common in clinical medicine. However, there are still many unresolved issues related to the methodology and implementation of instruments with this technology. In this study, we aimed to assess individual variability of fluorescence parameters of endogenous markers (NADH, FAD, etc.) measured by fluorescent spectroscopy (FS) in situ and to analyse the factors that lead to a significant scatter of results. Most studied fluorophores have an acceptable scatter of values (mostly up to 30%) for diagnostic purposes. Here we provide evidence that the level of blood volume in tissue impacts FS data with a significant inverse correlation. The distribution function of the fluorescence intensity and the fluorescent contrast coefficient values are a function of the normal distribution for most of the studied fluorophores and the redox ratio. The effects of various physiological (different content of skin melanin) and technical (characteristics of optical filters) factors on the measurement results were additionally studied. The data on the variability of the measurement results in FS should be considered when interpreting the diagnostic parameters, as well as when developing new algorithms for data processing and FS devices."
Infeasibility resolution for multi-purpose batch process scheduling,"Scheduling decisions give rise to some of the most challenging optimization problems in the process industry. Formulating mathematical models for scheduling problems and devising tailored solution algorithms for these models has been the main thrust of previous literature. In this work, we focus on analyzing and resolving the cause of bottlenecks and infeasibilities in generic scheduling problems. We present a systematic approach for infeasibility diagnosis. Our approach exploits the known structure of scheduling models to isolate interpretable infeasible sets of constraints. We demonstrate the power of the algorithm on infeasible instances of the Westenberger–Kallrath multipurpose batch process modeled using a state-task network (STN) representation. The methodology presented in the paper is able to successfully analyze the cause of infeasibility and provide recommendations for resolving it. We also demonstrate how these insights and recommendations can be presented to scheduling operators with little or no optimization expertise in an intuitive manner."
Influence of Electrodes Polarization on the Response of Resistive Soot Sensor,"This study reports on the influence of the polarization applied between platinum electrodes of a resistive soot sensor. The conductance measured between these electrodes represents the sensor response which increases during soot deposition over time at fixed parameters (soot flow, polarization). It is observed that a maximum response is obtained for an optimum polarization. This behavior is explained by equilibrium between the creation and destruction of inter-electrodes soot bridges."
Influence of non-Gaussian noise on the effectiveness of cyclostationary analysis – Simulations and real data analysis,"Cyclostationary analysis is a useful approach in diagnostics of the machinery with rotating components. It allows indicating the cyclic modulations in the signal via analysis of a bi-frequency map called Cyclic Spectral Coherence (CSC). A non-zero CSC value at two frequencies means the presence of the cyclic process. Unfortunately, we have found that for some cyclostationary signals CSC provides difficult to interpret information. These disturbances in the CSC map have been linked to the presence of non-Gaussian noise. To prove it an original procedure has been proposed. Using simulations covering the model of signal, α-stable distribution, and Monte Carlo simulations it has been shown that indeed increasing presence of non-Gaussian noise makes worse the quality of diagnostic information extracted from CSC map. It has been recalled that the Cyclic Spectral Coherence is based on the autocovariance function of a given signal, thus it is properly defined for data coming from the distribution with the finite second moment. Finally, the authors selected three real examples that confirm the simulation-based findings. The main conclusion is before using the CSC analysis for cyclostationary signal one should validate the type of the noise. If noise is Gaussian — the CSC will bring optimal results. For the increasing level of impulsive non-cyclic noise, the CSC map becomes more and more disturbed and the detection of periodic excitation is difficult. Performed simulations on a very generic model some guidelines have been formulated regarding the acceptable level of non-Gaussian noise."
In-situ monitoring of transient gas phase K–Cl–S chemistry in a pilot-scale combustor,"Biomass and waste derived fuels contain large amounts of sodium, potassium, and chlorine that form NaCl and KCl, that is, compounds that cause operational problems, such as slagging, fouling, and high-temperature corrosion. Therefore, alkali chlorides are the main reasons that explain why steam parameters are less advanced and efficient in biomass and waste-based power generation when compared to coal. These problems can be mitigated by introducing sulphur into the system to form alkali sulphates that are not as problematic on steel surfaces as alkali chlorides. However, the alkali sulphation process in realistic combustion environments needs further exploration. Thus, new diagnostic methods for in-situ monitoring of alkali sulphation kinetics in combustion systems are required. In this work, the simultaneous monitoring of KCl and KOH concentrations in a pilot-scale combustor using Collinear Photofragmentation and Atomic Absorption Spectroscopy (CPFAAS) during stationary and transient operation of the combustor, is introduced. The CPFAAS information is complemented by monitoring SO2 and HCl concentrations using Fourier-transform infrared spectroscopy (FTIR). The temporal performance of the system is demonstrated by measuring the temporal combustor response curves for KCl sulphation for different Cl/K ratios during rapid changes in gaseous SO2 concentrations. The temporal concentration curves obtained imply that the Cl/K ratio has a significant impact on the temporal alkali sulphation behaviour. The measurement system described enables further exploration of K–Cl–S chemistry in realistic large-scale power plant environments."
In-situ NDT testing procedure as an integral part of failure analysis of historical masonry arch bridges,"A nineteenth-century masonry arch bridge was analyzed as an illustrative example to explain the role of in-situ test campaigns in failure analysis and retrofit design. Test results were studied to find out the advantages of each technique, with the aim of proposing an optimized in-situ testing procedure. Standard static penetrometer, flat jack, thermographic and georadar in-situ tests were conducted. Traffic effects were analyzed by means of vibrational tests. The experimental analysis performed to investigate damage on the bridge structures shows the degree of reliability offered by each technique in evaluating specific information and reproducing the global behavior of the structure."
Instrument detection and pose estimation with rigid part mixtures model in video-assisted surgeries,"Localizing instrument parts in video-assisted surgeries is an attractive and open computer vision problem. A working algorithm would immediately find applications in computer-aided interventions in the operating theater. Knowing the location of tool parts could help virtually augment visual faculty of surgeons, assess skills of novice surgeons, and increase autonomy of surgical robots. A surgical tool varies in appearance due to articulation, viewpoint changes, and noise. We introduce a new method for detection and pose estimation of multiple non-rigid and robotic tools in surgical videos. The method uses a rigidly structured, bipartite model of end-effector and shaft parts that consistently encode diverse, pose-specific appearance mixtures of the tool. This rigid part mixtures model then jointly explains the evolving tool structure by switching between mixture components. Rigidly capturing end-effector appearance allows explicit transfer of keypoint meta-data of the detected components for full 2D pose estimation. The detector can as well delineate precise skeleton of the end-effector by transferring additional keypoints. To this end, we propose effective procedure for learning such rigid mixtures from videos and for pooling the modeled shaft part that undergoes frequent truncation at the border of the imaged scene. Notably, extensive diagnostic experiments inform that feature regularization is a key to fine-tune the model in the presence of inherent appearance bias in videos. Experiments further illustrate that estimation of end-effector pose improves upon including the shaft part in the model. We then evaluate our approach on publicly available datasets of in-vivo sequences of non-rigid tools and demonstrate state-of-the-art results."
Integrating domain knowledge with deep learning models: An interpretable AI system for automatic work progress identification of NATM tunnels,"Finding a reliable and cost-effective approach to monitor the activities of the New Austrian Tunneling Method (NATM) tunnel construction automatically is a challenging yet important task. This study presents an interpretable artificial intelligence (AI) framework that automatically identifies NATM construction works using low-cost site surveillance images. The framework adopts the Bayesian statistics to combine the prior NATM construction knowledge with the visual evidence extracted by deep learning (DL) based computer vision models. The analysis results of Site CCTV surveillance videos of four NATM tunneling projects are presented to demonstrate its ability (i) to label NATM work cycles from the work timeline, (ii) to identify NATM work categories inside each work cycle, and (iii) to estimate the degree of plan-work deviation at the construction cycle level. The proposed framework yields promising results on a real NATM tunneling project."
Integrating domain knowledge with deep learning models: An interpretable AI system for automatic work progress identification of NATM tunnels,"Finding a reliable and cost-effective approach to monitor the activities of the New Austrian Tunneling Method (NATM) tunnel construction automatically is a challenging yet important task. This study presents an interpretable artificial intelligence (AI) framework that automatically identifies NATM construction works using low-cost site surveillance images. The framework adopts the Bayesian statistics to combine the prior NATM construction knowledge with the visual evidence extracted by deep learning (DL) based computer vision models. The analysis results of Site CCTV surveillance videos of four NATM tunneling projects are presented to demonstrate its ability (i) to label NATM work cycles from the work timeline, (ii) to identify NATM work categories inside each work cycle, and (iii) to estimate the degree of plan-work deviation at the construction cycle level. The proposed framework yields promising results on a real NATM tunneling project."
Integrating domain knowledge with deep learning models: An interpretable AI system for automatic work progress identification of NATM tunnels,"Finding a reliable and cost-effective approach to monitor the activities of the New Austrian Tunneling Method (NATM) tunnel construction automatically is a challenging yet important task. This study presents an interpretable artificial intelligence (AI) framework that automatically identifies NATM construction works using low-cost site surveillance images. The framework adopts the Bayesian statistics to combine the prior NATM construction knowledge with the visual evidence extracted by deep learning (DL) based computer vision models. The analysis results of Site CCTV surveillance videos of four NATM tunneling projects are presented to demonstrate its ability (i) to label NATM work cycles from the work timeline, (ii) to identify NATM work categories inside each work cycle, and (iii) to estimate the degree of plan-work deviation at the construction cycle level. The proposed framework yields promising results on a real NATM tunneling project."
Integrating machine learning techniques and physiology based heart rate features for antepartum fetal monitoring,"Background and objectives Intrauterine Growth Restriction (IUGR) is a fetal condition defined as the abnormal rate of fetal growth. The pathology is a documented cause of fetal and neonatal morbidity and mortality. In clinical practice, diagnosis is confirmed at birth and may only be suspected during pregnancy. Therefore, designing an accurate model for the early and prompt identification of pathology in the antepartum period is crucial in view of pregnancy management. Methods We tested the performance of 15 machine learning techniques in discriminating healthy versus IUGR fetuses. The various models were trained with a set of 12 physiology based heart rate features extracted from a single antepartum CardioTocographic (CTG) recording. The reason for the utilization of time, frequency, and nonlinear indices is based on their standalone documented ability to describe several physiological and pathological fetal conditions. Results We validated our approach on a database of 60 healthy and 60 IUGR fetuses. The machine learning methodology achieving the best performance was Random Forests. Specifically, we obtained a mean classification accuracy of 0.911 [0.860, 0.961 (0.95 confidence interval)] averaged over 10 test sets (10 Fold Cross Validation). Similar results were provided by Classification Trees, Logistic Regression, and Support Vector Machines. A features ranking procedure highlighted that nonlinear indices showed the highest capability to discriminate between the considered fetal conditions. Nevertheless, is the combination of features investigating CTG signal in different domains, that contributes to an increase in classification accuracy. Conclusions We provided validation of an accurate artificially intelligence framework for the diagnosis of IUGR condition in the antepartum period. The employed physiology based heart rate features constitute an interpretable link between the machine learning results and the quantitative estimators of fetal wellbeing."
Integration of convolutional neural networks for pulmonary nodule malignancy assessment in a lung cancer classification pipeline,"Background and Objective The early identification of malignant pulmonary nodules is critical for a better lung cancer prognosis and less invasive chemo or radio therapies. Nodule malignancy assessment done by radiologists is extremely useful for planning a preventive intervention but is, unfortunately, a complex, time-consuming and error-prone task. This explains the lack of large datasets containing radiologists malignancy characterization of nodules; Methods In this article, we propose to assess nodule malignancy through 3D convolutional neural networks and to integrate it in an automated end-to-end existing pipeline of lung cancer detection. For training and testing purposes we used independent subsets of the LIDC dataset; Results Adding the probabilities of nodules malignity in a baseline lung cancer pipeline improved its F1-weighted score by 14.7%, whereas integrating the malignancy model itself using transfer learning outperformed the baseline prediction by 11.8% of F1-weighted score; Conclusions Despite the limited size of the lung cancer datasets, integrating predictive models of nodule malignancy improves prediction of lung cancer."
Intelligent acoustic-based fault diagnosis of roller bearings using a deep graph convolutional network,"Roller bearings form key components in many machines and, as such, their health status can directly influence the operation of the entire machine. Acoustic signals collected from roller bearings contain information on their health status. Hence, acoustic-based fault diagnosis techniques can provide novel solutions as condition monitoring tools for roller bearings. Traditionally, acoustic fault diagnosis methods have been based on conventional signal processing methods in which prior expert knowledge has been required in order to extract and interpret the health information contained within the collected acoustic signals. As an alternative, deep learning methods can be used to obtain heath information from the collected signals by constructing ‘end-to-end’ models that do not rely on prior knowledge. These approaches have been successfully applied in the condition monitoring of industrial machinery. However, conventional deep learning methods can only learn features from the vertices of input data and thereby ignore the information contained in the relationships (edges) between vertices. In this paper, which combines graph convolution operators, graph coarsening methods, and graph pooling operations; a deep graph convolutional network (DGCN) based on graph theory is applied to deliver acoustic-based fault diagnosis of roller bearings. In the proposed method, the collected acoustic signals are first transformed into graphs with geometric structures. The edge weights represent the similarity between connected vertices, which enriches the input information and hence improves the classification accuracy of the deep learning methods applied. To verify the effectiveness of the proposed system, experiments with roller bearings of varying condition were carried out in the laboratory. The experimental results demonstrate that the DGCN method can be used to detect different kinds and severities of faults in roller bearings by learning from the constructed graphs. The results have been compared to those obtained using other, conventional, deep learning methods applied to the same datasets. These comparative tests demonstrate improved classification accuracy when using the DGCN method."
Intelligent therapeutic decision support for 30 days readmission of diabetic patients with different comorbidities,"The significance of medication therapy in managing comorbid diabetes is vital for maintaining the overall wellness of patients and reducing the cost of healthcare. Thus, using appropriate medication or medication combinations will be necessary for improved person-centred care and reduce complications associated with diagnosis and treatment. This study explains an intelligent decision support framework for managing 30 days unplanned readmission (30_URD) of comorbid diabetes using the Random Forest (RF) algorithm and Bayesian Network (BN) model. After the analysis of the medical records of 101,756 de-identified diabetic patients treated with 21 medications for 28 comorbidity combinations, the optimal medications for minimizing the likelihood of early readmissions were determined. This approach can help for identifying and managing most vulnerable patients thereby giving room to enhance post-discharge monitoring through clinical specialist supports to build critical-self management skills that will minimize the cost of diabetes care."
Interaction of powerful hot plasma and fast ion streams with materials in dense plasma focus devices,"A process of irradiating and ablating solid-state targets with hot plasma and fast ion streams in two Dense Plasma Focus (DPF) devices – PF-6 and PF-1000 was examined by applying a number of diagnostics of nanosecond time resolution. Materials perspective for use in chambers of the mainstream nuclear fusion facilities (mainly with inertial plasma confinement like NIF and Z-machine), intended both for the first wall and for constructions, have been irradiated in these simulators. Optical microscopy, SEM, Atomic Emission Spectroscopy, images in secondary electrons and in characteristic X-ray luminescence of different elements, and X-ray elemental analysis, gave results on damageability for a number of materials including low-activated ferritic and austenitic stainless steels, β-alloy of Ti, as well as two types of W and a composite on its base. With an increase of the number of shots irradiating the surface, its morphology changes from weakly pronounced wave-like structures or ridges to strongly developed ones. At later stages, due to the action of the secondary plasma produced near the target materials they melted, yielding both blisters and a fracturing pattern: first along the grain and then “in-between” the grains creating an intergranular net of microcracks. At the highest values of power flux densities multiple bubbles appeared. Furthermore, in this last case the cracks were developed because of microstresses at the solidification of melt. Presence of deuterium within the irradiated ferritic steel surface nanolayers is explained by capture of deuterons in lattice defects of the types of impurity atoms, pores and oxycarbonitride particles existed in the material."
Internet gaming disorder: Social phobia and identifying with your virtual self,"Online role-playing video games provide opportunities to connect socially and can enhance self-esteem. For some players, however, overuse fosters dependency leading to negative psychosocial and health consequences. Per the American Psychiatric Association, criteria for diagnosis of Internet gaming disorder (IGD) follow an addiction model, and include characteristics such as preoccupation, tolerance, and withdrawal. Though useful, this approach lacks a focus on underlying motivations that may partially explain vulnerability to IGD. This study explored relationships among IGD symptoms and two potential risk factors: social phobia and player-avatar identification. Participants (N = 394; 50% female) were recruited from game-related internet forums and surveyed online. We tested a model in which a positive relationship between social phobia and IGD symptoms was partially mediated by stronger avatar identification. Social phobia, avatar identification, and IGD symptoms were strongly positively related, and we found modest support for mediation as proposed. Accordingly, we suggest that fundamental needs for social connection and approval are potent motivators to play, particularly for socially phobic players uncomfortable with face-to-face contact. Vicarious interactions through a gaming avatar may fulfill these needs, reinforcing stronger self-identification with the avatar, which in turn can offer players a stronger and more positive sense of self. Such influences may work synergistically to motivate increasing intensity of and preoccupation with gameplay, contributing to IGD. These results support the use of player-avatar identification in assessing risk for IGD, developing treatment options, and reaching a better understanding of how socialization and identity can be influenced by virtual interactions and accomplishments."
Internet-based CBT for social phobia and panic disorder in a specialised anxiety clinic in routine care: Results of a pilot randomised controlled trial,"Ample studies have demonstrated that internet-based cognitive behavioural therapy (iCBT) for anxiety disorders is effective and acceptable in controlled settings. Studies assessing the clinical effectiveness of iCBT for anxiety disorders among routine care populations are, however, not as numerous. The purpose of this study was to assess the effectiveness of iCBT among anxiety patients, who were on a waiting list for intensive outpatient treatment, in a specialised routine care clinic.11Clinic for OCD and Anxiety disorders, Aarhus University Hospital, DK. A randomised controlled pilot trial was conducted. Recruited patients were on a waiting list and had a primary diagnosis of either social phobia or panic disorder. Participants were randomised into either receiving iCBT with minimal therapist contact (received access to the programme FearFighter® (FF) and received support from a clinician via telephone) or no treatment (stayed on the waiting list). The primary outcome was self-reported symptomatic change of anxiety on Beck Anxiety Inventory (BAI). The secondary outcomes were comorbid depression measured on Beck Depression Inventory (BDI-II) and quality of life measured with the EuroQol one-item visual-analogue scale (EQ-vas). All results were analysed by intention-to-treat analyses using a mixed-effects approach. N=158 patients were assessed for eligibility of which N=67 met all eligibility inclusion criteria, signed informed consent forms, and were randomised. Post-treatment assessment was completed by N=47 (70%). In the intervention group, N=11 (31%) completed all modules of FF. No significant differences of change of symptomatic levels were found between the intervention and control group for anxiety (BAI: mean diff.=2.42; 95% CI −1.03 to 5.86; p=0.17; d=0.06) or for depression (BDI-II: mean diff. 1.87; 95% CI −2.25 to 6.00; p=0.37; d=0.02). A large and significant effect was found in self-reported quality of life in favour of the experimental group (EQ-vas: mean diff. −20.88; 95% CI −30.64 to −11.11; p<0.001; d=0.81). This study was not able to document statistically significant clinical effect of iCBT with minimal therapist contact compared to a waiting list control group in a specialised anxiety clinic in routine care. However, a large and significant effect was seen on self-reported quality of life. Although these results offer an interesting perspective on iCBT in specialised care, they should be interpreted with caution, due to the limitations of the study. A large scale fully powered RCT is recommended."
Internet-delivered Mindfulness-Based Cognitive Therapy for anxiety and depression in cancer survivors: Predictors of treatment response,"Background The present study investigates possible predictors of treatment response in an Internet-delivered Mindfulness-Based Cognitive Therapy (iMBCT) intervention with therapist support. This iMBCT program, a fully online delivered intervention with asynchronous therapist support, has previously been shown to be efficacious in reducing symptoms of anxiety and depression in women treated for breast cancer and men treated for prostate cancer. Methods Eighty-two breast- and prostate cancer survivors experiencing psychological distress received 8 weeks of therapist-guided iMBCT. Primary outcomes were improvement in anxiety and depression scores from baseline to post-treatment and from baseline to six-months follow-up. Clinical predictors included levels of depression and anxiety at the time of screening and at baseline, as well as time since diagnosis. Demographic predictors included age and educational level. Therapy-related predictors included working alliance, self-compassion, and five facets of mindfulness. Mixed Linear Models were employed to test the prediction effects over time. Results Higher levels of baseline depression were associated with increased treatment response in anxiety at post-treatment, and lower levels of self-compassion were associated with increased treatment response in depression at post-treatment. None of the proposed predictors significantly predicted treatment response at six-months follow-up. Conclusion The findings suggest that iMBCT can be provided for cancer survivors regardless of their age, educational level, and time since diagnosis (up to five years) and that therapeutic alliance is not crucial for treatment response. We did not identify characteristics predicting treatment response, although many factors were tested. Still, other characteristics may be predictors, and given the relatively small sample size and a large number of statistical tests, the results should be interpreted with caution."
Interpretable Anomaly Prediction: Predicting anomalous behavior in industry 4.0 settings via regularized logistic regression tools,"Prediction of anomalous behavior in industrial assets based on sensor reading represents a key focus in modern business practice. As a matter of fact, forecast of forthcoming faults is crucial to implement predictive maintenance, i.e. maintenance decision making based on real time information from components and systems, which allows, among other benefits, to reduce maintenance cost, minimize downtime, increase safety, enhance product quality and productivity. However, building a model able to predict the future occurrence of a failure is challenging for various reasons. First, data are usually highly imbalanced, meaning that patterns describing a faulty regime are much less numerous than normal behavior instances, which makes model design difficult. Second, model predictions should be not only accurate (to avoid false alarms and missed detections) but also explainable to operators responsible for scheduling maintenance or control actions. In this paper we introduce a method called Interpretable Anomaly Prediction (IAP) allowing to handle these issues by using regularized logistic regression as core prediction model. In particular, in contrast to anomaly detection algorithms which permit to identify if the current data are anomalous or not, the proposed technique is able to predict the probability that future data will be abnormal. Furthermore, feature extraction and selection mechanisms give insights on the possible root causes leading to failures. The proposed strategy is validated with a large imbalanced multivariate time-series dataset consisting of measurements of several process variables surrounding an high pressure plunger pump situated in a complex chemical plant."
Interpretable deep learning systems for multi-class segmentation and classification of non-melanoma skin cancer,"We apply for the first-time interpretable deep learning methods simultaneously to the most common skin cancers (basal cell carcinoma, squamous cell carcinoma and intraepidermal carcinoma) in a histological setting. As these three cancer types constitute more than 90% of diagnoses, we demonstrate that the majority of dermatopathology work is amenable to automatic machine analysis. A major feature of this work is characterising the tissue by classifying it into 12 meaningful dermatological classes, including hair follicles, sweat glands as well as identifying the well-defined stratified layers of the skin. These provide highly interpretable outputs as the network is trained to represent the problem domain in the same way a pathologist would. While this enables a high accuracy of whole image classification (93.6-97.9%), by characterising the full context of the tissue we can also work towards performing routine pathologist tasks, for instance, orientating sections and automatically assessing and measuring surgical margins. This work seeks to inform ways in which future computer aided diagnosis systems could be applied usefully in a clinical setting with human interpretable outcomes."
Interpretable deep learning systems for multi-class segmentation and classification of non-melanoma skin cancer,"We apply for the first-time interpretable deep learning methods simultaneously to the most common skin cancers (basal cell carcinoma, squamous cell carcinoma and intraepidermal carcinoma) in a histological setting. As these three cancer types constitute more than 90% of diagnoses, we demonstrate that the majority of dermatopathology work is amenable to automatic machine analysis. A major feature of this work is characterising the tissue by classifying it into 12 meaningful dermatological classes, including hair follicles, sweat glands as well as identifying the well-defined stratified layers of the skin. These provide highly interpretable outputs as the network is trained to represent the problem domain in the same way a pathologist would. While this enables a high accuracy of whole image classification (93.6-97.9%), by characterising the full context of the tissue we can also work towards performing routine pathologist tasks, for instance, orientating sections and automatically assessing and measuring surgical margins. This work seeks to inform ways in which future computer aided diagnosis systems could be applied usefully in a clinical setting with human interpretable outcomes."
Interpretable deep learning to map diagnostic texts to ICD-10 codes,"Background Automatic extraction of morbid disease or conditions contained in Death Certificates is a critical process, useful for billing, epidemiological studies and comparison across countries. The fact that these clinical documents are written in regular natural language makes the automatic coding process difficult because, often, spontaneous terms diverge strongly from standard reference terminology such as the International Classification of Diseases (ICD). Objective Our aim is to propose a general and multilingual approach to render Diagnostic Terms into the standard framework provided by the ICD. We have evaluated our proposal on a set of clinical texts written in French, Hungarian and Italian. Methods ICD-10 encoding is a multi-class classification problem with an extensive (thousands) number of classes. After considering several approaches, we tackle our objective as a sequence-to-sequence task. According to current trends, we opted to use neural networks. We tested different types of neural architectures on three datasets in which Diagnostic Terms (DTs) have their ICD-10 codes associated. Results and conclusions Our results give a new state-of-the art on multilingual ICD-10 coding, outperforming several alternative approaches, and showing the feasibility of automatic ICD-10 prediction obtaining an F-measure of 0.838, 0.963 and 0.952 for French, Hungarian and Italian, respectively. Additionally, the results are interpretable, providing experts with supporting evidence when confronted with coding decisions, as the model is able to show the alignments between the original text and each output code."
Interpretable deep learning to map diagnostic texts to ICD-10 codes,"Background Automatic extraction of morbid disease or conditions contained in Death Certificates is a critical process, useful for billing, epidemiological studies and comparison across countries. The fact that these clinical documents are written in regular natural language makes the automatic coding process difficult because, often, spontaneous terms diverge strongly from standard reference terminology such as the International Classification of Diseases (ICD). Objective Our aim is to propose a general and multilingual approach to render Diagnostic Terms into the standard framework provided by the ICD. We have evaluated our proposal on a set of clinical texts written in French, Hungarian and Italian. Methods ICD-10 encoding is a multi-class classification problem with an extensive (thousands) number of classes. After considering several approaches, we tackle our objective as a sequence-to-sequence task. According to current trends, we opted to use neural networks. We tested different types of neural architectures on three datasets in which Diagnostic Terms (DTs) have their ICD-10 codes associated. Results and conclusions Our results give a new state-of-the art on multilingual ICD-10 coding, outperforming several alternative approaches, and showing the feasibility of automatic ICD-10 prediction obtaining an F-measure of 0.838, 0.963 and 0.952 for French, Hungarian and Italian, respectively. Additionally, the results are interpretable, providing experts with supporting evidence when confronted with coding decisions, as the model is able to show the alignments between the original text and each output code."
Interpretable mammographic mass classification with fuzzy interpolative reasoning,"Breast mass cancer remains a great challenge for developing advanced computer-aided diagnosis (CADx) systems, to assist medical professionals for the determination of benignancy or malignancy of masses. This paper presents a novel approach to building fuzzy rule-based CADx systems for mass classification of mammographic images, via the use of weighted fuzzy rule interpolation. It describes an integrated implementation of such a classification system that ensures interpretable classification of masses through firing the rules that match given observations, while having the capability of classifying unmatched observations through fuzzy rule interpolation (FRI). In particular, a feature weight-guided FRI scheme is exploited to enable such inference. The work is implemented through integrating feature weights with a popular scale and move transformation-based FRI, with the individual feature weights derived from feature selection as a preprocessing process. The efficacy of the proposed CADx system is systematically evaluated using two real-world mammographic image datasets, demonstrating its explicit interpretability and potential classification performance."
Interpretation of surface degradation on polymeric insulators,"Silicone rubber based polymeric insulators are being used widely used as outdoor insulation for high voltage applications. However, their long-term service life performance is highly dependent on local environmental conditions that cause deterioration of material properties of the insulators. In the present work, tracking and erosion studies are conducted on silicone rubber samples using. Inclined Plane Tracking (IPT) and Erosion method based on IEC 60587. The contaminants used to simulate different environmental conditions are as per standard and in addition acidic rain composition is used. Leakage current flowing through samples was continuously monitored and recorded over the experimental duration. The consequence of leakage current variation is analyzed using. Recurrent Plot (RP) Analysis and this resulted in variation of quantitative parameters Recurrence Rate (RR), Determinism (DET), Entropy (ENT) and Length (L) these are used to interpret the tracking and erosion performance of silicone rubber samples. Further, physico-chemical analysis is conducted using Scanning Electron Microscopy (SEM), Energy Dispersive X-Ray (EDAX) and Fourier Transform Infra-Red (FTIR) spectroscopy to observe surface morphology and chemical changes happened in the samples. Thermo-Gravimetric Analysis (TGA) is performed to observe thermal stability and presence of Aluminum Tri-hydrate (ATH) fillers before and after experiments. The investigations show RP method as a potential tool for detection and diagnostic of polymeric insulators."
Interpretative identification of the faulty conditions in a cyclic manufacturing process,"The intensive development of information and communication technologies in recent years has led to an increase in data size and complexity. Conventional approaches, with associated methods of analysis based on descriptive and inductive statistics, may no longer be suitable for extracting the valuable information that is hidden in the available data. Computer-controlled manufacturing systems are becoming rich sources of data. Plastic injection moulding and die casting systems are typical examples of such manufacturing systems where the parts are produced by repeating the same sequence of steps that make up a manufacturing cycle. For each cycle, similarly structured data is generated. In this work a method for systematic data analysis for cyclic manufacturing processes is presented. The proposed data-analysis method integrates well-known heuristic algorithms, i.e., decision trees and clustering, with the purpose of identifying types of faulty operating conditions. The result of the analysis is an interpretable model for decision support that can be used for fault identification, to search for root causes, and to develop prognostic systems. A holistic approach of applying the proposed data-analysis method, along with suggestions and guidelines for implementation, is presented. A case study is presented in which the proposed method is applied to real industrial data from a plastic injection-moulding process."
Interpreting chest X-rays via CNNs that exploit hierarchical disease dependencies and uncertainty labels,"Chest radiography is one of the most common types of diagnostic radiology exams, which is critical for screening and diagnosis of many different thoracic diseases. Specialized algorithms have been developed to detect several specific pathologies such as lung nodules or lung cancer. However, accurately detecting the presence of multiple diseases from chest X-rays (CXRs) is still a challenging task. This paper presents a supervised multi-label classification framework based on deep convolutional neural networks (CNNs) for predicting the presence of 14 common thoracic diseases and observations. We tackle this problem by training state-of-the-art CNNs that exploit hierarchical dependencies among abnormality labels. We also propose to use the label smoothing technique for a better handling of uncertain samples, which occupy a significant portion of almost every CXR dataset. Our model is trained on over 200,000 CXRs of the recently released CheXpert dataset and achieves a mean area under the curve (AUC) of 0.940 in predicting 5 selected pathologies from the validation set. This is the highest AUC score yet reported to date. The proposed method is also evaluated on the independent test set of the CheXpert competition, which is composed of 500 CXR studies annotated by a panel of 5 experienced radiologists. The performance is on average better than 2.6 out of 3 other individual radiologists with a mean AUC of 0.930, which ranks first on the CheXpert leaderboard at the time of writing this paper."
Interpreting network knowledge with attention mechanism for bearing fault diagnosis,"Condition monitoring and fault diagnosis of bearings play important roles in production safety and limiting the cost of maintenance on a reasonable level. Nowadays, artificial intelligence and machine learning make fault diagnosis gradually become intelligent, and data-driven intelligent algorithms are receiving more and more attention. However, many methods use the existing deep learning models directly for the analysis of mechanical vibration signals, which is still lack of interpretability to researchers. In this paper, a method based on multilayer bidirectional gated recurrent units with attention mechanism is proposed to access the interpretability of neural networks in fault diagnosis, which combines the convolution neural network, gated recurrent unit, and the attention mechanism. Based on the attention mechanism, the attention distribution of input segments is visualized and thus the interpretability of neural networks can be further presented. Experimental validations and comparisons are conducted on bearings. The results present that the proposed model is effective for localizing the discriminative information from the input data, which provides a tool for better understanding the feature extraction process in neural networks, especially for mechanical vibration signals."
Interpreting support vector machine models for multivariate group wise analysis in neuroimaging,"Machine learning based classification algorithms like support vector machines (SVMs) have shown great promise for turning a high dimensional neuroimaging data into clinically useful decision criteria. However, tracing imaging based patterns that contribute significantly to classifier decisions remains an open problem. This is an issue of critical importance in imaging studies seeking to determine which anatomical or physiological imaging features contribute to the classifier’s decision, thereby allowing users to critically evaluate the findings of such machine learning methods and to understand disease mechanisms. The majority of published work addresses the question of statistical inference for support vector classification using permutation tests based on SVM weight vectors. Such permutation testing ignores the SVM margin, which is critical in SVM theory. In this work we emphasize the use of a statistic that explicitly accounts for the SVM margin and show that the null distributions associated with this statistic are asymptotically normal. Further, our experiments show that this statistic is a lot less conservative as compared to weight based permutation tests and yet specific enough to tease out multivariate patterns in the data. Thus, we can better understand the multivariate patterns that the SVM uses for neuroimaging based classification."
Intrusion alert prioritisation and attack detection using post-correlation analysis,"Event Correlation used to be a widely used technique for interpreting alert logs and discovering network attacks. However, due to the scale and complexity of today's networks and attacks, alert logs produced by these modern networks are much larger in volume and difficult to analyse. In this research we show that adding post-correlation methods can be used alongside correlation to significantly improve the analysis of alert logs. We proposed a new framework titled A Comprehensive System for Analysing Intrusion Alerts (ACSAnIA). The post-correlation methods include a new prioritisation metric based on anomaly detection and a novel approach to clustering events using correlation knowledge. One of the key benefits of the framework is that it significantly reduces false-positive alerts and it adds contextual information to true-positive alerts. We evaluated the post-correlation methods of ACSAnIA using data from a 2012 cyber range experiment carried out by industrial partners of the British Telecom Security Practice Team. In one scenario, our results show that false-positives were successfully reduced by 97% and in another scenario, 16%. It also showed that clustering correlated alerts aided in attack detection. The proposed framework is also being developed and integrated into a pre-existing Visual Analytic tool developed by the British Telecom SATURN Research Team for the analysis of cyber security data."
Investigating the applicability of the finite integration technique for studying the frequency response of the transformer winding,"The frequency response analysis (FRA) is a promising diagnostic method for detecting mechanical faults inside a power transformer. Despite the standardization of the FRA measurement procedure, the interpretation of the results is still a subject of study. The reason lies in the fact that the frequency response is different from case to case. Therefore, for interpreting a transformer FRA result, the effects of various possible mechanical changes on the FRA trace of that particular transformer should be known. Additionally, real mechanical deformations cannot be executed for obtaining the required information due to the destructive nature. Hence, the transformer winding models are utilized instead to study the frequency response. While the circuit model has been extensively discussed in the literature, there is little focus on the numerical methods for the FRA purpose. To take a step forward, this contribution investigates the applicability of the finite integration technique for the FRA studies. The presented model can simulate different fault types for the FRA interpretation while it has also other applications regarding the frequency response. In this contribution, the FRA traces are derived directly from a finite integration model which is an important improvement compared with the previous papers. The agreement between the simulation results and the experimental data indicates that the proposed model has the potential of providing a powerful tool to decipher the transformer frequency behavior."
Investigation of mixing processes of effusion cooling air and main flow in a single sector model gas turbine combustor at elevated pressure,"Mixing processes between main flow and effusion cooling air are investigated in an effusion cooled, swirl-stabilized pressurized single sector gas turbine combustor using advanced laser diagnostics. Quantitative planar laser-induced fluorescence of the hydroxyl radical (OH-PLIF) and planar laser-induced fluorescence of nitric oxide, seeded to the effusion cooling air, (NO-PLIF) are employed in the primary zone and close to the effusion cooled liner. This data is used to identify mixing events at three stages of premixed combustion, i.e. mixing before reaction, mixing during reaction and mixing after reaction. A parametric study of swirl and cooling air mass flow is conducted to investigate the mutual interaction between flame and cooling air. Within the primary zone, a significant radial asymmetry of OH concentration is observed. This asymmetry is partly explained by the presence of effusion cooling air within the unburned fresh gas, leading to lowered OH concentration within local reaction zones and their post-flame equilibrium concentration. Near the effusion cooled liner, adiabatic mixing after reaction is the dominant process across all investigated operating conditions. Notable mixing before reaction is only observed for the first effusion hole on the center line at low swirl conditions."
Investigation of photochemical effects in flame diagnostics with picosecond photofragmentation laser-induced fluorescence,"Photofragmentation laser-induced fluorescence (PFLIF) is for the first time performed based on picosecond laser pulses for detection of hydroperoxyl radicals (HO2) in a stoichiometric laminar methane/air flame. Photofragmentation is performed with a pump laser pulse of 80ps duration and a wavelength of 266nm, whereupon the produced OH photofragments are detected by a second picosecond probe laser pulse, inducing fluorescence via excitation in the A2Σ+(v=1) ← X2Π(v=0) band of OH near 283nm. Excitation spectra of the OH photofragments formed in the reaction zone were recorded for pump-probe delays ranging from 0 to 5ns. The spectra suggest that the population distribution of the nascent OH fragments is rotationally cold and that it takes on the order of 5ns for the nascent non-equilibrium rotational distribution to relax into a thermal distribution. The radial OH-fragment distribution was extracted from spectral images (radial position versus emission wavelength) recorded at six different pump-probe delays. Photochemical OH production was observed both in the reaction zone and the product zone. Comparison with a kinetic model for OH production suggests that more than 20% of the oxygen fragments produced by photolysis in the reaction zone are formed in the excited 1D state, explaining a very rapid initial signal growth. The OH-production model was also compared with previous reaction-zone data, acquired with nanosecond laser pulses in the same flame, indicating that no O(1D), but only O(3P), is formed. A plausible explanation of the discrepancy between the two results is that the picosecond pulses, having more than two-orders of magnitude higher irradiance than the nanosecond pulses used in the previous study, might cause 2-photon photodissociation, allowing production of O(1D). In terms of flame diagnostics with PFLIF, it is concluded that a setup based on nanosecond laser pulses, rather than picosecond pulses, appears preferable since photochemical OH production in the reaction zone can be avoided while for short delay times the ratio between the photofragment signal and the photochemical interference in the product zone, stemming from CO2 photolysis, is sufficiently large to clearly visualize the photofragments."
Investigation of temperature behavior for multi-fractured horizontal well in low-permeability gas reservoir,"This study aims to interpret the temperature behavior of a cemented multi-fractured horizontal well (MFHW) in a low-permeability gas reservoir (LPGR) during production. First, considering heat conduction, heat convection, thermal expansion, viscous dissipation, and the Joule–Thomson effect, a comprehensive numerical temperature prediction model is developed under a single-phase condition. The developed models are formulated for the reservoir and wellbore domains based on mass, momentum, and energy conservation. The non-Darcy law is applied to the numerical models, and radial flow in the hydraulic fractures is accounted for when the reservoir and wellbore models are coupled. These developed models are solved numerically by the finite difference method. Then, synthetic cases demonstrate the models’ ability to predict the temperature behavior and clarify the change regularity of the wellbore temperature profile for an MFHW in an LPGR. The effects of pressure interference among hydraulic fractures on the inflow rate are analyzed. Based on the sensitivity of arriving temperature to the fracture parameters, an approach to plotting fracture parameter diagnosis charts are introduced. In addition, a field case is provided to illustrate the application and feasibility of the new models on the basis of the accurate simulated results of wellbore temperature profiles."
Kinematics of Big Biomedical Data to characterize temporal variability and seasonality of data repositories: Functional Data Analysis of data temporal evolution over non-parametric statistical manifolds,"Aim The increasing availability of Big Biomedical Data is leading to large research data samples collected over long periods of time. We propose the analysis of the kinematics of data probability distributions over time towards the characterization of data temporal variability. Methods First, we propose a kinematic model based on the estimation of a continuous data temporal trajectory, using Functional Data Analysis over the embedding of a non-parametric statistical manifold which points represent data temporal batches, the Information Geometric Temporal (IGT) plot. This model allows measuring the velocity and acceleration of data changes. Next, we propose a coordinate-free method to characterize the oriented seasonality of data based on the parallelism of lagged velocity vectors of the data trajectory throughout the IGT space, the Auto-Parallelism of Velocity Vectors (APVV) and APVVmap. Finally, we automatically explain the maximum variance components of the IGT space coordinates by means of correlating data points with known temporal factors from the domain application. Materials Methods are evaluated on the US National Hospital Discharge Survey open dataset, consisting of 3,25M hospital discharges between 2000 and 2010. Results Seasonal and abrupt behaviours were present on the estimated multivariate and univariate data trajectories. The kinematic analysis revealed seasonal effects and punctual increments in data celerity, the latter mainly related to abrupt changes in coding. The APVV and APVVmap revealed oriented seasonal changes on data trajectories. For most variables, their distributions tended to change to the same direction at a 12-month period, with a peak of change of directionality at mid and end of the year. Diagnosis and Procedure codes also included a 9-month periodic component. Kinematics and APVV methods were able to detect seasonal effects on extreme temporal subgrouped data, such as in Procedure code, where Fourier and autocorrelation methods were not able to. The automated explanation of IGT space coordinates was consistent with the results provided by the kinematic and seasonal analysis. Coordinates received different meanings according to the trajectory trend, seasonality and abrupt changes. Discussion Treating data as a particle moving over time through a multidimensional probabilistic space and studying the kinematics of its trajectory has turned out to a new temporal variability methodology. Its results on the NHDS were aligned with the dataset and population descriptions found in the literature, contributing with a novel temporal variability characterization. We have demonstrated that the APVV and APVVmat are an appropriate tool for the coordinate-free and oriented analysis of trajectories or complex multivariate signals. Conclusion The proposed methods comprise an exploratory methodology for the characterization of data temporal variability, what may be useful for a reliable reuse of Big Biomedical Data repositories acquired over long periods of time."
K-nearest neighbors based methods for identification of different gear crack levels under different motor speeds and loads: Revisited,"Gears are the most commonly used components in mechanical transmission systems. Their failures may cause transmission system breakdown and result in economic loss. Identification of different gear crack levels is important to prevent any unexpected gear failure because gear cracks lead to gear tooth breakage. Signal processing based methods mainly require expertize to explain gear fault signatures which is usually not easy to be achieved by ordinary users. In order to automatically identify different gear crack levels, intelligent gear crack identification methods should be developed. The previous case studies experimentally proved that K-nearest neighbors based methods exhibit high prediction accuracies for identification of 3 different gear crack levels under different motor speeds and loads. In this short communication, to further enhance prediction accuracies of existing K-nearest neighbors based methods and extend identification of 3 different gear crack levels to identification of 5 different gear crack levels, redundant statistical features are constructed by using Daubechies 44 (db44) binary wavelet packet transform at different wavelet decomposition levels, prior to the use of a K-nearest neighbors method. The dimensionality of redundant statistical features is 620, which provides richer gear fault signatures. Since many of these statistical features are redundant and highly correlated with each other, dimensionality reduction of redundant statistical features is conducted to obtain new significant statistical features. At last, the K-nearest neighbors method is used to identify 5 different gear crack levels under different motor speeds and loads. A case study including 3 experiments is investigated to demonstrate that the developed method provides higher prediction accuracies than the existing K-nearest neighbors based methods for recognizing different gear crack levels under different motor speeds and loads. Based on the new significant statistical features, some other popular statistical models including linear discriminant analysis, quadratic discriminant analysis, classification and regression tree and naive Bayes classifier, are compared with the developed method. The results show that the developed method has the highest prediction accuracies among these statistical models. Additionally, selection of the number of new significant features and parameter selection of K-nearest neighbors are thoroughly investigated."
Knowledge extraction and insertion to deep belief network for gearbox fault diagnosis,"Deep neural network (DNN) with a complex structure and multiple nonlinear processing units has achieved great success for feature learning in machinery fault diagnosis. Due to the “black box” problem in DNNs, there are still many obstacles to the application of DNNs in fault diagnosis. This paper proposes a new DNN model, knowledge-based deep belief network (KBDBN), which inserts confidence and classification rules into the deep network structure. This not only enables the model to have good pattern recognition performance but also to adaptively determine the network structure and obtain a good understanding of the features learned by the deep network. The knowledge extraction algorithm is proposed to offer a good representation of layerwise networks (i.e., restricted Boltzmann machines (RBMs)). The layerwise extraction can produce an improvement in feature learning of RBMs. Moreover, the extracted confidence rules that characterize the deep network offers a novel method for insertion of prior knowledge in the deep RBM. The classification knowledge extracted from the data is further inserted into the classification layer of DBN. KBDBN is used to generate the discriminant features from the data and then construct a complex mapping between vibration signals and gearbox defects. The testing results of KBDBN on a gearbox test rig not only effectively extracts knowledge from the deep network, but also shows better classification performance than the typical classifiers and DBNs. Moreover, the interpretable network model helps us understand what DBN has learned from vibration signals and then makes it be applied easily in real-world cases."
Leak localization in water distribution networks using classifiers with cosenoidal features,"This paper presents a leak localization approach for water distribution networks using classifiers with pressure residuals as input features. This approach is based on applying a non-linear transformation to the residuals of the node pressures to increase the separability of the leak classes. The transformed features can be interpreted as the direction cosines in the subspace spanned by the residuals of the measured pressures. In order to illustrate the method, different tests were performed with MATLAB® applying four different classification algorithms on a synthetic dataset obtained from an EPANET model of the Hanoi network. Then, by considering the cosenoidal features, a significant improvement in the leak location error was achieved. In this way, the leak location error decreases by more than 97% compared to the use of residual features when accurate measurements are used, and about 50% when noisy measurements with 60 dB SNR are used."
"Learning efficient, explainable and discriminative representations for pulmonary nodules classification","Automatic pulmonary nodules classification is significant for early diagnosis of lung cancers. Recently, deep learning techniques have enabled remarkable progress in this field. However, these deep models are typically of high computational complexity and work in a black-box manner. To combat these challenges, in this work, we aim to build an efficient and (partially) explainable classification model. Specially, we use neural architecture search (NAS) to automatically search 3D network architectures with excellent accuracy/speed trade-off. Besides, we use the convolutional block attention module (CBAM) in the networks, which helps us understand the reasoning process. During training, we use A-Softmax loss to learn angularly discriminative representations. In the inference stage, we employ an ensemble of diverse neural networks to improve the prediction accuracy and robustness. We conduct extensive experiments on the LIDC-IDRI database. Compared with previous state-of-the-art, our model shows highly comparable performance by using less than 1/40 parameters. Besides, empirical study shows that the reasoning process of learned networks is in conformity with physicians’ diagnosis. Related code and results have been released at: https://github.com/fei-hdu/NAS-Lung."
Learning Organization as a Tool for Better and More Effective Schools,"The socio-economic context of learning and the needs for increasing the quality of the teaching profession in Hungary. The research was aimed at increasing employability via improving the quality of higher and public education. This paper analyses and highlights the relations between a knowledge-rich economy and the quality of higher education with special regard to the preparation of teachers and schools that all serve as a motivating environment for learning. Schools as learning organizations and a model used for diagnosing them The second part of the paper introduces the scientific basis for diagnosing schools. It analyses how the concept of a learning organization developed by Senge can be interpreted and adapted to schools. The applied model integrates three key concepts: the culture and the efficiency of a school organization were investigated with the application of the Competing Values Framework model; the behavioral competences of opinion leaders in schools were tested with a professional HR model (INRO/RDA); schools made SWOT analyses of themselves focusing on their learning capacity. These three concepts/models were integrated in one complex and holistic model, which provided a firm basis for the analyses. Profiles and characteristics of schools as learning organizations The core part of the paper provides a brief overview of the characteristics of the 82 schools based on the complex diagnosis model. This part of the paper describes how the characteristics of schools as learning organizations were formed, and what differences arose among the schools. Based on the fitness of the schools to the LO profile, the paper analyses what kind of differences appear in the CVF model. Conclusions The closing part of the paper summarizes the main conclusions of the diagnosis, and gives suggestions that were articulated for the schools on the basis of the research."
Length of stay prediction for ICU patients using individualized single classification algorithm,"Background and Objective: In intensive care units (ICUs), length of stay (LOS) prediction is critical to help doctors and nurses select appropriate treatment options and predict patients’ condition. Considering that most hospitals use universal models to predict patients’ condition, which cannot meet the individual needs of special ICU patients. Our goal is to create a personalized model for patients to determine the number of hospital stays. Methods: In this study, a new combination of just-in-time learning (JITL) and one-class extreme learning machine (one-class ELM) is proposed to predict the number of days a patient stays in hospital. This combination is shortened as one-class JITL-ELM, where JITL is used to search for personalized cases for a new patient and one-class ELM is used to determine whether the patient can be discharged within 10 days. Results: The experimental results show that the one-class JITL-ELM model has an area under the curve (AUC) index of 0.8510, lift value of 2.1390, precision of 1, and G-mean is 0.7842. Its accuracy, specificity, and sensitivity were found as 0.82, 1, and 0.6150, respectively. Moreover, a novel simple mortality risk level estimation system that can determine the mortality rate of a patient by combining LOS and age is proposed. It has an accuracy rate of 66% and the miss rate of only 6.25%. Conclusions: Overall, the one-class JITL-ELM can accurately predict hospitalization days and mortality using early physiological parameters. Moreover, a simple mortality risk level estimation system based on a combination of LOS and age is proposed; the system is simple, highly interpretable, and has strong application value."
Lifted and reattached behaviour of laminar premixed flame under external acoustic excitation,"The flame chemiluminescent emission fluctuations and the vortex structure of the lifted jet flame under acoustic excitation were studied in this investigation. By employing high-speed visualization and DFCD (Digital Flame Colour Discrimination) image processing method, the fluctuation of the instantaneous mixture fraction has been found highly correlated with the lifted height variations. It has been observed that during the flame drifting to downstream, there is no obvious shifting on the mixture fraction. However, when the flame travels back to upstream, the fuel mixture has been evidently diluted. In addition, the stabilisation mechanism can be further explained by analysing the velocity fluctuation of the vortices in the shear layer via PIV. Measurements show that, the turbulent stretching at the shear layer generated by the excitation leading to the flame lift-off. On the other hand, the Kelvin-Helmholtz vortices in the unburn part play a key role in preventing flame lift-off. But, the excessive external acoustic excitation leads to blow-off due to over dilution and increased lifted height."
Linking hyperelastic theoretical models and experimental data of vaginal tissue through histological data,"Mechanical characterization of living tissues and computer-based simulations related to medical issues, has become increasingly important to improve diagnostic processes and treatments evaluation. This work proposes a link between the mechanical testing and the material model predictions through histological data of vaginal tissue. Histological data was used to link tensile testing experiments with material-dependent parameters; the approach was adequate to capture the nonlinear response of ovine vaginal tissue over a large strain range. The experimental data obtained on a previous study, has two main components: tensile testing and histological analysis of the ovine vaginal tissue. Uniaxial tensile test data and histological data were collected from three sheep groups: virgins, pregnant and parous. The distal part of vaginal wall was selected since it is prone to tears induced by vaginal delivery. The HGO (Holzapfel-Gasser-Ogden) model parameters were fitted using a stochastic approach, namely the Simple Genetic Algorithm (SGA). The SGA was able to fit the experimental data successfully (R2 > 0.986). The dimensionless coefficient ξ, was highly correlated with histological data. The ratio was seen to increase linearly with increasing collagen content. Coefficient ξ brings a new way of interpreting and understanding experimental data; it connects the nonlinear mechanical behaviour (tensile test) with tissue’s morphology (histology). It can be used as an ‘inverse’ (approximate) method to estimate the mechanical properties without direct experimental measurements, through basic histology. In this context, the proposed methodology appears very promising in estimating the response of the tissue via histological information."
Liquid floodback detection for scroll compressor in a VRF system under heating mode,"Liquid floodback leads to negative consequences, such as extra energy consumption, comfort degradation for building occupants, and mechanical failure of compressors. Fault detection and diagnosis of liquid floodback at incipient stage are necessary for variable refrigerant flow (VRF) systems. This study proposes decision tree models to detect and diagnose the liquid floodback of the scroll compressor in a VRF system. First, feature selection through correlation analysis was performed to remove redundant variables. Second, classification and regression tree (CART) was used to develop decision tree models for detecting liquid floodback. Third, two faults that cause liquid floodback were examined, and another CART model was established to diagnose fault causes when a fault occurred. The CART models demonstrated desirable performance in floodback FDD. The rules extrapolated from the two data-driven models were in agreement with domain knowledge."
Liver fibrosis diagnosis support using the Dempster–Shafer theory extended for fuzzy focal elements,"Classifiers are used in a variety of applications, among them the classification of medical data. Their efficiency depends on the quality of training data, which is a disadvantage in the case of medical data that are often imperfect (e.g. incomplete, imbalanced, uncertain). Moreover, numerous classifiers are black-boxes from the perspective of diagnosticians who perform the final diagnoses. These drawbacks degrade the potential usefulness of classifiers in diagnosis support. A rule-based reasoning may overcome these mentioned limitations. We introduce both a rule selection and a diagnosis support method based on the Dempster–Shafer and fuzzy set theories. The theories can manage an interpretation of incomplete and imbalanced data, imprecision of medical information and knowledge uncertainty. The usefulness of the method will be proven on a test case of liver fibrosis diagnosis. The liver fibrosis stage is difficult to recognize even for experienced physicians. The diagnosis of the liver state by an invasive biopsy is ambiguous and dependent on its finite precision. Therefore, knowledge-based methods are being sought to reduce the need of invasive testing. We use a real medical database related to patients affected by hepatitis C virus to extract knowledge. The database has missing and outlying values and patients’ diagnoses are uncertain. The proposed methods provide simple diagnostic rules that are helpful in this study of liver fibrosis and in processing deficient data. The greatest benefit and novelty of the approach is the ability to assess three stages of fibrosis in a non-invasive way, whereas other medical tests allow to detect only the last stage, i.e. the cirrhosis."
"Localization of time shift failures in (max,+)-linear systems","The goal of this paper is to propose a localization method of time shift failures in timed discrete event systems (TDES) called (max, +)-linear systems and graphically represented by Timed Event Graph (TEG). First, a detection process produces indicators that determine whether such failures have happened by the observation of incoming and outcoming timed flows. Then, thanks to the knowledge of the behavior of the system through its corresponding TEG, set of failures that could explain the detected timed shift are obtained. It comes from matrices of signatures for each indicator built on each observable output of the system."
Long short-term memory for machine remaining life prediction,"Reliable tracking of performance degradation in dynamical systems such as manufacturing machines or aircraft engines and consequently, prediction of the remaining useful life (RUL) are one of the major challenges in realizing smart manufacturing. Traditional machine learning algorithms are often constrained in adapting to the complex and non-linear characteristics of manufacturing systems and processes. With the rapid development of modern computational hardware, Deep Learning has emerged as a promising computational technique for dynamical system prediction due to its enhanced capability to characterize the system complexity, overcoming the shortcomings of those traditional methods. In this paper, a new approach based on the Long Short-Term Memory (LSTM) network, an architecture that is specialized in discovering the underlying patterns embedded in time series, is proposed to track the system degradation and consequently, to predict the RUL. The objectives of this paper are: 1) translating the raw sensor data to an interpretable health index with the aim of better describing the system health condition; and 2) tracking the historical system degradation for accurate prediction of its future health condition. Evaluation using NASA’s C-MAPSS dataset verifies the effectiveness of the proposed method. Compared with other machine learning techniques, LSTM turns out to be more powerful and accurate in revealing degradation patterns, enabled by its time-dependent structure in nature."
Low frequency noise assessment in n- and p-channel sub-10nm triple-gate FinFETs: Part II: Measurements and results,"Low frequency noise measurements are used as a non-destructive diagnostic tool in order to evaluate the quality of the gate oxide and the silicon film of sub-10nm triple-gate Silicon-on-Insulator (SOI) FinFETs. It was found that the carrier number fluctuations explain the 1/f noise in moderate inversion for n- and p-FinFETs, which allows estimating the gate oxide trap densities. The noise spectroscopy with respect to temperature (study of the generation-recombination noise) led to the identification of the traps located in the transistors silicon film."
"LPV-MPC Fault Tolerant Control of Automotive Suspension Dampers⁎⁎This work has been supported by LabEx PERSYVAL-Lab (ANR11 -11- LABX - 0025 - 01), funded by the French program Investisse-ments d’Avenir (www.gipsa-lab.fr/projet/LPV4FTC/), and CAPES project BRAFITEC ECoSud.","The design of a Fault Tolerant dynamic output-feedback controller for Semi-Active Suspension Systems is considered in this work. The suspension system is assumed to undergo loss of effectiveness (time-varying) faults on each of the four actuators (suspension’s dampers). An active fault tolerant reconfiguration scheme is proposed, considering a Linear Parameter Varying (LPV) Model Predictive Control approach. The proposed solution aims to maintain the vehicle’s driving performances (handling and comfort indexes) whenever there are sudden actuator faults. These faults are identified through a parallel Fault Detection and Diagnosis scheme, which is also explained. The performance of the proposed control structure is demonstrated through simulation. Results show the good operation of this control scheme which is compared to other standard control approaches, considering a reduced-size car."
Lung Cancer Detection using CT Scan Images,"Lung cancer is one of the dangerous and life taking disease in the world. However, early diagnosis and treatment can save life. Although, CT scan imaging is best imaging technique in medical field, it is difficult for doctors to interpret and identify the cancer from CT scan images. Therefore computer aided diagnosis can be helpful for doctors to identify the cancerous cells accurately. Many computer aided techniques using image processing and machine learning has been researched and implemented. The main aim of this research is to evaluate the various computer-aided techniques, analyzing the current best technique and finding out their limitation and drawbacks and finally proposing the new model with improvements in the current best model. The method used was that lung cancer detection techniques were sorted and listed on the basis of their detection accuracy. The techniques were analyzed on each step and overall limitation, drawbacks were pointed out. It is found that some has low accuracy and some has higher accuracy but not nearer to 100%. Therefore, our research targets to increase the accuracy towards 100%."
Lung cancer survival period prediction and understanding: Deep learning approaches,"Introduction Survival period prediction through early diagnosis of cancer has many benefits. It allows both patients and caregivers to plan resources, time and intensity of care to provide the best possible treatment path for the patients. In this paper, by focusing on lung cancer patients, we build several survival prediction models using deep learning techniques to tackle both cancer survival classification and regression problems. We also conduct feature importance analysis to understand how lung cancer patients’ relevant factors impact their survival periods. We contribute to identifying an approach to estimate survivability that are commonly and practically appropriate for medical use. Methodologies We have compared the performance across three of the most popular deep learning architectures - Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN) while comparing the performing of deep learning models against traditional machine learning models. The data was obtained from the lung cancer section of Surveillance, Epidemiology, and End Results (SEER) cancer registry. Results The deep learning models outperformed traditional machine learning models across both classification and regression approaches. We obtained a best of 71.18 % accuracy for the classification approach when patients’ survival periods are segmented into classes of ‘<=6 months’,’ 0.5 – 2 years’ and ‘>2 years’ and Root Mean Squared Error (RMSE) of 13.5 % andR2 value of 0.5 for the regression approach for the deep learning models while the traditional machine learning models saturated at 61.12 % classification accuracy and 14.87 % RMSE in regression. Conclusions This approach can be a baseline for early prediction with predictions that can be further improved with more temporal treatment information collected from treated patients. In addition, we evaluated the feature importance to investigate the model interpretability, gaining further insight into the survival analysis models and the factors that are important in cancer survival period prediction."
Magnetic field associated with an internal fire whirl: A simple model,"A fire whirl can be generated by a pool fire in vertical shafts in tall buildings under certain ventilation conditions. Internal fire whirls are much more dangerous and destructive than non-whirling fires in the shaft and deserve more attention. As fires or flames consist of ions in motion, the characteristics of the magnetic field generated by a fire whirl would provide important information. The present study aims at measuring the magnetic field generated by a fire whirl and at building a simple model to explain the generated magnetic field. The physical origin of the magnetic field associated with a fire whirl is proposed, which consists in the interaction of the moving ions with the Earth’s magnetic field. It is shown that as far as the magnetic field around a fire whirl is concerned, the fire whirl is equivalent to a solenoid carrying a current Is, which is related to characteristics pertinent to the fire whirl. The vertical component of the magnetic field obtained from this model (Bzm) is compared with experimental results (Bze) acquired in fire whirls in a shaft model. The two sets of values (Bzm and Bze) are well correlated, with deviations which are reasonably acceptable. The results of the present work could be of value as a diagnostic tool in monitoring internal fire whirl and in studying various aspects of the whirl."
Measurements and interpretation of shock tube ignition delay times in highly CO2 diluted mixtures using multiple diagnostics,"Common definitions for ignition delay time are often hard to determine due to the issue of bifurcation and other non-idealities that result from high levels of CO2 addition. Using high-speed camera imagery in comparison with more standard methods (e.g., pressure, emission, and laser absorption spectroscopy) to measure the ignition delay time, the effect of bifurcation has been examined in this study. Experiments were performed at pressures between 0.6 and 1.2atm for temperatures between 1650 and 2040K. The equivalence ratio for all experiments was kept at a constant value of 1 with methane as the fuel. The CO2 mole fraction was varied between a value of XCO2=0.00 to 0.895. The ignition delay time was determined from three different measurements at the sidewall: broadband chemiluminescent emission captured viphotodetector, CH4 concentrations determined using a distributed feedback interband cascade laser centered at 3403.4nm, and pressure recorded vidynamic Kistler type transducer. All methods for the ignition delay time were compared to high-speed camera images taken of the axial cross-section during combustion. Methane time-histories and the methane decay times were also measured using the laser. It was determined that the flame could be correlated to the ignition delay time measured at the side wall but that the flame as captured by the camera was not homogeneous as assumed in typical shock tube experiments. The bifurcation of the shock wave resulted in smaller flames with large boundary layers and that the flame could be as small as 30% of the cross-sectional area of the shock tube at the highest levels of CO2 dilution. Comparisons between the camera images and the different ignition delay time methods show that care must be taken in interpreting traditional ignition delay data for experiments with large bifurcation effects as different methods in measuring the ignition delay time could result in different interpretations of kinetic mechanisms and impede the development of future mechanisms."
Medical image segmentation algorithm based on positive scaling invariant-self encoding CCA,"The quality of medical image segmentation results directly affect disease analysis and diagnosis. Although traditional medical image segmentation methods have achieved certain results, this type of method is not only computationally inefficient, but also difficult to achieve fully automatic segmentation. Deep learning has a good generalization ability, which provides a new technical approach to solve the above problems. However, the following problems exist in the application of deep learning in medical image segmentation: (1) Optimization of deep learning models. The parameters of the existing deep learning model have not been well optimized; it is easy to cause wrong medical image segmentation. (2) Overfitting problem. As the number of deep learning model layers increases, it makes the deep model fit the training data easily, but its generalization performance on the test set will be poor. In view of this, in order to solve the problem of deep learning model optimization, this paper first reveals that the weight space of deep learning is not equivalent to its parameter space, and explains in detail how to optimize the deep learning model in a positive scaling invariant space. The method of mapping the derivative of the base path back to the weight, and analyzed the stochastic optimization algorithm in F space. In addition, to solve the overfitting problem of deep learning models, this paper first gives a method of self-encoding Canonical Correlation Analysis (Self-encoding CCA) to optimize both self-coding loss and correlation loss in the fine-tuning stage. Based on the above ideas, this paper proposes a medical image segmentation algorithm based on positive scaling invariant-self encoding CCA. The experimental results show that the index obtained by the method proposed in this paper is not only improved by a large margin compared with traditional machine learning methods, but also improved to a certain extent compared with other deep learning methods."
Medical knowledge embedding based on recursive neural network for multi-disease diagnosis,"The representation of knowledge based on first-order logic captures the richness of natural language and supports multiple probabilistic inference models. Although symbolic representation enables quantitative reasoning with statistical probability, it is difficult to utilize with machine learning models as they perform numerical operations. In contrast, knowledge embedding (i.e., high-dimensional and continuous vectors) is a feasible approach to complex reasoning that can not only retain the semantic information of knowledge, but also establish the quantifiable relationship among embeddings. In this paper, we propose a recursive neural knowledge network (RNKN), which combines medical knowledge based on first-order logic with a recursive neural network for multi-disease diagnosis. After the RNKN is efficiently trained using manually annotated Chinese Electronic Medical Records (CEMRs), diagnosis-oriented knowledge embeddings and weight matrixes are learned. The experimental results confirm that the diagnostic accuracy of the RNKN is superior to those of four machine learning models, four classical neural networks and Markov logic network. The results also demonstrate that the more explicit the evidence extracted from CEMRs, the better the performance. The RNKN gradually reveals the interpretation of knowledge embeddings as the number of training epochs increases."
Meta-knowledge dictionary learning on 1-bit response data for student knowledge diagnosis,"This paper focuses on the problem of student knowledge diagnosis that is a basic task of realizing personalized education. Most traditional methods rely on the question-concept matrix empirically designed by experts. However, the expert concepts are expensive and inter-overlapping in their constructions, leading to ambiguous explanations. With the intuition that each student can master a part of the knowledge involved in all questions, in this paper, we propose a novel learning-based model for student knowledge diagnosis, dubbed Meta-knowledge Dictionary Learning (metaDL). MetaDL aims to learn a meta-knowledge dictionary from student responses, where any knowledge entity (e.g., student, question or expert concept) is a linear combination of a few atoms in the meta-knowledge dictionary. The resultant problem could be effectively solved by developing the alternating direction method of multipliers. This study has three innovations: learning independent meta-knowledges instead of traditional complex concepts, sparely representing knowledge entity instead of densely weighted representation, and interpreting expert concepts with the resulting meta-knowledges. For evaluation, the diagnosis results from metaDL are used to group students and predict responses on two public datasets and a private dataset from our institution. The experiment results show that metaDL delivers an effective student knowledge diagnosis and then results in good performances on the two applications in comparison with other methods. This technique could provide significant insights into student’s knowledge state and facilitate the progress on personalized education."
Metric-based meta-learning model for few-shot fault diagnosis under multiple limited data conditions,"The real-world large industry has gradually become a data-rich environment with the development of information and sensor technology, making the technology of data-driven fault diagnosis acquire a thriving development and application. The success of these advanced methods depends on the assumption that enough labeled samples for each fault type are available. However, in some practical situations, it is extremely difficult to collect enough data, e.g., when the sudden catastrophic failure happens, only a few samples can be acquired before the system shuts down. This phenomenon leads to the few-shot fault diagnosis aiming at distinguishing the failure attribution accurately under very limited data conditions. In this paper, we propose a new approach, called Feature Space Metric-based Meta-learning Model (FSM3), to overcome the challenge of the few-shot fault diagnosis under multiple limited data conditions. Our method is a mixture of general supervised learning and episodic metric meta-learning, which will exploit both the attribute information from individual samples and the similarity information from sample groups. The experiment results demonstrate that our method outperforms a series of baseline methods on the 1-shot and 5-shot learning tasks of bearing and gearbox fault diagnosis across various limited data conditions. The time complexity and implementation difficulty have been analyzed to show that our method has relatively high feasibility. The feature embedding is visualized by t-SNE to investigate the effectiveness of our proposed model."
mHealth: Monitoring Platform for Diabetes Patients,"Diabetes is a metabolic disease that can be explained by the high level of glucose in the blood. Constant monitoring of patients with this type of disease is crucial to the success of their treatment due to the high number of factors that condition it, such as nutrition, exercise and insulin production. This research consists of a software development project based on mHealth practice, which aims to cover all the needs of patients and health professionals, introducing improvements in the prevention, diagnosis and control of endocrine pathology, as well as improvements in hospital management. The web platform should be able to send a warning to the healthcare professional in cases where a patient’s recorded level exceeds normal values and contain all the patient’s records. The aim is to provide support to treatment, monitoring and data collection based on IoT principles, where medical devices allow communication between machines and interaction between them, sharing and managing data. The healthcare professional will have the necessary information to assess the health status of his patient and, if necessary, make some changes to improve the patient’s daily routines."
Mining association rules for anomaly detection in dynamic process runtime behavior and explaining the root cause to users,"Detecting anomalies in process runtime behavior is crucial: they might reflect, on the one side, security breaches and fraudulent behavior and on the other side desired deviations due to, for example, exceptional conditions. Both scenarios yield valuable insights for process analysts and owners, but happen due to different reasons and require a different treatment. Hence a distinction into malign and benign anomalies is required. Existing anomaly detection approaches typically fall short in supporting experts when in need to take this decision. An additional problem are false positives which could result in selecting incorrect countermeasures. This paper proposes a novel anomaly detection approach based on association rule mining. It fosters the explanation of anomalies and the estimation of their severity. In addition, the approach is able to deal with process change and flexible executions which potentially lead to false positives. This facilitates to take the appropriate countermeasure for a malign anomaly and to avoid the possible termination of benign process executions. The feasibility and result quality of the approach are shown by a prototypical implementation and by analyzing real life logs with injected artificial anomalies. The explanatory power of the presented approach is evaluated through a controlled experiment with users."
Mixed impact of torsion on LV hemodynamics: A CFD study based on the Chimera technique,"Image-based patient-specific Computational Fluid Dynamics (CFD) models of the Left Ventricle (LV) can be used to quantify hemodynamics-based biomarkers that can support the clinicians in the early diagnosis, follow-up and treatment planning of patients, beyond the capabilities of the current imaging modalities. We propose a workflow to build patient-specific CFD models of the LV with moving boundaries based on the Chimera technique to overcome the convergence issues previously encountered by means of the Arbitrarian Lagrangian Eulerian approach. The workflow was tested while investigating whether the torsional motion has an impact on LV fluid dynamics. Starting from 3D cine MRI scans of a healthy volunteer, six cardiac cycles were simulated in three CFD LV models: with no, physiological, and exaggerated torsion. The Chimera technique was robust in handling the impulsive motion of the LV endocardium, allowing to notice cycle-to-cycle variations in every simulated case. Torsion affected slightly velocity, vorticity, WSS. It did not affect energy loss and induced a double-sided effect in terms of residence time: the particles ejected in one beat decreased, whereas the motility of the particles remaining in the LV was affected only in the exaggerated torsion case, indicating that implementation of torsion can be discarded in case of physiological levels. Nonetheless, caution is warranted when interpreting these results given the absence of the mitral valve, the papillary muscles, and the trabeculae. The effects of the mitral valve will be evaluated within an Fluid Structure Interaction simulation framework as further development of the current model."
Mixed potential type acetone sensor based on GDC used for breath analysis,"Breath analysis is of great significance for the early diagnosis of diabetic ketoacidosis. Gas sensors with simple structure and low cost have become candidates for breath analysis. The detection range and reliability of the device play an important role in the actual application. In this work, we used Bi1-xSrxFeO3 (x = 0.2, 0.4, 0.6, 0.8) prepared by sol-gel method to fabricate Ce0.8Gd0.2O1.95 (GDC)-based mixed potential type acetone sensors, and obtained satisfactory sensing performance. The amount of Sr substitution in sensing materials could change the content of deﬁcient oxygen inside the material and the electrochemical catalytic activity of the material, thereby improving the sensitivity of the sensor. Mixed potential sensing mechanism and the reason for sensitivity segmentation phenomenon were also explained. In addition, we did clinical tests for the sensor towards the exhalation of diabetic patients and healthy people, which displayed the potential of the sensor used for breath analysis."
Mixed-integer optimization approach to learning association rules for unplanned ICU transfer,"After admission to emergency department (ED), patients with critical illnesses are transferred to intensive care unit (ICU) due to unexpected clinical deterioration occurrence. Identifying such unplanned ICU transfers is urgently needed for medical physicians to achieve two-fold goals: improving critical care quality and preventing mortality. A priority task is to understand the crucial rationale behind diagnosis results of individual patients during stay in ED, which helps prepare for an early transfer to ICU. Most existing prediction studies were based on univariate analysis or multiple logistic regression to provide one-size-fit-all results. However, patient condition varying from case to case may not be accurately examined by such a simplistic judgment. In this study, we present a new decision tool using a mathematical optimization approach aiming to automatically discover rules associating diagnostic features with high-risk outcome (i.e., unplanned transfers) in different deterioration scenarios. We consider four mutually exclusive patient subgroups based on the principal reasons of ED visits: infections, cardiovascular/respiratory diseases, gastrointestinal diseases, and neurological/other diseases at a suburban teaching hospital. The analysis results demonstrate significant rules associated with unplanned transfer outcome for each subgroups and also show comparable prediction accuracy (>70%) compared to state-of-the-art machine learning methods while providing easy-to-interpret symptom-outcome information."
Mobile Expert System Using Fuzzy Tsukamoto for Diagnosing Cattle Disease,"Handling reproductive disorders in the cattle farming businesses is still ineffective due to a lack of information about the treatment of cattle, which has led to a significant decline in meat production. To reduce the impact of reproductive disease, it is necessary to perform detection and early treatment to prevent significant losses and a wider spread of the disease. This article explains the application of an expert system that provides a means of consultation imitating the reasoning process of an expert in solving complex problems concerning the health of cows reproduction. We will apply Fuzzy Tsukamoto method to help diagnose the level of risk of disease in cattle based on six clinical symptoms. The result of this research is a mobile expert system that will conclude the level of risk of endometritis disease in cattle. This mobile application is developed based on android, for ease of use, and can be used by farmers in making the diagnosis by themselves. The validation results of this expert system show that the system is able to determine the level of risk of endometritis for cow’s reproduction disease."
Model Selection ensuring Practical Identifiability for Models of Electric Drives with Coupled Mechanics,"Physically motivated models of electric drive trains with coupled mechanics are ubiquitous in industry for control design, simulation, feed-forward, model-based fault diagnosis etc. Often, however, the effort of model building prohibits these model-based methods. In this paper an automated model selection strategy is proposed for dynamic simulation models that not only optimizes the accuracy of the fit but also ensures practical identifiability of model parameters during structural optimization. Practical identifiability is crucial for physically motivated, interpretable models as opposed to pure prediction and inference applications. Our approach extends structural optimization considering practical identifiability to nonlinear models. In spite of the nonlinearity, local and linear criteria are evaluated, the integrity of which is investigated exemplarily. The methods are validated experimentally on a stacker crane."
Model-based information fusion investigation on fault isolation of subsea systems considering the interaction among subsystems and sensors,"The offshore oil industry has expanded to deep water and Arctic. The harsh operating conditions (e.g., ice and strong wind) and increasing complicated system raise the occurrence likelihood of system faults. This requires timely fault isolation and management in the subsea system. However, the offshore oil industry mainly relies on humans to isolate faults based on alarms. With harsh operating conditions and increasing complicated system, this industry urgently needs research on more efficient fault isolation and cause diagnosis methods. Unfortunately, limited research is conducted on fault isolation method in the offshore oil industry. Furthermore, in industry 4.0 era, large amounts of information are obtained. This provides precondition for the application of information fusion technique which aims to improve diagnosis results. However, to the authors’ knowledge, information fusion has not been much studied in the fault isolation of the offshore oil industry. Moreover, the interaction of different subsystems contains valuable information. How the interaction of different subsystems can influence the fault diagnosis has not been explored. This paper proposes a Bayesian network (BN) based method for timely fault isolation and cause diagnosis for the offshore oil industry. The work fuses different information, and it also includes the dependency among different subsystems in the fault diagnosis. As an important alarm source, false alarms are also taken into account in the model. A case study on the subject of the subsea wellhead and chemical injection systems is conducted to demonstrate the functions and merits of the proposed method."
Model-free prostate cancer segmentation from dynamic contrast-enhanced MRI with recurrent convolutional networks: A feasibility study,"Dynamic contrast enhanced (DCE) magnetic resonance imaging (MRI) is a method of temporal imaging that is commonly used to aid in prostate cancer (PCa) diagnosis and staging. Typically, machine learning models designed for the segmentation and detection of PCa will use an engineered scalar image called Ktrans to summarize the information in the DCE time-series images. This work proposes a new model that amalgamates the U-net and the convGRU neural network architectures for the purpose of interpreting DCE time-series in a temporal and spatial basis for segmenting PCa in MR images. Ultimately, experiments show that the proposed model using the DCE time-series images can outperform a baseline U-net segmentation model using Ktrans. However, when other types of scalar MR images are considered by the models, no significant advantage is observed for the proposed model."
Modeling and simulation of crew to crew response variability due to problem-solving styles,"An accurate and prompt diagnosis of the power plant state is crucial for the operators to successfully handle an accident or anomaly. A successful situation diagnosis is the result of a series of reasoning and a thought process which integrate the operator's observations, knowledge, and experiences. Simulating reasoning processes is useful for analyzing operator performance on situation diagnosis. Reasoning and other cognitive and physical response of operators are in part influenced by behavioral tendencies. This paper introduces a new capability of the ADS-IDAC dynamic PRA model to simulate the effects of different cognitive tendencies (problem-solving styles) of the operating crew. To do so a set of generic rules are defined and implemented in the operator model of the ADS-IDAC simulation platform. These rules cover three distinctly problem-solving styles, named Vagabond, Hamlet, and Garden-path operators. Each problem-solving style represents a set of behavioral or cognitive tendencies. Modeling techniques are proposed for these problem-solving styles and implemented in a new version of ADS-IDAC. A simulation case study based on an international benchmark study on HRA methods is used to demonstrate the new capabilities in partially explaining some of the crew-to-crew variability observed in the empirical study."
Modeling eye movement patterns to characterize perceptual skill in image-based diagnostic reasoning processes,"Experts have a remarkable capability of locating, perceptually organizing, identifying, and categorizing objects in images specific to their domains of expertise. In this article, we present a hierarchical probabilistic framework to discover the stereotypical and idiosyncratic viewing behaviors exhibited with expertise-specific groups. Through these patterned eye movement behaviors we are able to elicit the domain-specific knowledge and perceptual skills from the subjects whose eye movements are recorded during diagnostic reasoning processes on medical images. Analyzing experts’ eye movement patterns provides us insight into cognitive strategies exploited to solve complex perceptual reasoning tasks. An experiment was conducted to collect both eye movement and verbal narrative data from three groups of subjects with different levels or no medical training (eleven board-certified dermatologists, four dermatologists in training and thirteen undergraduates) while they were examining and describing 50 photographic dermatological images. We use a hidden Markov model to describe each subject’s eye movement sequence combined with hierarchical stochastic processes to capture and differentiate the discovered eye movement patterns shared by multiple subjects within and among the three groups. Independent experts’ annotations of diagnostic conceptual units of thought in the transcribed verbal narratives are time-aligned with discovered eye movement patterns to help interpret the patterns’ meanings. By mapping eye movement patterns to thought units, we uncover the relationships between visual and linguistic elements of their reasoning and perceptual processes, and show the manner in which these subjects varied their behaviors while parsing the images. We also show that inferred eye movement patterns characterize groups of similar temporal and spatial properties, and specify a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the occurrences of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies in a novel way. In each category, images share similar lesion distributions and configurations. Our results show that modeling with multi-modal data, representative of physicians’ diagnostic viewing behaviors and thought processes, is feasible and informative to gain insights into physicians’ cognitive strategies, as well as medical image understanding."
Modular Fault Diagnosis in Fixed-Block Railway Signaling Systems,"The diagnosis of possible faults in railway signaling systems is an important issue to provide safe travel and transportation in railways. Signaling system designers have to consider the possible faults which may occur in railway field components both on the requirements preparation phase and on the development phase of the signaling system software or namely, the interlocking system. Although the diagnosis of different unobservable faults is relatively hard, especially for large scale railway fields, this complexity can be overcome by using the Discrete Event System (DES) based modular diagnosis approach which is explained in this paper. The main advantage of using such modular approach for fault diagnosis in fixed-block signaling systems is the inspection of the diagnosability of the whole system with respect to its subsystems (railway field components). In this study, the diagnosability of the railway field equipment and the whole system is also explained with a case study."
Molecularly Imprinted Polymers for Diagnostics: Sensing High Density Lipoprotein and Dengue Virus,"Molecularly imprinted polymers (MIP) have been an attractive area of research for the past couple decades. Even though they constitute a well-established field of functional materials MIPs still have to make their way into commercial applications. The lack thereof may be explained in part by the absence of MIP systems aimed at analytes that are of relevance in everyday life. In this article two different sensing applications are presented each targeting a clinically relevant analyte. These include the first-ever MIP of high density lipoprotein (HDL), which is exploited as a cardiovascular biomarker, as well as a MIP targeting Dengue virus particles. In both cases the data presented suggests that MIPs targeting these two particular analytes show characteristic behavior that makes them potentially suitable for diagnostic purposes."
Monitoring of laser metal deposition height by means of coaxial laser triangulation,"Laser metal deposition (LMD) is an additive manufacturing technique, whose performances can be influenced by several factors and parameters. Monitoring their evolution allows for a better comprehension and control of the process, hence enhancing the deposition quality. In particular, the deposition height is an important variable that, if it does not match the process growth, can bring to defects and geometrical inaccuracies in the deposited structures. The current work presents a system based on optical triangulation for the height monitoring, implemented on a LMD setup composed of a fiber laser, a deposition head, and an anthropomorphic robot. Its coaxial and non-intrusive configuration allows for flexibility in the deposition strategy and direction. A measurement laser beam is launched through the powder nozzle and hits the melt pool. A coaxial camera acquires the probe spot, whose position is converted to relative height. The device has been demonstrated for monitoring the deposition of a stainless steel cylinder. The measurements allowed to reconstruct a spatial map of the height variation, highlighting a transient in the deposition growth which can be explained in terms of a self-regulating mechanism for the layer thickness."
MorphoCol: An ontology-based knowledgebase for the characterisation of clinically significant bacterial colony morphologies,"Background One of the major concerns of the biomedical community is the increasing prevalence of antimicrobial resistant microorganisms. Recent findings show that the diversification of colony morphology may be indicative of the expression of virulence factors and increased resistance to antibiotic therapeutics. To transform these findings, and upcoming results, into a valuable clinical decision making tool, colony morphology characterisation should be standardised. Notably, it is important to establish the minimum experimental information necessary to contextualise the environment that originated the colony morphology, and describe the main morphological features associated unambiguously. Results This paper presents MorphoCol, a new ontology-based tool for the standardised, consistent and machine-interpretable description of the morphology of colonies formed by human pathogenic bacteria. The Colony Morphology Ontology (CMO) is the first controlled vocabulary addressing the specificities of the morphology of clinically significant bacteria, whereas the MorphoCol publicly Web-accessible knowledgebase is an end-user means to search and compare CMO annotated colony morphotypes. Its ultimate aim is to help correlate the morphological alterations manifested by colony-forming bacteria during infection with their response to the antimicrobial treatments administered. Conclusions MorphoCol is the first tool to address bacterial colony morphotyping systematically and deliver a free of charge resource to the community. Hopefully, it may introduce interesting features of analysis on pathogenic behaviour and play a significant role in clinical decision making. Database URL http://morphocol.org."
Mortality prediction of septic shock patients using probabilistic fuzzy systems,"Mortality scores based on multiple regressions are common in critical care medicine for prognostic stratification of patients. However, to be used at the point of care, they need to be both accurate and easily interpretable. In this work, we propose the application of one existent type of rule base system using statistical information – probabilistic fuzzy systems (PFS) – to predict mortality of septic shock patients. To assess its accuracy and interpretability, these models are compared to methodologies previously proposed in this domain: Takagi-Sugeno fuzzy models and logistic regression models. The methods are tested using a retrospective cohort study including ICU patients with abdominal septic shock. Regarding accuracy, PFS models are comparable to fuzzy modeling and logistic regression. In terms of interpretability, results indicate that PFS models increase the transparency of the learned system (using fuzzy rules), but at the same time, provide additional means for validating the fuzzy classifier using expert knowledge (from physicians in this paper). By providing accurate and interpretable estimates for the mortality risk, results suggest the usefulness of PFS to develop scores for critical care medicine."
Motion estimation and correction in cardiac CT angiography images using convolutional neural networks,"Cardiac motion artifacts frequently reduce the interpretability of coronary computed tomography angiography (CCTA) images and potentially lead to misinterpretations or preclude the diagnosis of coronary artery disease (CAD). In this paper, a novel motion compensation approach dealing with Coronary Motion estimation by Patch Analysis in CT data (CoMPACT) is presented. First, the required data for supervised learning is generated by the Coronary Motion Forward Artifact model for CT data (CoMoFACT) which introduces simulated motion to 19 artifact-free clinical CT cases with step-and-shoot acquisition protocol. Second, convolutional neural networks (CNNs) are trained to estimate underlying 2D motion vectors from 2.5D image patches based on the coronary artifact appearance. In a phantom study with computer-simulated vessels, CNNs predict the motion direction and the motion magnitude with average test accuracies of 13.37°±1.21° and 0.77 ± 0.09 mm, respectively. On clinical data with simulated motion, average test accuracies of 34.85°±2.09° and 1.86 ± 0.11 mm are achieved, whereby the precision of the motion direction prediction increases with the motion magnitude. The trained CNNs are integrated into an iterative motion compensation pipeline which includes distance-weighted motion vector extrapolation. Alternating motion estimation and compensation in twelve clinical cases with real cardiac motion artifacts leads to significantly reduced artifact levels, especially in image data with severe artifacts. In four observer studies, mean artifact levels of 3.08 ± 0.24 without MC and 2.28 ± 0.29 with CoMPACT MC are rated in a five point Likert scale."
Motion periods of sun gear dynamic fault meshing positions in planetary gear systems,"Special assembly manner provides planetary gearboxes unique transmission characteristics, meanwhile brings the diagnostic complexity vifixed sensor due to the irregular dynamic nature of gear fault meshing positions. This paper proposes a systematic scheme to explore the motion periods of the sun gear fault meshing positions in a planetary gearbox, namely tidal periods. The generalized expressions to the tidal periods are derived and validated through the operational mechanism of a planetary gearbox. The tidal periods are incorporated into simulated signal models so that the unique influences to the vibration responses of a planetary gearbox are revealed. Some previously uncleared and uninterpreted fault-related sidebands can, therefore, be properly interpreted. Moreover, a tidal period is proved to be a minimized time duration that embraces complete fault induced information. Through simple statistical analysis of experimental data under different operating speeds, a minimum required data length for proper fault diagnosis is recommended based on the analysis of a tidal period. The tidal period is an intrinsic nature due to the operational property of a planetary gearbox."
"Multi-channel, convolutional attention based neural model for automated diagnostic coding of unstructured patient discharge summaries","Effective coding of patient records in hospitals is an essential requirement for epidemiology, billing, and managing insurance claims. The prevalent practice of manual coding, carried out by trained medical coders, is error-prone and time-consuming. Mitigating this labor-intensive process by developing diagnostic coding systems built on patients’ Electronic Medical Records (EMRs) is vital. However, developing nations with low digitization rates have limited availability of structured EMRs, thereby necessitating a need for systems that leverage unstructured data sources. Despite the rich clinical information available in such unstructured data, modeling them is complex, owing to the variety and sparseness of diagnostic codes, complex structural and temporal nature of summaries, and prolific use of medical jargon. This work proposes a context-attentive network to facilitate automatic diagnostic code assignment as a multi-label classification problem. The proposed model facilitates information aggregation across a patient’s discharge summary via multi-channel, variable-sized convolutional filters to extract multi-granular snippets. The attention mechanism enables selecting vital segments in those snippets that map to the clinical codes. The model’s superior performance underscores its effectiveness compared to the state-of-the-art on the MIMIC-III database. Additionally, experimental validation using the CodiEsp dataset exhibited the model’s interpretability and explainability."
Multi-class Arrhythmia detection from 12-lead varied-length ECG using Attention-based Time-Incremental Convolutional Neural Network,"Automatic arrhythmia detection from Electrocardiogram (ECG) plays an important role in early prevention and diagnosis of cardiovascular diseases. Convolutional neural network (CNN) is a simpler, more noise-immune solution than traditional methods in multi-class arrhythmia classification. However, suffering from lack of consideration for temporal feature of ECG signal, CNN couldn’t accept varied-length ECG signal and had limited performance in detecting paroxysmal arrhythmias. To address these issues, we proposed attention-based time-incremental convolutional neural network (ATI-CNN), a deep neural network model achieving both spatial and temporal fusion of information from ECG signals by integrating CNN, recurrent cells and attention module. Comparing to CNN model, this model features flexible input length, halved parameter amount as well as more than 90% computation reduction in real-time processing. The experiment result shows that, ATI-CNN reached an overall classification accuracy of 81.2%. In comparison with a classical 16-layer CNN named VGGNet, ATI-CNN achieved accuracy increases of 7.7% in average and up to 26.8% in detecting paroxysmal arrhythmias. Combining all these excellent features, ATI-CNN offered an exemplification for all kinds of varied-length signal processing problems."
Multi-domain modeling of atrial fibrillation detection with twin attentional convolutional long short-term memory neural networks,"Atrial fibrillation (AF) is a common arrhythmia, and its incidence increases with age. Many methods have been developed to identify AF, including both the hand-picked features by experts and the recent emerging artificial intelligent (AI) methods. As the traditional hand-picked features have almost reached the boundary of their capability, the AI methods have shown their great potentials to achieve high accuracy for the AF identification. However, some common AI methods, especially deep learning methods, do not provide good properties of interpretability, making it difficult to explore the internal relationship between input and prediction results. In addition, most of the reported methods are only for the intra-patient test of AF and Non-AF. In this study, we try to develop an AF detector based on a twin-attentional convolutional long short-term memory neural network (TAC-LSTM), which can not only generate results with high accuracy but also enable a human-friendly function to provide the possible explanations of the automated extracted features by AI. TAC-LSTM was applied to extract multi-domain features of ECG signals for AF detection and to mine the influence of different input segments on the final prediction results. Finally, the proposed method is validated on the MIT-BIH Atrial Fibrillation Database (AFDB) with intra-patient test and inter-patient test and the results also have shown that multi-domain features extracted by TAC-LSTM can provide more useful information. Collectively, TAC-LSTM can be used for clinicians as an auxiliary diagnostic tool."
Multiple instance learning: A survey of problem characteristics and applications,"Multiple instance learning (MIL) is a form of weakly supervised learning where training instances are arranged in sets, called bags, and a label is provided for the entire bag. This formulation is gaining interest because it naturally fits various problems and allows to leverage weakly labeled data. Consequently, it has been used in diverse application fields such as computer vision and document classification. However, learning from bags raises important challenges that are unique to MIL. This paper provides a comprehensive survey of the characteristics which define and differentiate the types of MIL problems. Until now, these problem characteristics have not been formally identified and described. As a result, the variations in performance of MIL algorithms from one data set to another are difficult to explain. In this paper, MIL problem characteristics are grouped into four broad categories: the composition of the bags, the types of data distribution, the ambiguity of instance labels, and the task to be performed. Methods specialized to address each category are reviewed. Then, the extent to which these characteristics manifest themselves in key MIL application areas are described. Finally, experiments are conducted to compare the performance of 16 state-of-the-art MIL methods on selected problem characteristics. This paper provides insight on how the problem characteristics affect MIL algorithms, recommendations for future benchmarking and promising avenues for research. Code is available on-line at https://github.com/macarbonneau/MILSurvey."
Multiplex reverse transcription loop-mediated isothermal amplification combined with nanoparticle-based lateral flow biosensor for the diagnosis of COVID-19,"The ongoing global pandemic (COVID-19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has become a huge public health issue. Hence, we devised a multiplex reverse transcription loop-mediated isothermal amplification (mRT-LAMP) coupled with a nanoparticle-based lateral flow biosensor (LFB) assay (mRT-LAMP-LFB) for diagnosing COVID-19. Using two LAMP primer sets, the ORF1ab (opening reading frame 1a/b) and N (nucleoprotein) genes of SARS-CoV-2 were simultaneously amplified in a single-tube reaction, and detected with the diagnosis results easily interpreted by LFB. In presence of FITC (fluorescein)-/digoxin- and biotin-labeled primers, mRT-LAMP produced numerous FITC-/digoxin- and biotin-attached duplex amplicons, which were determined by LFB through immunoreactions (FITC/digoxin on the duplex and anti-FITC/digoxin on the test line of LFB) and biotin/treptavidin interaction (biotin on the duplex and strptavidin on the polymerase nanoparticle). The accumulation of nanoparticles leaded a characteristic crimson band, enabling multiplex analysis of ORF1ab and N gene without instrumentation. The limit of detection (LoD) of COVID-19 mRT-LAMP-LFB was 12 copies (for each detection target) per reaction, and no cross-reactivity was generated from non-SARS-CoV-2 templates. The analytical sensitivity of SARS-CoV-2 was 100% (33/33 oropharynx swab samples collected from COVID-19 patients), and the assay's specificity was also 100% (96/96 oropharynx swab samples collected from non-COVID-19 patients). The total diagnostic test can be completed within 1 h from sample collection to result interpretation. In sum, the COVID-19 mRT-LAMP-LFB assay is a promising tool for diagnosing SARS-CoV-2 infections in frontline public health field and clinical laboratories, especially from resource-poor regions."
Multi-State System Modeling and Reliability Assessment for Groups of High-speed Train Wheels,"The polygonization of high-speed train wheels is commonly considered as a serious threat to reliability of high-speed trains all over the world. Traditional studies in polygonization of wheels focus on modeling its mechanism via physics-based models. However, to the knowledge of the authors, the polygonization process has not been fully explained by existing studies due to the complex degradation mechanism and varying operation environment which is difficult to monitor and measure. In this article, the data-driven multi-state system (MSS) models are developed for the reliability assessment of high-speed train wheels based on the analysis of actual operation data. The results can be used to support practical maintenance decision making. Another contribution of this paper is to prove that the environmental factors exert significant impacts on the reliability of Chinese high-speed train wheels. This finding has been included in the development of the MSS models. The results by these models are used to suggest a more reasonable maintenance schedule for Chinese high-speed train wheels. The obtained maintenance intervals are expected to assist the railway companies to reduce the operational risk as well as improve the maintenance efficiency."
Multi-task contrastive learning for automatic CT and X-ray diagnosis of COVID-19,"Computed tomography (CT) and X-ray are effective methods for diagnosing COVID-19. Although several studies have demonstrated the potential of deep learning in the automatic diagnosis of COVID-19 using CT and X-ray, the generalization on unseen samples needs to be improved. To tackle this problem, we present the contrastive multi-task convolutional neural network (CMT-CNN), which is composed of two tasks. The main task is to diagnose COVID-19 from other pneumonia and normal control. The auxiliary task is to encourage local aggregation though a contrastive loss: first, each image is transformed by a series of augmentations (Poisson noise, rotation, etc.). Then, the model is optimized to embed representations of a same image similar while different images dissimilar in a latent space. In this way, CMT-CNN is capable of making transformation-invariant predictions and the spread-out properties of data are preserved. We demonstrate that the apparently simple auxiliary task provides powerful supervisions to enhance generalization. We conduct experiments on a CT dataset (4,758 samples) and an X-ray dataset (5,821 samples) assembled by open datasets and data collected in our hospital. Experimental results demonstrate that contrastive learning (as plugin module) brings solid accuracy improvement for deep learning models on both CT (5.49%-6.45%) and X-ray (0.96%-2.42%) without requiring additional annotations. Our codes are accessible online."
Multivariate fuzzy analysis of brain tissue volumes and relaxation rates for supporting the diagnosis of relapsing-remitting multiple sclerosis,"Multiple Sclerosis (MS) is a chronic neuroinflammatory disorder of the brain and spinal cord, widely studied nowadays, due to its relevant prevalence in the population. Even though no cure exists, an earlier and more adequate choice of treatment could delay its evolution and prevent irreversible sequelae and disability progression. Currently, Magnetic Resonance Imaging (MRI) represents an essential nonclinical tool for the detection of a hallmark of the disease, i.e. the presence of demyelinating lesions within cerebral white matter (WM), and, consequently, for the diagnosis of MS early within its course. However, errors in estimating lesions can contribute to a wrong diagnosis, if only the WM lesion load is taken into account, with a more relevant impact in individuals with a reduced lesion load at an initial clinical event, delaying the start of a treatment until a second clinical relapse or after confirming, successively, dissemination of lesions in time. In this context, this work proposes an innovative system, employing a multivariate analysis approach, with the aim of mining and integrating multiple sensitive neuroimaging markers, including but not limited to the WM lesion load, into classification models for supporting a more robust diagnosis of Relapsing-Remitting-MS (RR-MS) already at an initial clinical event. To this aim, a retrospective study of 81 patients with diagnosis of RR-MS (39 males and 42 females, 37.3 ± 8.1 years old, age range 20–58) and 29 healthy people of comparable age and gender (15 males and 14 females, 39.7 ± 11.1 years old, age range 22–57) is used. All the individuals were enrolled at Multiple Sclerosis Centre of the “Federico II” University Hospital (Naples, Italy). A machine learning method based on both statistics and Fuzzy Logic, already validated for its desirable characteristics, is applied to volumetric and relaxometric data extracted from brain MRI by means of a multiparametric relaxometric segmentation method. As results of this work, one- and multi-dimensional models are generated and compared with those obtained with other state-of-the-art methods, confirming their goodness in terms of performance, interpretability and robustness, even if their generalizability is limited by the dependence of measurements on MRI scanner. More in particular, the most promising model is two-dimensional and presents a high value of accuracy (89.1%), a particularly high sensitivity (93.8%) and satisfactory negative predictive value (81.5%) and positive predictive value (91.6%). This model identifies a further imaging marker related to grey matter alterations that, combined to the WM lesion load, allows to discriminate more accurately between RR-MS patients and healthy subjects, perform a more robust detection of RR-MS patients at the disease onset and, contextually, confidently exclude the disease presence in subjects classified as healthy. In addition, due to its interpretability, this model is able to clarify how and how much WM lesion load and grey matter alterations are correlated to RR-MS. Finally, it allows personalized diagnosis support, by calculating reliable classification confidence associated to each particular case."
Mutual information-based sparse multiblock dissimilarity method for incipient fault detection and diagnosis in plant-wide process,"Multiblock methods have attracted much attention in monitoring of plant-wide processes. However, most recent works only provide rough division results and do not take the data distribution changes among the process into consideration. To solve these defects, a new multiblock monitoring scheme that integrates a mutual information (MI)-based sparsification method, dissimilarity analysis (DISSIM) method and support vector data description (SVDD) is proposed in this paper. This method makes use of the complex relations between variables and the connections between sub-blocks in process decomposition to produce easy-to-interpret sub-blocks related to the process mechanism. Then DISSIM method is applied in each sub-block for distribution monitoring. The multiblock DISSIM strategy can deal with the local behaviors and distribution changes, improving its sensitivity to incipient changes in plant-wide process. Finally, results in all blocks are combined with SVDD to provide a final decision, and a diagnosis method is proposed for fault diagnosis. Case studies upon a numerical case and Tennessee Eastman (TE) benchmark process demonstrate the effectiveness of our method."
Neural network models for supporting drug and multidrug resistant tuberculosis screening diagnosis,"Tuberculosis (TB) is the leading cause of global mortality among communicable diseases. The diagnosis of Drug-Resistant Tuberculosis (DR-TB) demands even more attention, leading to longer treatments and higher deceased rates. All diagnostic methods available have deficiencies in their detection rates, time release results, or have a higher cost and need a complex infrastructure to setup. New molecular diagnostics, such as the Xpert MTB/RIF assay, have great potential for revolutionizing the diagnosis of Rifampicin Resistance (RR). But, a positive RR result with this test should be carefully interpreted and take into consideration the risk of Multidrug-Resistant TB (MDR-TB) according to its prevalence, locally. Therefore, the development of screening approaches for DR/MDR-TB suspects would help to identify those should be tested by Xpert MTB/RIF. This work develops Artificial Neural Network (ANN) models considering data from presumed DR/MDR-TB subjects according to the National Guidelines at Rio de Janeiro/Brazil, attended in reference centers in Rio de Janeiro, from Feb 2011 and May 2013. Subjects aged 18 years or older, and results were compared with models based on Classification And Regression Trees (CART). Practical operation at different epidemiological scenarios are considered by constructing models using different variable selection criteria, so that environments with low resource conditions can be assisted. Among 280 presumed DR-TB cases included, 38 were DR-TB, 48-MDR, 32-Drug-Sensitive and 162 with no TB. Between DR-TB and non DR-TB, the sensitivity and specificity reached 95.1%(±5.0) and 85.0%(±4.9), respectively. The promising results of clinical score for DR/MDR-TB diagnosis indicate that this approach may be used in the evaluation of presumed DR/MDR-TB."
Nonlinear aging of cylindrical lithium-ion cells linked to heterogeneous compression,"Second-life applications of automotive lithium-ion batteries are currently investigated for grid stabilization. Reutilization depends on reliable projections of the remaining useful life. However, reports on sudden degradation of lithium-ion-cells near 80% state of health challenge these extrapolations. Sudden degradation was demonstrated for different positive active materials. This work elucidates the cause of sudden degradation in detail. As part of a larger study on nonlinear degradation, in-depth analyses of cells with different residual capacities are performed. Sudden degradation of capacity is found to be triggered by the appearance of lithium plating confined to small characteristic areas, generated by heterogeneous compression. The resulting lithium loss rapidly alters the balancing of the electrodes, thus generating a self-amplifying circle of active material and lithium loss. Changes in impedance and open-circuit voltage are explained by the expansion of degraded patches. Destructive analysis reveals that sudden degradation is caused by the graphite electrode while the positive electrode is found unchanged except for delithiation caused by side reactions on the negative electrode. Our findings illustrate the importance of homogeneous compression of the electrode assembly and carbon electrode formulation. Finally, a quick test to evaluate the vulnerability of cell designs toward sudden degradation is proposed."
Nonlinear aspects of “breathing” crack-disturbed plate waves: 3-D analytical modeling with experimental validation,"Previously, a two-dimensional (2-D) analytical model for interpreting the modulation mechanism of a “breathing” crack on guided ultrasonic waves (GUWs) is developed [1]. Based on the theory of wave propagation in three-dimensional (3-D) waveguides and using an elastodynamic analysis, the 2-D model is extended to a 3-D regime, to shed light on the nonlinear aspects of GUWs disturbed by cracks with “breathing” traits. With the model, generation of contact acoustic nonlinearity (CAN) embodied in GUWs, subjected to the key parameters of a “breathing” crack (e.g., crack length), is scrutinized quantitatively. On this basis, a nonlinearity index is defined to link crack parameters to the quantity of extracted CAN. In virtue of the index, initiation of an undersized fatigue crack in a 3-D waveguide can be delineated at its embryonic stage, and, in particular, the crack severity can be quantitatively depicted. This facilitates prognosis of imminent failure of the monitored structure. Experimental validation is performed in which a hairline fatigue crack is evaluated, and the results well corroborate the crack parameters predicted by the 3-D analytical model."
Nonlinear vibration signatures for localized fault of rolling element bearing in rotor-bearing-casing system,"Fault signatures in the vibration signal are an important basis for diagnosis of localized defects in a rolling element bearing. Normally the defect-induced impulses will result in harmonics of the fault characteristic frequency and sidebands in the demodulated spectrum. But there will be obvious difference for fault signatures of different bearings with different surroundings, especially the shape of the modulation sidebands. This paper gives an explanation on the causes and influencing factors of bearing fault signatures using an impulse sequence model and a nonlinear multi-body dynamic bearing model in the rotor-bearing-casing system. The frequency components of the load force function on the defect will affect the amplitude of the impulses due to contact loss, becoming the modulation sidebands of the fault characteristic frequency. The extend of the load distribution on rolling elements are controlled by the initial bearing clearance, while the load force function on the defect is given by substituting the estimation of the load force on the bearing and the motion of the defect into the load distribution function. The influence of the defect position and random fluctuations are also analyzed in the simulation, as well as multiple localized defects of the same or different types. The numerical and experimental results show that this modeling and analysis method could be applied to explain and estimate fault signatures of localized bearing faults in a typical rotor-bearing-casing system."
Non-obvious correlations to disease management unraveled by Bayesian artificial intelligence analyses of CMS data,"Objective Given the availability of extensive digitized healthcare data from medical records, claims and prescription information, it is now possible to use hypothesis-free, data-driven approaches to mine medical databases for novel insight. The goal of this analysis was to demonstrate the use of artificial intelligence based methods such as Bayesian networks to open up opportunities for creation of new knowledge in management of chronic conditions. Materials and methods Hospital level Medicare claims data containing discharge numbers for most common diagnoses were analyzed in a hypothesis-free manner using Bayesian networks learning methodology. Results While many interactions identified between discharge rates of diagnoses using this data set are supported by current medical knowledge, a novel interaction linking asthma and renal failure was discovered. This interaction is non-obvious and had not been looked at by the research and clinical communities in epidemiological or clinical data. A plausible pharmacological explanation of this link is proposed together with a verification of the risk significance by conventional statistical analysis. Conclusion Potential clinical and molecular pathways defining the relationship between commonly used asthma medications and renal disease are discussed. The study underscores the need for further epidemiological research to validate this novel hypothesis. Validation will lead to advancement in clinical treatment of asthma & bronchitis, thereby, improving patient outcomes and leading to long term cost savings. In summary, this study demonstrates that application of advanced artificial intelligence methods in healthcare has the potential to enhance the quality of care by discovering non-obvious, clinically relevant relationships and enabling timely care intervention."
Numerical analysis of structural damage in the church of the Poblet Monastery,"Identifying the possible causes of existing damage in historical structures is a complex task. Common difficulties are the limited information regarding the history of the construction, the properties of materials and the morphology of structural members. This paper investigates the capability of advanced numerical modelling to address the aforementioned issues and to simulate the damage in complex masonry structures. The case under study is the church of the Poblet Monastery, one of the UNESCO World Heritage Sites in Spain. The developed finite element model includes the actual-deformed 3D geometry of a selected bay, aiming to consider the important deformation of the structure during the analysis. Based on the historical survey, the analysis considers different hypotheses to investigate on the possible causes of existing damage: gravitational loading, settlements, past earthquakes and reported structural alterations. The influence of the material parameters on the structural safety is assessed with parametric analysis. Additionally, the parallel use of graphic statics with numerical analysis helps to interpret realistically the structure’s current equilibrium. The applied numerical strategy is effective to understand the causes of the present damage in the structure. The results of the study are useful to make a first diagnosis and are the prelude for future inspection, testing and monitoring activities."
Numerical computation of blood hemodynamic through constricted human left coronary artery: Pulsatile simulations,"Background and objective The accumulation of plaque in the coronary artery of the human heart restricts the path of blood flow in that region and leads to Coronary Artery Disease. This study's goal is to present the pulsatile blood flow conduct through four different levels of constrictions, i.e., healthy, 25%, 50%, and 75% in human left coronary arteries. Methods Using CT scan data of a healthy person, the two-dimensional coronary model is constructed. A non-Newtonian Carreau model is used to study the maximum flow velocity, streamline effect, and maximum Wall Shear Stress at the respective constricted areas over the entire cardiac cycle. Finite Volume Method is executed for solving the governing equations. The fluctuating Wall Shear Stress (WSS) at different levels was assessed using Computational Fluid Dynamics (CFD). Results The comparative study of the diseased arteries showcases that at the systolic phase, the 75% blocked artery attains the maximum velocity of 0.14 m/s and 0.53 m/s at t=0.005 s and t=0.115 s, respectively. While the maximum velocity takes a significant drop at t=0.23 s and t=0.345 s, this marks the diastolic phase. The streamline contour showcased the blood flow conduct at different phases of the cardiac cycle. At the peak systolic phase, a dense flow separation was observed near the blocked regions. It highlights the disturbed flow in that particular region. The most severely diseased artery acquires the maximum WSS of 18.81 Pa at the peak systolic phase, i.e., at t=0.115 s. Conclusions The computational study of the hemodynamic parameters can aid in the early anticipation of the degree of the severity of the diseased arteries. This study, in a way, could benefit doctors/surgeons to plan an early treatment/surgery on the grounds of the severity of the disease. Thus, a before time prognosis could restrain the number of deaths caused due to Coronary Artery Disease."
Object-oriented regression for building predictive models with high dimensional omics data from translational studies,"Maturing omics technologies enable researchers to generate high dimension omics data (HDOD) routinely in translational clinical studies. In the field of oncology, The Cancer Genome Atlas (TCGA) provided funding support to researchers to generate different types of omics data on a common set of biospecimens with accompanying clinical data and has made the data available for the research community to mine. One important application, and the focus of this manuscript, is to build predictive models for prognostic outcomes based on HDOD. To complement prevailing regression-based approaches, we propose to use an object-oriented regression (OOR) methodology to identify exemplars specified by HDOD patterns and to assess their associations with prognostic outcome. Through computing patient’s similarities to these exemplars, the OOR-based predictive model produces a risk estimate using a patient’s HDOD. The primary advantages of OOR are twofold: reducing the penalty of high dimensionality and retaining the interpretability to clinical practitioners. To illustrate its utility, we apply OOR to gene expression data from non-small cell lung cancer patients in TCGA and build a predictive model for prognostic survivorship among stage I patients, i.e., we stratify these patients by their prognostic survival risks beyond histological classifications. Identification of these high-risk patients helps oncologists to develop effective treatment protocols and post-treatment disease management plans. Using the TCGA data, the total sample is divided into training and validation data sets. After building up a predictive model in the training set, we compute risk scores from the predictive model, and validate associations of risk scores with prognostic outcome in the validation data (P-value=0.015)."
OctNET: A Lightweight CNN for Retinal Disease Classification from Optical Coherence Tomography Images,"Background and Objective Retinal diseases are becoming a major health problem in recent years. Their early detection and ensuing treatment are essential to prevent visual damage, as the number of people affected by diabetes is expected to grow exponentially. Retinal diseases progress slowly, without any discernible symptoms. Optical Coherence Tomography (OCT) is a diagnostic tool capable of analyzing and identifying the quantitative discrimination in the disease affected retinal layers with high resolution. This paper proposes a deep neural network-based classifier for the computer-aided classification of Diabetic Macular Edema (DME), drusen, Choroidal NeoVascularization (CNV) from normal OCT images of the retina. Methods In the proposed method, we demonstrate the feasibility of classifying and detecting severe retinal pathologies from OCT images using a deep convolutional neural network having six convolutional blocks. The classification results are explained using a gradient-based class activation mapping algorithm. Results Training and validation of the model are performed on a public dataset of 83,484 images with expert-level disease grading of CNV, DME, and drusen, in addition to normal retinal image. We achieved a precision of 99.69%, recall of 99.69%, and accuracy of 99.69% with only three misclassifications out of 968 test cases. Conclusion In the proposed work, downsampling and weight sharing were introduced to improve the training efficiency and were found to reduce the trainable parameters significantly. The class activation mapping was also performed, and the output image was similar to the retina’s actual color OCT images. The proposed network used only 6.9% of learnable parameters compared to the existing ResNet-50 model and yet outperformed it in classification. The proposed work can be potentially employed in real-time applications due to reduced complexity and fewer learnable parameters over other models."
Offline computer-aided diagnosis for Glaucoma detection using fundus images targeted at mobile devices,"Background and Objective: Glaucoma, an eye condition that leads to permanent blindness, is typically asymptomatic and therefore difficult to be diagnosed in time. However, if diagnosed in time, Glaucoma can effectively be slowed down by using adequate treatment; hence, an early diagnosis is of utmost importance. Nonetheless, the conventional approaches to diagnose Glaucoma adopt expensive and bulky equipment that requires qualified experts, making it difficult, costly and time-consuming to diagnose large amounts of people. Consequently, new alternatives to diagnose Glaucoma that suppress these issues should be explored. Methods: This work proposes an interpretable computer-aided diagnosis (CAD) pipeline that is capable of diagnosing Glaucoma using fundus images and run offline in mobile devices. Several public datasets of fundus images were merged and used to build Convolutional Neural Networks (CNNs) that perform segmentation and classification tasks. These networks are then used to build a pipeline for Glaucoma assessment that outputs a Glaucoma confidence level and also provides several morphological features and segmentations of relevant structures, resulting in an interpretable Glaucoma diagnosis. To assess the performance of this method in a restricted environment, this pipeline was integrated into a mobile application and time and space complexities were assessed. Results: Considering the test set, the developed pipeline achieved 0.91 and 0.75 of Intersection over Union (IoU) in the optic disc and optic cup segmentation, respectively. With regards to the classification, an accuracy of 0.87 with a sensitivity of 0.85 and an AUC of 0.93 were attained. Moreover, this pipeline runs on an average Android smartphone in under two seconds. Conclusions: The results demonstrate the potential that this method can have in the contribution to an early Glaucoma diagnosis. The proposed approach achieved similar or slightly better metrics than the current CAD systems for Glaucoma assessment while running on more restricted devices. This pipeline can, therefore, be used to construct accurate and affordable CAD systems that could enable large Glaucoma screenings, contributing to an earlier diagnose of this condition."
On the influence of inhomogeneous stiffness and growth on mechanical instabilities in the developing brain,"The characteristic surface morphology of the mammalian brain is closely correlated with brain function and dysfunction. During development, the initially smooth surface evolves into an elaborately convoluted pattern. Growing evidence suggests that mechanical instabilities emerging from differential growth between a faster growing outer gray matter and a slower growing inner white matter play a major role in brain morphogenesis. Previous studies assume uniform growth and stiffness; yet, recent experiments indicate that the properties of brain tissue are highly inhomogeneous. Here, we hypothesize that regionally varying developmental pathways across the brain result in nonuniform material properties at the onset of cortical folding. We establish a computational model of brain growth to explore the effects of stiffness and growth variations in gray and white matter tissue to mimic cellular processes and evolving tissue microstructure. We present an effective approach to determine critical growth values from geometrical data and systematically study the effect of inhomogeneous material properties on growth-induced primary and secondary instabilities. Our results reveal that critical growth and wavelength strongly depend on the stiffness distribution in the developing brain. Regional variations in cortical growth affect secondary instabilities and evoke highly irregular folding patterns, but characteristic wavelength and critical growth remain relatively stable. The interplay of different influential factors including cortical thickness, brain geometry, stiffness, and growth explains how primary folds are highly preserved across individuals, whereas secondary and tertiary folds vary significantly. Our findings are directly applicable to imaging data of fetal brains and ultimately enable early diagnostics of cortical malformations to improve treatment of neurodevelopmental disorders including epilepsy, autism, spectrum disorders, and schizophrenia."
On the role of buoyancy-driven instabilities in horizontal liquid–liquid flow,"Horizontal flows of two initially stratified immiscible liquids with matched refractive indices, namely an aliphatic hydrocarbon oil (Exxsol D80) and an aqueous-glycerol solution, are investigated by combining two laser-based optical-diagnostic measurement techniques. Specifically, high-speed Planar Laser-Induced Fluorescence (PLIF) is used to provide spatiotemporally resolved phase information, while high-speed Particle Image and Tracking Velocimetry (PIV/PVT) are used to provide information on the velocity field in both phases. The two techniques are applied simultaneously in a vertical plane through the centreline of the investigated pipe flow, illuminated by a single laser-sheet in a time-resolved manner (at a frequency of 1–2kHz depending on the flow condition). Optical distortions due to the curvature of the (transparent) circular tube test-section are corrected with the use of a graticule (target). The test section where the optical-diagnostic methods are applied is located 244 pipe-diameters downstream of the inlet section, in order to ensure a significant development length. The experimental campaign is explicitly designed to study the long-length development of immiscible liquid–liquid flows by introducing the heavier (aqueous) phase at the top of the channel and above the lighter (oil) phase that is introduced at the bottom, which corresponds to an unstably-stratified “inverted” inlet orientation in the opposite orientation to that in which the phases would naturally separate. The main focus is to evaluate the role of the subsequent interfacial instabilities on the resulting long-length flow patterns and characteristics, also by direct comparison to an existing liquid–liquid flow dataset generated in previous work, downstream of a “normal” inlet orientation in which the oil phase was introduced over the aqueous phase in a conventional stably-stratified inlet orientation. To the best knowledge of the authors this is the first time that detailed spatiotemporally resolved phase and velocity data have been generated by advanced measurement techniques in such experiments, specifically devoted to the study of long-length liquid–liquid flow development. In particular, the change in the inlet orientation imposes a Rayleigh–Taylor instability at the inlet. The effects of this instability are shown to persist along the tube, increasing the propensity for oil droplets to appear below the interface. Generally, the characteristics of the flows generated with the two inlet orientations are found to be comparable, although only six flow regimes are identified here, as opposed to eight for the original “normal” inlet orientation. The unobserved regimes are: (1) three-layer flow, and (2) aqueous-solution dispersion with an aqueous solution film. Furthermore, similar mean axial-velocity profiles are observed in the current study to those reported for the corresponding “normal” inlet orientation liquid–liquid flows. These findings are important to consider when interpreting published data from experiments performed in laboratory environments and attempting to draw conclusions relating to applications in the field. The generated data promote not only a qualitative, but importantly and uniquely, a quantitate understanding of the role of multiple instabilities on the development of these complex interfacial flows, with detailed insight into how the deviations manifest at distances 244 pipe-diameters downstream of the inlet (from high level information such as regime maps, to detailed flow information such as phase and velocity profiles). The data can be used directly for the development and validation of advanced multiphase flow models that require such detailed information."
On the useful life of tribo-pairs experiencing variable loading and sliding speed,"Tribo-pairs experiencing fluctuating operating conditions are expected to have lower useful life compared to those that operate in a steady state condition. The objective of the present study is to gain insight into the wear characteristics of such tribo-pairs operating in dry conditions and to develop a general procedure to reliably predict their useful life. Results are presented based on a series of experiments performed using a ball-on-disk tribometer. The disk is made of brass coated with a black paint to clearly detect the onset of failure defined as the time when the steel ball contacts the brass disk and friction force rapidly fluctuates and becomes erratic. First, experiments are performed at various loads (1 N, 2 N and 3 N) at a constant sliding speed (0.1 m/s), and different sliding speeds (0.05 m/s, 0.1 m/s and 0.2 m/s) at a constant load of 2 N) for establishing the useful life. Following these experiments, fluctuating operating conditions are imposed by varying (i) loading sequence, (ii) sliding speed sequence and (iii) both load and sliding speed in an arbitrary manner. For all these experiments the remaining useful life for the last sequence are determined by considering Miner's constant C = 1 and cumulative power to failure of 103 W. The results revealed that the maximum error between the power generation method and experimental results vary from 2% to 10.3%, while using Miner's constant, the error was substantially higher. Further, the applicability of power generation method is validated for an extreme operating condition for an Al-steel tribo-pair. Finally, a detailed mathematical explanation is provided to illustrate the reason for the advantage of power generation method over Miner's rule."
One-step synthesis of 2D C3N4-tin oxide gas sensors for enhanced acetone vapor detection,"This work proposed a one-step method to fabricate the two-dimensional (2D) heterostructural C3N4-tin oxide (SnO2) nanocomposites with excellent acetone vapor sensing performance. A facile calcination treatment of melamine and SnCl2·2H2O without further processing can obtain the expected C3N4-SnO2 sensor. Specially, the SnO2 nanoparticles directly anchor onto C3N4 layer to construct a heterostructure, giving enhanced sensing properties. Compared with pure SnO2 the heterostructural C3N4-SnO2 Exhibits 22 times enhancement of sensing sensitivity as well as fast response/recovery (7s/8s). The limit of detection (LOD) of acetone can be as low as 67ppb, which is far below the concentration in exhaled breath of a diabetic and predicts a possible for diagnosis of diabetes. Such enhancement can be interpreted as the transformation of electrons from SnO2 to C3N4 layer to form an asymmetric electronic structure in the electron depletion layer of SnO2, for which few electrons can change its resistance significantly. Moreover, the large surface area of C3N4 layer provide vast adsorption sites for target gases. Importantly, SnO2 and C3N4 are low-cost, easy fabrication and eco-friendly materials, and the synthesis strategy presented here is simple, repeatable and operable, thus can be extended to build other-type metal oxides-based nanocomposites for various applications."
Online detection of abnormal passenger out-flow in urban metro system,"Accurate forecasting of the number of passengers coming into the urban metro system, namely, passenger in-flow has been one of the most important issues in the research on the intelligent transportation system. Existing studies aim at making the prediction mainly by learning the recurring patterns from historical passenger in-flows. However, passenger out-flows from the urban metro system, which may usually contribute to understanding passenger behavior, and help forecast passenger in-flows, especially burst in-flows on unusual conditions, have rarely been studied yet. In this paper, we propose a model called EAD-OF (Elastic Abnormality Detection for Out-flow) to real-time detect the abnormal passenger out-flows and alarm the administrators in time. Furthermore, we analyze the characteristics of passenger in-flow, out-flow and their correlation. We evaluate our model with real transportation data of the biggest city in China and use large-scale social event data to further explain our model. The result shows that EAD-OF can effectively detect the abnormal out-flows which would cause burst in-flows with performance tens of times better than the baseline models. In term of efficiency, EAD-OF can also beat the direct computation approach several times."
"Optical and laser diagnostic investigation of flame stabilization in a novel, ultra-lean, non-premixed model GT burner","The flame dynamics and stabilization mechanism in a novel, ultra-lean, non-premixed, model GT burner is experimentally investigated with optical and laser diagnostics. The burner operating with methane as fuel exhibits a convergent-divergent flow field and is capable of stabilizing a non-sooty flame at ultra-low global equivalence ratio (ϕglob) down to 0.1. At 1.0 > ϕglob > 0.6, the flame stabilizes in the diverging section where the flow field is characterized predominantly by the swirl. The flame stabilizes at locations of low velocity and low turbulence where the mean flow strain rates are found to be well below the extinctions strain rates for turbulent non-premixed flames. With decreasing ϕglob, the flame is located progressively near the burner lip, where a large recirculation zone of the central bluff body in the burner exists. The transition from swirl stabilized to bluffbody stabilized region occurs between 0.6 > ϕglob > 0.4 and bi-stability of the flame with low-frequency oscillation between the two stabilization locations was observed. The upstream marching of the flame, despite a decreasing ϕglob, and a corresponding decrease in heat input, is explained in terms of the interaction of the flame front with the evolution of the scalar profile upstream, in presence of the recirculating hot gases. The results support flame stabilization theories based on partial premixing and flow modification through heat release upstream of the flame. For the leaner conditions, 0.4 > ϕglob > 0.1, the flame is stabilizing in a region characterized by bluffbody induced toroidal flow structures of high strain rates, where the presence of recirculating hot burned gases aid in flame stabilization by the ignition of flammable mixture."
Optimised Spectral Kurtosis for bearing diagnostics under electromagnetic interference,"The selection of the optimal demodulation frequency band is a significant step in bearing fault diagnosis because it determines whether the fault information can be extracted from the demodulated signal via envelope analysis. Two well-known methods for selecting the demodulation band are the Fast Kurtogram, based on the kurtosis of the filtered time signal, and the Protrugram, which uses the kurtosis of the envelope (amplitude) spectrum. Although these two methods have been successfully applied in many cases, the authors have observed that they may fail in specific environments, such as in the presence of electromagnetic interference (EMI) or other impulsive masking signals. In this paper, a simple spectral kurtosis-based approach is proposed for selecting the best demodulation band to extract bearing fault-related impulsive content from vibration signals contaminated with strong EMI. The method is applied to vibration signals obtained from a planetary gearbox test rig with planet bearings seeded with inner and outer race faults. Results from the Fast Kurtogram and Protrugram methods are also included for comparison. The proposed approach is found to exhibit superior diagnostic performance in the presence of intense EMI. Another contribution of the paper is to introduce and explain the issue of EMI to the condition monitoring community. The paper outlines the characteristics of EMI arising from widely-used variable frequency drives, and these characteristics are used to simulate an EMI-contaminated vibration signal to further test the performance of the proposed approach. Although EMI has been acknowledged as a serious problem in many industrial cases, there have been very few studies showing its adverse effects on machine diagnostics. It is important for analysts to be able to identify EMI in measured vibration signals, lest it interfere with the analysis undertaken."
Overview and implementation of dynamic thermoeconomic & diagnosis analyses in HVAC&R systems,"The purpose of this paper is to apply thermoeconomics in a HVAC&R facility in order to implement the dynamic behaviour as well as to solve the direct problem of diagnosis. New methodological aspects of thermoeconomic applications are developed, such as a new method for setting the productive dynamic structure, in particular to define the commonly used three-way valves and the inertial components. Besides, a novel easy-to-use methodology is explained to filter the induced effects; and, consequently, a new way to solve the direct problem of the diagnosis is obtained. Some crucial aspects of Second Law applications are discussed and all of this is applied to a real HVAC&R system using data from an experimental test in which a two-fault operation condition was deliberately assigned. The results demonstrate that this new thermoeconomic methodology can be applied to systems that behave dynamically to obtain extra information in costs distribution and in the evaluation of the effects of anomalies; and the method is particularly relevant when the dynamic behaviour of the components needs to be contemplated. In summary, the novelty of this research is to go a step forward in thermoeconomic applications, developing a new methodology to be implemented in dynamic systems for fault detection and diagnosis, using a critical point of view."
PadChest: A large chest x-ray image dataset with multi-label annotated reports,"We present a labeled large-scale, high resolution chest x-ray dataset for the automated exploration of medical images along with their associated reports. This dataset includes more than 160,000 images obtained from 67,000 patients that were interpreted and reported by radiologists at San Juan Hospital (Spain) from 2009 to 2017, covering six different position views and additional information on image acquisition and patient demography. The reports were labeled with 174 different radiographic findings, 19 differential diagnoses and 104 anatomic locations organized as a hierarchical taxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. Of these reports, 27% were manually annotated by trained physicians and the remaining set was labeled using a supervised method based on a recurrent neural network with attention mechanisms. The labels generated were then validated in an independent test set achieving a 0.93 Micro-F1 score. To the best of our knowledge, this is one of the largest public chest x-ray databases suitable for training supervised models concerning radiographs, and the first to contain radiographic reports in Spanish. The PadChest dataset can be downloaded from http://bimcv.cipf.es/bimcv-projects/padchest/."
Paper-based multiplex analytical device for simultaneous detection of Clostridioides difficile toxins and glutamate dehydrogenase,"We report a new paper-based multiplex analytical device (mPAD) for simultaneous screening of three analytes (glutamate dehydrogenase, toxin A, and toxin B) known as biomarkers for Clostridioides difficile infection (CDI). To overcome the limitation of common rapid assays (e.g. lateral flow immunochromatographic and enzyme immunoassays) in terms of multiplexing, sensitivity, simplicity, and ease-of-use, the mPAD is constructed with a three dimensional (3D) configuration of paper components with a multi-channel design. Multiple fluidic paths developed with wax-patterned paper allow the simultaneous detection of glutamate dehydrogenase, toxin A, and toxin B without any cross-reactivity. The 3D fluidic network on the mPAD facilitates a self-operating test procedure for the mixing and addition of amplification reagents with a one-step sliding operation. The results of the multiplex CDI assay can be easily interpreted by the naked eye within 10 min, and are visually intensified over time resulting in up to 3-fold signal amplification. Our device exhibited remarkable analytical performances for the simultaneous detection of three CDI biomarkers, providing a sensitivity of 97%, specificity of 88%, accuracy of 95%, and limits of detection for glutamate dehydrogenase, toxin A, and toxin B of 0.16 ng mL−1, 0.09 ng mL−1, and 0.03 ng mL−1, respectively. These results indicate the high applicability and feasibility of mPAD for multiplex testing for CDI with the advantages of being simple, sensitive, inexpensive, user-friendly, and equipment-free."
Parallel deep solutions for image retrieval from imbalanced medical imaging archives,"Learning and extracting representative features along with similarity measurements in high dimensional feature spaces is a critical task. Moreover, the problem of how to bridge the semantic gap, between the low-level information captured by a machine learning model and the high-level one interpreted by a human operator, is still a practical challenge, especially in medicine. In medical applications, retrieving similar images from archives of past cases can be immensely beneficial in diagnostic imaging. However, large and balanced datasets may not be available for many reasons. Exploring the ways of using deep networks, for classification to retrieval, to fill this semantic gap was a key question for this research. In this work, we propose a parallel deep solution approach based on convolutional neural networks followed by a local search using LBP, HOG and Radon features. The IRMA dataset, from ImageCLEF initiative, containing 14,400 X-ray images, is employed to validate the proposed scheme. With a total IRMA error of 165.55, the performance of our scheme surpasses the dictionary approach and many other learning methods applied on the same dataset."
Patient representation learning and interpretable evaluation using clinical notes,"We have three contributions in this work: 1. We explore the utility of a stacked denoising autoencoder and a paragraph vector model to learn task-independent dense patient representations directly from clinical notes. To analyze if these representations are transferable across tasks, we evaluate them in multiple supervised setups to predict patient mortality, primary diagnostic and procedural category, and gender. We compare their performance with sparse representations obtained from a bag-of-words model. We observe that the learned generalized representations significantly outperform the sparse representations when we have few positive instances to learn from, and there is an absence of strong lexical features. 2. We compare the model performance of the feature set constructed from a bag of words to that obtained from medical concepts. In the latter case, concepts represent problems, treatments, and tests. We find that concept identification does not improve the classification performance. 3. We propose novel techniques to facilitate model interpretability. To understand and interpret the representations, we explore the best encoded features within the patient representations obtained from the autoencoder model. Further, we calculate feature sensitivity across two networks to identify the most significant input features for different classification tasks when we use these pretrained representations as the supervised input. We successfully extract the most influential features for the pipeline using this technique."
Performance Enhancement and Analysis of Filters in Ultrasound Image Denoising,"Image denoising could be a major concern within the field of medical imaging. An ultrasound is also referred to as ultrasonography, which is a device that is used to create images of the inner body by using high frequency sound waves. Ultrasound is an imaging technique which is noninvasive, nonradioactive and also inexpensive and is used for diagnosis and treatment. Presence of noise in ultrasound images is a major issue as it can lead to improper diagnosis of the disease. It is very important to restore the quality of the images so as to gain information from the medical images. It is important to preserve the features of the image by reducing the noise levels. This work explains about the various kinds of noises present within the ultrasound medical images and also the filters that are used for the noise removal purpose. The noises were introduced in the ultrasound images are Salt and Pepper Noise (impulse or spike noise), Poisson noise (shot noise), Gaussian or amplifier noise and Speckle Noise. For the noise reduction from ultrasound images, a study is carried out by using Gaussian filter, bilateral filter, Order statistic filter, Mean filter and Laplacian filter for efficient noise reduction from the images. The results we obtained after performing the experiments shows the performance and comparisons of different filters supported their PSNR (Peak signal to noise ratio), MSE (Mean Square Error), and RMSE (Root Mean Square Error) values. This paper explains about the Image Noises and Filtering techniques used in effective removal of the Image Noises and performance of different Filters is analyzed on the basis of the PSNR, MSE and RMSE values."
Performance Enhancement of Cylindrical Grinding Process with a Portable Diagnostic System,"Material removal processes are one among several manufacturing processes that necessitates the enhancement of their precision capability to cater the demanding needs of higher technological innovations. The performance of a material removal process depends on several factors like the precision of the machine tool, process parameters, process consumables and to a certain extent on the skill of the operator. This paper presents an approach to develop a diagnostic system that can enhance the performance of cylindrical grinding process by monitoring vital process signals like grinding power and infeed of axis. The developed diagnostic system comprises of a powercell and LVDT enabling the measurement of power drawn by wheel spindle along with the wheel infeed movement. Using the measured signals, the developed system is used to optimize the grinding cycle parameters in order to enhance the efficiency of the process. The effectiveness of the developed in-process portable diagnostic system is demonstrated with two case studies. The effect of dressing on the performance of the grinding process is explained in one of the case studies using the diagnostic tool. In another industrial case study, the application of diagnostic tool for selecting the grinding cycle and to determine the frequency of dressing is explained."
Performance of SiO2-water nanofluids for single bubble-based nucleate pool boiling heat transfer,"Experiments on single bubble-based pool boiling have been performed under saturated bulk conditions to understand the influence of suspended nanoparticles on bubble dynamics and the associated temperature gradients field. Water and two concentrations of silica-water nanofluid (0.005 and 0.01% (V/V)) have been subjected to isolated nucleate pool boiling on a ITO-coated borofloat heater by supplying constant heat flux. Experimental measurements have been made in a purely non-intrusive manner by employing refractive index-based rainbow schlieren deflectometry technique. Bubble dynamic parameters, such as bubble departure diameter and departure frequency, wait and growth times etc. and the temperature gradients in the bulk fluid have been mapped simultaneously in the form of schlieren images. Results on bubble dynamic parameters revealed that these parameters show more variability and assume skewed normal distribution in the case of nanofluids whereas water experiments showed insignificant variations in these parameters. Of the significant changes, the bubble departure diameter reduced, departure frequency and growth rate increased with increasing concentration of nanofluids. Schlieren images corresponding to 0.005 and 0.01% (V/V) nanofluids showed more spreading out of the superheat layer adjacent to the heater substrate than that observed in the case of water experiments. Time-averaged values of natural convection and evaporative heat transfer rates associated with single bubble-based boiling indicated that both these rates decrease in the presence of dispersed silica nanoparticles. Reduction in the overall heat transfer rate in the case of nanofluids has been explained on the basis of direct experimental measurements. In this direction, the observed phenomenon has been attributed to the role played by the suspended nanoparticles in diffusing the thermal gradients (reflected in the form of the broadening of the superheat layer) adjacent to the heater substrate along with substantially increasing the wait time of bubble inception in the case of nanofluids as compared to water-based experiments conducted under same wall superheat conditions."
Performance predictions of ground source heat pump system based on random forest and back propagation neural network models,"With rapid development of artificial intelligence, data-driven prediction models play an important role in energy prediction, fault detection, and diagnosis. This paper proposes an ensemble approach using random forest (RF) for hourly performance predictions of GSHP system. Two years of in situ data were collected in an educational building situated in severe cold area in China. Prediction models were established for performance indicators, and results indicate that the average error for COPs, COPu, EERs and EERu were all controlled within 5%. The model established by small amount of data can accurately predict long-term performance, thereby reducing time and difficulty of data collection. RF models, trained with different parameter settings were compared, results indicate that model accuracy was not very sensitive to variables numbers. The impact of input variables on prediction performance was analyzed, and importance ranking changed with period and performance indicators. By comparing the variable importance list, it was possible to establish which parameters were abnormal and lists of different periods can reflect whether the energy structure of building has changed. The overall superiority of RF was verified by comparing with back propagation neural network (BPNN) from robustness, interpretability, and efficiency. First, since GSHP system involving multiple indicators, the robustness, measured by average accuracy, was used to evaluate the accuracy level. According to CV-RMSE, robustness of RF is approximately 3.3% higher than that of BPNN. Second, RF is highly interpretive but BPNN is typical black box model. Finally, modeling complexity and training time of BPNN were much greater than RF."
Personalized conciliation of clinical guidelines for comorbid patients through multi-agent planning,"The conciliation of multiple single-disease guidelines for comorbid patients entails solving potential clinical interactions, discovering synergies in the diagnosis and the recommendations, and managing clinical equipoise situations. Personalized conciliation of multiple guidelines considering additionally patient preferences brings some further difficulties. Recently, several works have explored distinct techniques to come up with an automated process for the conciliation of clinical guidelines for comorbid patients but very little attention has been put in integrating the patient preferences into this process. In this work, a Multi-Agent Planning (MAP) framework that extends previous work on single-disease temporal Hierarchical Task Networks (HTN) is proposed for the automated conciliation of clinical guidelines with patient-centered preferences. Each agent encapsulates a single-disease Computer Interpretable Guideline (CIG) formalized as an HTN domain and conciliates the decision procedures that encode the clinical recommendations of its CIG with the decision procedures of the other agents’ CIGs. During conciliation, drug-related interactions, scheduling constraints as well as redundant actions and multiple support interactions are solved by an automated planning process. Moreover, the simultaneous application of the patient preferences in multiple diseases may potentially bring about contradictory clinical decisions and more interactions. As a final step, the most adequate personalized treatment plan according to the patient preferences is selected by a Multi-Criteria Decision Making (MCDM) process. The MAP approach is tested on a case study that builds upon a simplified representation of two real clinical guidelines for Diabetes Mellitus and Arterial Hypertension."
"Personalized predictive models for symptomatic COVID-19 patients using basic preconditions: Hospitalizations, mortality, and the need for an ICU or ventilator","Background The rapid global spread of the SARS-CoV-2 virus has provoked a spike in demand for hospital care. Hospital systems across the world have been over-extended, including in Northern Italy, Ecuador, and New York City, and many other systems face similar challenges. As a result, decisions on how to best allocate very limited medical resources and design targeted policies for vulnerable subgroups have come to the forefront. Specifically, under consideration are decisions on who to test, who to admit into hospitals, who to treat in an Intensive Care Unit (ICU), and who to support with a ventilator. Given today’s ability to gather, share, analyze and process data, personalized predictive models based on demographics and information regarding prior conditions can be used to (1) help decision-makers allocate limited resources, when needed, (2) advise individuals how to better protect themselves given their risk profile, (3) differentiate social distancing guidelines based on risk, and (4) prioritize vaccinations once a vaccine becomes available. Objective To develop personalized models that predict the following events: (1) hospitalization, (2) mortality, (3) need for ICU, and (4) need for a ventilator. To predict hospitalization, it is assumed that one has access to a patient’s basic preconditions, which can be easily gathered without the need to be at a hospital and hence serve citizens and policy makers to assess individual risk during a pandemic. For the remaining models, different versions developed include different sets of a patient’s features, with some including information on how the disease is progressing (e.g., diagnosis of pneumonia). Materials and Methods National data from a publicly available repository, updated daily, containing information from approximately 91,000 patients in Mexico were used. The data for each patient include demographics, prior medical conditions, SARS-CoV-2 test results, hospitalization, mortality and whether a patient has developed pneumonia or not. Several classification methods were applied and compared, including robust versions of logistic regression, and support vector machines, as well as random forests and gradient boosted decision trees. Results Interpretable methods (logistic regression and support vector machines) perform just as well as more complex models in terms of accuracy and detection rates, with the additional benefit of elucidating variables on which the predictions are based. Classification accuracies reached 72 %, 79 %, 89 %, and 90 % for predicting hospitalization, mortality, need for ICU and need for a ventilator, respectively. The analysis reveals the most important preconditions for making the predictions. For the four models derived, these are: (1) for hospitalization:age, pregnancy, diabetes, gender, chronic renal insufficiency, and immunosuppression; (2) for mortality: age, immunosuppression, chronic renal insufficiency, obesity and diabetes; (3) for ICU need: development of pneumonia (if available), age, obesity, diabetes and hypertension; and (4) for ventilator need: ICU and pneumonia (if available), age, obesity, and hypertension."
Perylenetetracarboxylic acid and carbon quantum dots assembled synergistic electrochemiluminescence nanomaterial for ultra-sensitive carcinoembryonic antigen detection,"It is important to design a nice electrochemiluminescence (ECL) biological nanomaterial for fabricating sensitive ECL immunosensor to detect tumor markers. Most reported ECL nanomaterial was decorated by a number of mono-luminophore. Here, we report a novel ECL nanomaterial assembled by dual luminophores perylenetetracarboxylic acid (PTCA) and carbon quantum dots (CQDs). In the ECL nanomaterial, graphene was chosen as nanocarrier. Significant ECL intensity increases are seen in the ECL nanomaterial, which was interpreted with the proposed synergistic promotion ECL meachanism of PTCA and CQDs. Furthermore, this ECL nanomaterial was used to label secondary antibody and fabricate a sandwiched carcinoembryonic antigen (CEA) immunosensor. The CEA immunosensor exhibits high sensitivity and the linear semilogarithmical range was from 0.001fgmL−1 to 1ngmL−1 with low detection limit 0.00026fgmL−1. And the CEA immunosensor is also suitable for various cancers’ sample detection providing potential specific applications in diagnostics."
Phonation threshold pressure at large asymmetries of the vocal folds,"The phonation threshold value of the lung pressure has been interpreted as a measure of ease of phonation and proposed as a diagnostic parameter for vocal health. Therefore, it is important to understand its behavior as a function of laryngeal parameters, particularly in abnormal configurations. This paper compares results from a theoretical model of the vocal folds with measures from a mechanical replica, in the presence of a natural frequency asymmetry. It shows that, at small asymmetry, the threshold pressure increases with the degree of asymmetry, whereas at large asymmetry, the threshold pressure reaches a plateau."
Plantar fascia segmentation and thickness estimation in ultrasound images,"Ultrasound (US) imaging offers significant potential in diagnosis of plantar fascia (PF) injury and monitoring treatment. In particular US imaging has been shown to be reliable in foot and ankle assessment and offers a real-time effective imaging technique that is able to reliably confirm structural changes, such as thickening, and identify changes in the internal echo structure associated with diseased or damaged tissue. Despite the advantages of US imaging, images are difficult to interpret during medical assessment. This is partly due to the size and position of the PF in relation to the adjacent tissues. It is therefore a requirement to devise a system that allows better and easier interpretation of PF ultrasound images during diagnosis. This study proposes an automatic segmentation approach which for the first time extracts ultrasound data to estimate size across three sections of the PF (rearfoot, midfoot and forefoot). This segmentation method uses artificial neural network module (ANN) in order to classify small overlapping patches as belonging or not-belonging to the region of interest (ROI) of the PF tissue. Features ranking and selection techniques were performed as a post-processing step for features extraction to reduce the dimension and number of the extracted features. The trained ANN classifies the image overlapping patches into PF and non-PF tissue, and then it is used to segment the desired PF region. The PF thickness was calculated using two different methods: distance transformation and area-length calculation algorithms. This new approach is capable of accurately segmenting the PF region, differentiating it from surrounding tissues and estimating its thickness."
Postpartum depression prediction through pregnancy data analysis for emotion-aware smart systems,"Emotion-aware computing represents an evolution in machine learning enabling systems and devices process to interpret emotional data to recognize human behavior changes. As emotion-aware smart systems evolve, there is an enormous potential for increasing the use of specialized devices that can anticipate life-threatening conditions facilitating an early response model for health complications. At the same time, applications developed for diagnostic and therapy services can support conditions recognition (as depression, for instance). Hence, this paper proposes an improved algorithm for emotion-aware smart systems, capable for predicting the risk of postpartum depression in women suffering from hypertensive disorders during pregnancy through biomedical and sociodemographic data analysis. Results show that ensemble classifiers represent a leading solution concerning predicting psychological disorders related to pregnancy. Merging novel technologies based on IoT, cloud computing, and big data analytics represent a considerable advance in monitoring complex diseases for emotion-aware computing, such as postpartum depression."
Practical Implementation of a Framework for European Quality Assurance in VET towards Environmentally Sustainable Economy,"This paper presents the eQvet-us practice set composed of an implementation guide and a self diagnostic tool supporting implementation of the new eQvet-us framework that contains 61 indicators based on the 3 pillars of sustainable development: social, economic and environmental. The implementation guide is a series of 13 implementation steps organized in 5 stages, in a cycle arrangement explaining how to use the tools for the implementation of the eQvet-us framework. The self diagnostic tool is a table with questions derived from the indicators of the framework that are answered by evaluators and then evaluated on a scale with 4 levels. A quality indicator is calculated which is rated by means of the quality scale. It provides a general overview about organization's state in per cent, and further evaluation can be done on each indicator group in order to know the strengths and weaknesses of the organization and to develop an improvement plan. The employment of eQvet-us practice set in a VET organization is presented. The results can be used to develop sustainability policies and improve internal sustainability performance."
Practical use of the Stiffness Damage Test (SDT) for assessing damage in concrete infrastructure affected by alkali-silica reaction,"The Stiffness Damage Test (SDT) is an interesting tool for assessing damage in concrete affected by ASR. The use of a fixed loading level for stiffness damage testing, as previously suggested in the literature, limits its ability to reliably appraise the distress level of concrete samples, especially when different concrete strengths and aggregate types are used. Moreover, the test input and output parameters should be properly interpreted so that the SDT becomes a powerful diagnostic procedure. This paper presents an evaluation of the practical use of the SDT in engineering purposes. Results show that the use of output indices, i.e. SDI and PDI, instead of absolute output values, makes the procedure more diagnostic (i.e. ≠ distress degrees can be easier distinguished) and less influenced by practical assumptions (e.g. choice of the loading level for stiffness damage testing). Based on the prior results, a step-by-step standard test procedure is proposed aiming to minimize the test variability."
Predicting dementia with routine care EMR data,"Our aim is to develop a machine learning (ML) model that can predict dementia in a general patient population from multiple health care institutions one year and three years prior to the onset of the disease without any additional monitoring or screening. The purpose of the model is to automate the cost-effective, non-invasive, digital pre-screening of patients at risk for dementia. Towards this purpose, routine care data, which is widely available through Electronic Medical Record (EMR) systems is used as a data source. These data embody a rich knowledge and make related medical applications easy to deploy at scale in a cost-effective manner. Specifically, the model is trained by using structured and unstructured data from three EMR data sets: diagnosis, prescriptions, and medical notes. Each of these three data sets is used to construct an individual model along with a combined model which is derived by using all three data sets. Human-interpretable data processing and ML techniques are selected in order to facilitate adoption of the proposed model by health care providers from multiple institutions. The results show that the combined model is generalizable across multiple institutions and is able to predict dementia within one year of its onset with an accuracy of nearly 80% despite the fact that it was trained using routine care data. Moreover, the analysis of the models identified important predictors for dementia. Some of these predictors (e.g., age and hypertensive disorders) are already confirmed by the literature while others, especially the ones derived from the unstructured medical notes, require further clinical analysis."
Prediction of mortality after radical cystectomy for bladder cancer by machine learning techniques,"Bladder cancer is a common cancer in genitourinary malignancy. For muscle invasive bladder cancer, surgical removal of the bladder, i.e. radical cystectomy, is in general the definitive treatment which, unfortunately, carries significant morbidities and mortalities. Accurate prediction of the mortality of radical cystectomy is therefore needed. Statistical methods have conventionally been used for this purpose, despite the complex interactions of high-dimensional medical data. Machine learning has emerged as a promising technique for handling high-dimensional data, with increasing application in clinical decision support, e.g. cancer prediction and prognosis. Its ability to reveal the hidden nonlinear interactions and interpretable rules between dependent and independent variables is favorable for constructing models of effective generalization performance. In this paper, seven machine learning methods are utilized to predict the 5-year mortality of radical cystectomy, including back-propagation neural network (BPN), radial basis function (RBFN), extreme learning machine (ELM), regularized ELM (RELM), support vector machine (SVM), naive Bayes (NB) classifier and k-nearest neighbour (KNN), on a clinicopathological dataset of 117 patients of the urology unit of a hospital in Hong Kong. The experimental results indicate that RELM achieved the highest average prediction accuracy of 0.8 at a fast learning speed. The research findings demonstrate the potential of applying machine learning techniques to support clinical decision making."
Preliminary design of the HEBT of IFMIF DONES,"IFMIF-DONES (International Fusion Materials Irradiation Facility – DEMO Oriented Neutron Source) is currently being developed in the frame of the EUROfusion Early Neutron Source work package (WPENS) and will be an installation for fusion material testing, that will generate a flux of neutrons of 1018 m−2s−1 with a broad peak at 14 MeV by Li(d,xn) nuclear reactions thanks to a 40 MeV deuteron beam colliding on a liquid Li flow. The accelerator system is in charge of providing such high energy deuterons in order to produce the expected neutron flux. The High Energy Beam Transport line (HEBT) is the last subsystem of the accelerator and its main functions are to guide the deuteron beam towards the Lithium target and to shape it by the use of magnetic elements to the reference beam footprint at the Lithium Target. The present work summarizes the current status of the HEBT design, including beam dynamics, vacuum, radioprotection, diagnostics and remote handling studies performed, along with the layout and integration of the line."
Premature ventricular contraction analysis for real-time patient monitoring,"Background and objective Improvements in wearable sensor devices make it possible to constantly monitor physiological parameters such as electrocardiograph (ECG) signals for long periods. Remotely monitoring patients using wearable sensors has an important role to play in health care, particularly given the prevalence of chronic conditions such as premature ventricular contraction (PVC), one of the prominent causes of death world-wide. PVC is a serious cardiovascular condition that can lead to life-threatening conditions. The instant recognition of life-threatening cardiac arrhythmias based on a wearable ECG sensor for a few seconds is a challenging problem of clinical significance. Method Twenty seconds of consecutive ECG beats that were identified empirically to characterise a PVC episode were analysed. Three morphological features and seven statistical features were directly extracted in real time. These features were normalized and fed into an artificial neural network (ANN) classifier for classification. The PVC detector was uploaded into a smartphone to classify each episode as either PVC or non-PVC. Results The proposed algorithm was tested on the MIT-BIH Arrhythmia, St. Petersburg Institute of Cardiological Technics (INCART) and Shimmer3 ECG databases. The proposed method resulted in an improved sensitivity, positive predictive value and accuracy of 98.7%, 97.8% and 98.6% respectively compared to recently published methods. In addition, the proposed method is suitable for real-time patient monitoring as it is computationally simple and requires only a few seconds of ECG recording to detect a PVC rhythm. Conclusion This study provides a better and more accurate identification of the presence of PVC beats from wearable ECG recordings/mobile environment and standard environment, leading to more timely diagnosis and treatment outcomes."
Probing PAH formation chemical kinetics from benzene and toluene pyrolysis in a single-pulse shock tube,"Benzene and toluene were pyrolyzed under highly argon-diluted conditions at a nominal pressure of 20 bar in a single-pulse shock tube coupled to gas chromatography/gas chromatography–mass spectrometry (GC/GC–MS) diagnostics. Concentration evolutions of polycyclic aromatic hydrocarbon (PAH) intermediates were measured in a temperature range of 1100–1800 K by analyzing the post-shock gas mixtures. Different PAH speciation behaviors, regarding types, concentrations and formation temperature windows, were observed in the two reaction systems. A kinetic model was proposed to predict and interpret the measurements. Through a combination of experimental and modeling efforts, PAH formation patterns from species pools of benzene and toluene pyrolysis were illustrated. In both cases, channels leading to PAHs basically originate from the respective fuel radicals, phenyl and benzyl. Due to the higher thermal stability of benzene, the production of phenyl, and thus most PAH species, occur in higher temperature windows, in comparison to the case of toluene. In benzene pyrolysis, benzyne participates in the formation of crucial PAH species such as naphthalene and acenaphthylene. Phenyl self-recombination takes considerable carbon flux into biphenyl, which serves as an important intermediate leading to acenaphthylene through hydrogen loss and ring closure. The resonantly-stabilized benzyl is abundant in toluene pyrolysis, and its decomposition further produces other resonantly-stabilized radicals such as fulvenallenyl and propargyl. Barrierless addition reactions among these radicals are found to be important sources of PAHs. Fuel-specific pathways have pronounced effects on PAH speciation behaviors, particularly at lower temperatures where fuel depletion is not completed within the reaction time of 4.0 ms. Contributions from the commonly existing Hydrogen-Abstraction-Carbon-Addition (HACA) routes increase with the temperature in both cases."
Process fault diagnosis with model- and knowledge-based approaches: Advances and opportunities,"Fault diagnosis plays a vital role in ensuring safe and efficient operation of modern process plants. Despite the encouraging progress in its research, developing a reliable and interpretable diagnostic system remains a challenge. There is a consensus among many researchers that an appropriate modelling, representation and use of fundamental process knowledge might be the key to addressing this problem. Over the past four decades, different techniques have been proposed for this purpose. They use process knowledge from different sources, in different forms and on different details, and are also named model-based methods in some literature. This paper first briefly introduces the problem of fault detection and diagnosis, its research status and challenges. It then gives a review of widely used model- and knowledge-based diagnostic methods, including their general ideas, properties, and important developments. Afterwards, it summarises studies that evaluate their performance in real processes in process industry, including the process types, scales, considered faults, and performance. Finally, perspectives on challenges and potential opportunities are highlighted for future work."
Progress and plan of KSTAR plasma control system upgrade,"The plasma control system (PCS) has been one of essential systems in annual KSTAR plasma campaigns: starting from a single-process version in 2008, extensive upgrades are done through the previous 7 years in order to achieve major goals of KSTAR performance enhancement. Major implementations are explained in this paper. In consequences of successive upgrades, the present KSTAR PCS is able to achieve ∼48s of 500kA plasma pulses with full real-time shaping controls and real-time NB power controls. It has become a huge system capable of dealing with 8 separate categories of algorithms, 26 actuators directly controllable during the shot, and real-time data communication units consisting of +180 analog channels and +600 digital input/outputs through the reflective memory (RFM) network. The next upgrade of the KSTAR PCS is planned in 2015 before the campaign. An overview of the upgrade layout will be given for this paper. The real-time system box is planned to use the CERN MRG-Realtime OS, an ITER-compatible standard operating system. New hardware is developed for faster real-time streaming system for future installations of actuators/diagnostics."
Proof techniques in Membrane Computing,"From the creation of the field of Membrane Computing in 1998, several research lines have been opened. On the one hand, theoretical questions like the computational power and the computational efficiency of P systems have been studied. In this sense, several techniques to demonstrate the ability of these systems to provide solutions to computational problems have been explored. The study of efficient (polynomial-time) solutions to presumably hard problems for finding thin frontiers of efficiency is a very active area. On the other hand, several applications in biology, ecology, economy, robotics and fault diagnosis, among others, have been investigated. Real systems with some characteristics seem to be easy to model with membrane systems due to their behaviour. In this work, a survey of the theoretical part will be given, explaining techniques both in the field of computability theory and in the field of computational complexity theory."
Proposal for improving discharge quantification in urban drainage,"The concepts of a permanent diagnosis and self-monitoring of sewer systems were introduced twenty years ago in France, and an increasing number of discharge measurement devices are being implemented in sewer networks. Selecting the locations for these devices is often difficult, since the efficiency of available sensors is heavily dependent on hydraulic conditions. In this paper, we propose considering the discharge transiting in a given geometric configuration for given hydraulic conditions as a reference. Three-dimensional modelling of the velocity field allows simulating the response of sensors in order to asses the associated errors and therefore reduce (or optimize?) the number of sensors with numerical calibration. First the paper presents two technologies commonly used for discharge measurements in sewers. Then we explain the three-dimensional modelling. Finally the effect of a bend on the flow discharge measured is presented and discussed."
Propositional logic concept for fault diagnosis in complex systems,"A great number of monitoring technologies have been developed especially for complex systems within the critical zones such as electric power substations, nuclear energy systems. But also, there is no single instruction or standardization in fault-focused on-line/off-line monitoring applications due to the acceleration of technological developments. Field experts have difficulty in choosing which test and measurement systems should be used in which stage of the complex systems. In this study, the propositional logic-based concept is presented, which field experts can use to manage this process. In this concept, test and measurement systems can be grouped according to the priority-order. According to the results of the graded groups on this concept, the suspected fault is verified by the cause of the occurrence. The applicability of the proposed concept has been tried to be explained by creating possible failure scenarios on the transformer. The theoretically validated concept can be used for even more fault situations. This concept can also be used in another complex systems with a large number of T&M systems where very different fault conditions can occur. 2009 Elsevier Ltd. All rights reserved."
Protesting the Singapore government: The role of collective action frames in social media mobilization,"This study engages collective action framing theory to examine the strategies employed by Singaporeans, rarely seen on the front lines of politics, on social media to organize a protest against the government’s immigration policy. It addresses critical and theoretical questions on the formation and dynamics of collective action frames on social media, and the implications for social movements. An analysis of 1805 posts and comments on blogs and Facebook leading up to the May 1, 2013 protest reveals that while organizers and protestors resonated in their development of diagnostic, prognostic and motivational frames, they placed different emphasis on the frames. This variable use of frames helps explain that while the protest was successful in mobilizing thousands to the outdoor site, it was not able to sustain the movement demanding immigration reform."
QCM mass underestimation in molecular biotechnology: Proximity ligation assay for norovirus detection as a case study,"The development of piezoelectric mass-sensitive devices is based on the shift in resonance frequency that is proportional to the deposited mass. However, this holds true only for small, rigid masses, while it can result in mass underestimation for heavy, non-rigid masses. In this work, we demonstrate this ‘missing mass’ phenomenon by measurement of high molecular weight biomolecules on a Quartz Crystal Microbalance (QCM) platform. For this, we present a model bioassay consisting of a sandwich-type proximity ligation assay for the detection of norovirus-like particles, and its real-time build-up on QCM as an experimental evidence. Upon combination with a localized QCM platform, we explain the pronounced slipping effect in multilayer biological systems resulting in energy dissipation and subsequent mass underestimation. This helps in pointing out the limitations of mega-gravity field sensors for molecular diagnostics where absolute quantification of pathogen load becomes indispensable towards biosensing applications."
Quantitative breast mass classification based on the integration of B-mode features and strain features in elastography,"Background Elastography is a new sonographic imaging technique to acquire the strain information of tissues and transform the information into images. Radiologists have to observe the gray-scale distribution of tissues on the elastographic image interpreted as the reciprocal of Young׳s modulus to evaluate the pathological changes such as scirrhous carcinoma. In this study, a computer-aided diagnosis (CAD) system was developed to extract quantitative strain features from elastographic images to reduce operator-dependence and provide an automatic procedure for breast mass classification. Method The collected image database was composed of 45 malignant and 45 benign breast masses. For each case, tumor segmentation was performed on the B-mode image to obtain tumor contour which was then mapped to the elastographic images to define the corresponding tumor area. The gray-scale pixels around tumor area were classified into white, gray, and black by fuzzy c-means clustering to highlight stiff tissues with darker values. Quantitative strain features were then extracted from the black cluster and compared with the B-mode features in the classification of breast masses. Results The performance of the proposed strain features achieved an accuracy of 80% (72/90), a sensitivity of 80% (36/45), a specificity of 80% (36/45), and a normalized area under the receiver operating characteristic curve, Az=0.84. Combining the strain features with the B-mode features obtained a significantly better Az=0.93, p-value<0.05. Conclusions Summarily, the quantified strain features can be combined with the B-mode features to provide a promising suggestion in distinguishing malignant from benign tumors."
Quantitative fault analysis of roller bearings based on a novel matching pursuit method with a new step-impulse dictionary,"A novel matching pursuit method based on a new step-impulse dictionary to measure the size of a bearing׳s spall-like fault is presented in this study. Based on the seemingly double-impact theory and the rolling bearing fault mechanism, a theoretical model for the bearing fault with different spall-like fault sizes is developed and analyzed, and the seemingly double-impact characteristic of the bearing faults is explained. The first action that causes a bearing fault is due to the entry of the roller element into the spall-like fault which can be described as a step-like response. The second action is the exit of the roller element from the spall-like fault, which can be described as an impulse-like response. Based on the quantitative relationship between the time interval of the seemingly double-impact actions and the fault size, a novel matching pursuit method is proposed based on a new step-impulse dictionary. In addition, the quantitative matching pursuit algorithm is proposed for bearing fault diagnosis based on the new dictionary model. Finally, an atomic selection mechanism is proposed to improve the measurement accuracy of bearing fault size. The simulation results of this study indicate that the new matching pursuit method based on the new step-impulse dictionary can be reliably used to measure the sizes of bearing spall-like faults. The applications of this method to the fault signals of bearing outer-races measured at different speeds have shown that the proposed method can effectively measure a bearing׳s spall-like fault size."
Radiation induced absorption of hydrogen-loaded pure silica optical fibers with carbon coating for ITER diagnostics,"We present results of gamma irradiation tests of 200 μm pure silica core optical fibers with a low and high-OH concentration, with and without hydrogen loading. All the fibers were manufactured by a candidate supplier of the fiber bundles for optical plasma diagnostics in ITER. The hydrogen-loaded fibers were stored for a one year before the irradiation start. The fibers were exposed at two Co-60 gamma facilities using dose-rates of 15 and 194 mGy/s up to 15 and 70 kGy absorbed doses, respectively. The optical transmission losses of the fibers were measured in-situ in the spectral range of 450–900 nm. The results are interpreted on the basis of available data on radiation defects in silica core fibers. It is demonstrated that hydrogen loading provides a radiation hardness improvement, which allows to satisfy the ITER requirements."
Random survival forests using linked data to measure illness burden among individuals before or after a cancer diagnosis: Development and internal validation of the SEER-CAHPS illness burden index,"Purpose To develop and internally validate an illness burden index among Medicare beneficiaries before or after a cancer diagnosis. Methods Data source: SEER-CAHPS, linking Surveillance, Epidemiology, and End Results (SEER) cancer registry, Medicare enrollment and claims, and Medicare Consumer Assessment of Healthcare Providers and Systems (Medicare CAHPS) survey data providing self-reported sociodemographic, health, and functional status information. To generate a score for everyone in the dataset, we tabulated 4 groups within each annual subsample (2007–2013): 1) Medicare Advantage (MA) beneficiaries or 2) Medicare fee-for-service (FFS) beneficiaries, surveyed before cancer diagnosis; 3) MA beneficiaries or 4) Medicare FFS beneficiaries surveyed after diagnosis. Random survival forests (RSFs) predicted 12-month all-cause mortality and drew predictor variables (mean per subsample = 44) from 8 domains: sociodemographic, cancer-specific, health status, chronic conditions, healthcare utilization, activity limitations, proxy, and location-based factors. Roughly two-thirds of the sample was held out for algorithm training. Error rates based on the validation (“out-of-bag,” OOB) samples reflected the correctly classified percentage. Illness burden scores represented predicted cumulative mortality hazard. Results The sample included 116,735 Medicare beneficiaries with cancer, of whom 73 % were surveyed after their cancer diagnosis; overall mean mortality rate in the 12 months after survey response was 6%. SEER-CAHPS Illness Burden Index (SCIBI) scores were positively skewed (median range: 0.29 [MA, pre-diagnosis] to 2.85 [FFS, post-diagnosis]; mean range: 2.08 [MA, pre-diagnosis] to 4.88 [MA, post-diagnosis]). The highest decile of the distribution had a 51 % mortality rate (range: 29–71 %); the bottom decile had a 1% mortality rate (range: 0–2 %). The error rate was 20 % overall (range: 9% [among FFS enrollees surveyed after diagnosis] to 36 % [MA enrollees surveyed before diagnosis]). Conclusions This new morbidity measure for Medicare beneficiaries with cancer may be useful to future SEER-CAHPS users who wish to adjust for comorbidity."
RATE-iPATH: On the design of integrated ultrasonic biomarkers for breast cancer detection,"Multi-modal complementary information integrated noninvasive digital biomarkers are emerging with enormous potential for early detection of female breast cancer and reduction of mortality rate among women around the globe. The objective of this study is to design new quantitative ultrasound (QUS) biomarkers integrated from pathological information of tissue microstructure, conventional B-mode imaging-based acoustic and morphological features, and strain imaging-based tissue elasticity features for detecting cancer more effectively. We also develop simulation models to evaluate the estimation accuracy of effective scatterer diameter (ESD), a characteristic parameter of tissue pathology. The proposed biomarkers are designed by optimally integrating 2 QUS indirect pathology (iPATH) parameters with 27 QUS radiology and tissue elasticity (RATE) parameters. To determine effective biomarkers, several techniques are employed: a wrapper-based feature reduction technique in the original feature domain as well as in the empirical mode decomposition (EMD)-discrete wavelet transforms (DWT) domain, and a genetic algorithm-based feature weighting technique. Simulation phantoms determining the accuracy of the ESD estimation algorithm and explaining the increasing trend of ESD with the carcinogenic transformation of normal tissue are created in MATLAB and analyzed using Field II. The resulting average mean absolute percentage error of the ESD estimation algorithm on the simulation phantoms is 4.06%, proving its efficacy. On 139 in-vivo patient data, the three techniques yield accuracy, sensitivity, and specificity within the range of 97.84–99.28%, 97.84–100%, and 95.45–98.95%, respectively. The high values of these metrics indicate that the RATE-iPATH feature set can be used as non-invasive integrated biomarkers for computer-aided diagnosis of breast cancer."
Rational design of sensor for broadband dielectric spectroscopy of biomolecules,"Knowledge of electromagnetic properties of biomolecules is essential for a fundamental understanding of electric field interaction with biosystems and for development of novel biomedical diagnostic and therapeutic methods. To enable systematic analysis of the dielectric properties of biomolecule solutions we presented here a method for a rational design of radiofrequency and microwave chip for quantitative dielectric sensing. At first, we estimated the primary frequency band of interest using a relaxation time of targeted molecule via the Stokes–Einstein–Debye equation. Then we proposed a microwave sensing chip for the estimated frequency band and evaluated its performance using both analytical modeling and numerical electromagnetic simulations. We fabricated the chip and experimentally demonstrated that we can extract the complex permittivity (0.5–40 GHz) of the water solution of alanine – one of the most common proteinogenic amino acids – without any calibration liquid and with about 20-fold smaller volume than with commercial methods. The observed dependence of extracted complex permittivity on the alanine concentration was interpreted using molecular dynamics simulations. The procedure we described here can be applied for the development of dielectric sensing method of any polar biomolecule solution."
Real-time fault detection and diagnosis using sparse principal component analysis,"With the emergence of smart factories, large volumes of process data are collected and stored at high sampling rates for improved energy efficiency, process monitoring and sustainability. The data collected in the course of enterprise-wide operations consists of information from broadly deployed sensors and other control equipment. Interpreting such large volumes of data with limited workforce is becoming an increasingly common challenge. Principal component analysis (PCA) is a widely accepted procedure for summarizing data while minimizing information loss. It does so by finding new variables, the principal components (PCs) that are linear combinations of the original variables in the dataset. However, interpreting PCs obtained from many variables from a large dataset is often challenging, especially in the context of fault detection and diagnosis studies. Sparse principal component analysis (SPCA) is a relatively recent technique proposed for producing PCs with sparse loadings via variance-sparsity trade-off. Using SPCA, some of the loadings on PCs can be restricted to zero. In this paper, we introduce a method to select the number of non-zero loadings in each PC while using SPCA. The proposed approach considerably improves the interpretability of PCs while minimizing the loss of total variance explained. Furthermore, we compare the performance of PCA- and SPCA-based techniques for fault detection and fault diagnosis. The key features of the methodology are assessed through a synthetic example and a comparative study of the benchmark Tennessee Eastman process."
Real-time quality monitoring and diagnosis for manufacturing process profiles based on deep belief networks,"A large number of real-time quality data are collected through various sensors in the manufacturing process. However, most process data are high-dimension, nonlinear and high-correlated, so that it is difficult to model the process profiles, which restricts the application of conventional statistical process control technique. Motivated by the powerful ability of deep belief network (DBN) to extract the essential features of input data, this paper develops a real-time quality monitoring and diagnosis scheme for manufacturing process profiles based on DBN. The profiles collected from a manufacturing process are mapped into quality spectra. A novel DBN recognition model for quality spectra is established in the off-line learning phase, which can be applied to monitor and diagnose the process profiles in the on-line phase. The effectiveness of DBN recognition model for manufacturing process profiles is demonstrated by simulation experiment, and a real injection molding process example is applied to analyze the performance. The results show that the proposed DBN model outperforms alternative methods."
Reconstructing shaft orbit using angle measurement to detect bearing faults,"The use of Instantaneous Angular Speed (IAS) in condition monitoring of rotating machines is an appealing alternative to traditional approaches such as those based on accelerometers: the direct angular sampling characteristic of IAS measurements has proven to be a favoured framework to study mechanical phenomena involved in rotating machines. Physical models have been established to link the variations of IAS to torque disturbances in case of mechanical fault such as bearing failure. In this paper an original point of view is given on the IAS measurement by linking IAS variations and shaft vibrations. First the orbit of a rotating shaft is studied to show that it contains useful information about the health state of the bearing. Then angle position sensors traditionally used in IAS measurements are combined to reconstruct this orbit. It is shown that IAS measurements are sensitive to the shaft vibrations, which may be used advantageously to enhance the diagnosis possibilities. Finally the links between various defect sizes and the reconstructed orbits are investigated to propose a fault severity indicator."
Relationship between lower lumbar spine shape and patient bone metabolic activity as characterised by 18F NaF bio-markers,"Chronic lower lumbar pain has been associated with elevated bone metabolic activity in the spine. Diagnosis of bone metabolic activity is currently through integrating Positron Emission Tomography (PET) with Sodium Fluoride (18F–NaF) biomarkers. It has been reported that numerous observable pathologies including lumbar fusion, disc abnormalities and scoliosis have often been associated with increased 18F–NaF uptake. The aim of this study was to identify what features of lower lumbar shape most strongly correlate with 18F–NaF uptake. Following a principal component analysis of 23 patients who presented with lumbar pain and underwent 18F–NaF PET-CT, it was revealed that three modes interpreted as (i) sacral tilt, (ii) vertebral disc spacing and (iii) spine size were the three characteristics that described 88.7% of spine shape in our study population. 18F–NaF was described by two modes including 18F–NaF intensity and spatial variation (anterior-inferior to posterior-superior). 18F–NaF was most sensitive to sacral tilt followed by vertebral disc spacing. A predictive model derived from that spine population was able to predict 18F–NaF ‘hot-spot’ locations with 85 ± 5% accuracy and with 71 ± 3% accuracy for the 18F–NaF magnitude. These results suggest that patients reporting with lower lumbar pain and who present with increased sacral tilt profiles and/or reduced disc spacing are good candidates for further 18F–NaF PET-CT imaging, evidenced by the high association between those shape profiles and 18F–NaF uptake."
Reliability modeling for a two-phase degradation system with a change point based on a Wiener process,"A two-phase degradation system with a change point is commonly seen in many real-world degradation systems due to influence of internal mechanisms and external environments, such as lithium-ion batteries, light-emitting diodes (LEDs), plasma display panels (PDPs), and vacuum fluorescent displays (VFDs). Random jumps exist at the change points of the degradation processes. Motivated by this phenomenon, this paper attempts to model such kinds of systems based on the Wiener process. Specifically, In Model 1, a constant change point model is first considered. The analytical results of the system reliability, lifetime distribution, and the remaining useful lifetime (RUL) are obtained. Then, based on Model 1, a random change point model, i.e., Model 2, is proposed to explain the change point phenomenon which is influenced by the accumulative effect of a shock process. Two phase-change patterns (PCPs) are considered in Model 2. For PCP I, the change point is triggered when the number of shocks reaches a predetermined threshold. For PCP II, the change point is triggered when the accumulative shock damage exceeds the corresponding threshold. The system reliability indexes are studied by using analytical and simulation methods. Finally, several numerical examples are given to illustrate the approaches and results."
Repetitive transient extraction for machinery fault diagnosis using multiscale fractional order entropy infogram,"The presence of repetitive transients in vibration signals is a typical symptom of local faults of rotating machinery. Infogram was developed to extract the repetitive transients from vibration signals based on Shannon entropy. Unfortunately, the Shannon entropy is maximized for random processes and unable to quantify the repetitive transients buried in heavy random noise. In addition, the vibration signals always contain multiple intrinsic oscillatory modes due to interaction and coupling effects between machine components. Under this circumstance, high values of Shannon entropy appear in several frequency bands or high value of Shannon entropy doesn’t appear in the optimal frequency band, and the infogram becomes difficult to interpret. Thus, it also becomes difficult to select the optimal frequency band for extracting the repetitive transients from the whole frequency bands. To solve these problems, multiscale fractional order entropy (MSFE) infogram is proposed in this paper. With the help of MSFE infogram, the complexity and nonlinear signatures of the vibration signals can be evaluated by quantifying spectral entropy over a range of scales in fractional domain. Moreover, the similarity tolerance of MSFE infogram is helpful for assessing the regularity of signals. A simulation and two experiments concerning a locomotive bearing and a wind turbine gear are used to validate the MSFE infogram. The results demonstrate that the MSFE infogram is more robust to the heavy noise than infogram and the high value is able to only appear in the optimal frequency band for the repetitive transient extraction."
Representation learning for mammography mass lesion classification with convolutional neural networks,"Background and objective The automatic classification of breast imaging lesions is currently an unsolved problem. This paper describes an innovative representation learning framework for breast cancer diagnosis in mammography that integrates deep learning techniques to automatically learn discriminative features avoiding the design of specific hand-crafted image-based feature detectors. Methods A new biopsy proven benchmarking dataset was built from 344 breast cancer patients’ cases containing a total of 736 film mammography (mediolateral oblique and craniocaudal) views, representative of manually segmented lesions associated with masses: 426 benign lesions and 310 malignant lesions. The developed method comprises two main stages: (i) preprocessing to enhance image details and (ii) supervised training for learning both the features and the breast imaging lesions classifier. In contrast to previous works, we adopt a hybrid approach where convolutional neural networks are used to learn the representation in a supervised way instead of designing particular descriptors to explain the content of mammography images. Results Experimental results using the developed benchmarking breast cancer dataset demonstrated that our method exhibits significant improved performance when compared to state-of-the-art image descriptors, such as histogram of oriented gradients (HOG) and histogram of the gradient divergence (HGD), increasing the performance from 0.787 to 0.822 in terms of the area under the ROC curve (AUC). Interestingly, this model also outperforms a set of hand-crafted features that take advantage of additional information from segmentation by the radiologist. Finally, the combination of both representations, learned and hand-crafted, resulted in the best descriptor for mass lesion classification, obtaining 0.826 in the AUC score. Conclusions A novel deep learning based framework to automatically address classification of breast mass lesions in mammography was developed."
Research and application of improved adaptive MOMEDA fault diagnosis method,"In a strong noise environment, vibration signals are easily submerged by noise. In recent years, many scholars have studied a large number of noise reduction methods. In 2017, Multipoint Optimal Minimum Entropy Deconvolution Adjusted (MOMEDA) is applied to the fault diagnosis of gearbox. Although MOMEDA overcomes Maximum correlation kurtosis deconvolution (MCKD) defects and it can extract continuous impulse signal, but it still has the following problems: 1) It can only extract single periodic pulse. If we want to extract the characteristics of multiple periodic pulse signals, we need to further update the algorithm; 2) In a strong noise environment, MOMEDA can also search for a fixed periodic signal, but most of the information is false component, it is easy to cause misdiagnosis, therefore, the signal needs to be preprocessed; 3) The accuracy of MOMEDA noise reduction is affected by the search interval and filter size, and McDondald did not reasonably explain them, so an adaptive selection method is needed. Considering these problems. Firstly the article preprocesses the composite fault with ensemble empirical mode decomposition (EEMD) and then reconstructs the intrinsic mode function with the same time scale. Further, proposing kurtosis spectral entropy as the objective function, the grid search method is used to search the filter length of MOMEDA, and the reconstructed intrinsic mode function is further denominated by MOMEDA. Finally, the proposed method is used to search the complex fault pulse signals in strong noise environment. It proves its reliability."
Reversible data hiding in medical images with enhanced contrast in texture area,"In order to realize the patient privacy protection in medical image, opposite to traditional reversible data hiding (RDH) methods which prior to embed message into the smooth area for pursuing high PSNR value, the proposed method priors to embed message into the texture area of the medical images for improving the quality of the details information and helping accurate diagnosis. Furthermore, in order to decrease the embedding distortion while enhancing the contrast of the texture area, this paper also proposes a message sparse representation method. Experiments implemented on medical images showed that the proposed method enhances the contrast of texture area when compared with previous methods."
Robot and its living space: A roadmap for robot development based on the view of living space,"Robots, as the creation of human, became an irreplaceable component in human society. With the advancement of technologies, robots become more and more intelligent and have been widely used in many fields, such as disease diagnosis, customer services, healthcare for the older people, and so on. As robots made our lives much more convenient than ever before, they also brought many potential risks and challenges in technology, security, and ethic. To better understand the development of robots, we proposed a concept of a robot’s living space and analyzed the role of robots in our society. In this paper, we focus on setting a theoretical framework of the robot’s living space to further understand the human-robot relationship. The research in this paper contains three central aspects. First, we interpret the concept of the robot’s living space and functions of each space. Second, we analyze and summarize the relative technologies which support robots living well in each space. Finally, we provide advice and improvement measures based on a discussion of potential problems caused by the developments of robots. With the trend of robots humanization and human-robot society integration, we should seriously consider how to make a collaboration with intelligent robots to achieve hybrid intelligence. To build a harmonious human-robot integrated society, studying the robot’s living space and its relationship with humans is the pre-requisite and roadmap."
Robot task planning and explanation in open and uncertain worlds,"A long-standing goal of AI is to enable robots to plan in the face of uncertain and incomplete information, and to handle task failure intelligently. This paper shows how to achieve this. There are two central ideas. The first idea is to organize the robot's knowledge into three layers: instance knowledge at the bottom, commonsense knowledge above that, and diagnostic knowledge on top. Knowledge in a layer above can be used to modify knowledge in the layer(s) below. The second idea is that the robot should represent not just how its actions change the world, but also what it knows or believes. There are two types of knowledge effects the robot's actions can have: epistemic effects (I believe X because I saw it) and assumptions (I'll assume X to be true). By combining the knowledge layers with the models of knowledge effects, we can simultaneously solve several problems in robotics: (i) task planning and execution under uncertainty; (ii) task planning and execution in open worlds; (iii) explaining task failure; (iv) verifying those explanations. The paper describes how the ideas are implemented in a three-layer architecture on a mobile robot platform. The robot implementation was evaluated in five different experiments on object search, mapping, and room categorization."
Robust hybrid deep learning models for Alzheimer’s progression detection,"The prevalence of Alzheimer’s disease (AD) in the growing elderly population makes accurately predicting AD progression crucial. Due to AD’s complex etiology and pathogenesis, an effective and medically practical solution is a challenging task. In this paper, we developed and evaluated two novel hybrid deep learning architectures for AD progression detection. These models are based on the fusion of multiple deep bidirectional long short-term memory (BiLSTM) models. The first architecture is an interpretable multitask regression model that predicts seven crucial cognitive scores for the patient 2.5 years after their last observations. The predicted scores are used to build an interpretable clinical decision support system based on a glass-box model. This architecture aims to explore the role of multitasking models in producing more stable, robust, and accurate results. The second architecture is a hybrid model where the deep features extracted from the BiLSTM model are used to train multiple machine learning classifiers. The two architectures were comprehensively evaluated using different time series modalities of 1371 subjects participated in the study of the Alzheimer’s disease neuroimaging initiative (ADNI). The extensive, real-world experimental results over ADNI data help establish the effectiveness and practicality of the proposed deep learning models."
RST-BatMiner: A fuzzy rule miner integrating rough set feature selection and Bat optimization for detection of diabetes disease,"Fuzzy classification rules are more interpretable and cope better with pervasive uncertainty and vagueness with respect to crisp rules. Because of this fact, fuzzy classification rules are extensively used in classification and decision support systems for disease diagnosis. But, most of the rule mining techniques failed to generate accurate and comprehensive fuzzy rules. This paper presents a hybrid decision support system based on Rough Set Theory (RST) and Bat optimization Algorithm (BA) called RST-BatMiner. It consists of two stages. In the first stage, redundant features have been removed from the data set through RST-based QUICK-REDUCT approach. In the second stage, for each class BA is invoked to generate fuzzy rules by minimizing proposed fitness function. Further, an Ada-Boosting technique is applied to the rules generated by BA to increase the accuracy rate of generated fuzzy rules. Moreover, to generate comprehensive fuzzy rules, a new ≠ (not equal) operator along with = (equal) operator is introduced into BA encoding scheme. The proposed RST-BatMiner builds consolidated fuzzy ruleset by learning the rules associated with each class separately. The proposed RST-BatMiner is experimented on six bench-mark datasets namely Pima Indians Diabetes, Wisconsin Breast Cancer, Cleveland Heart disease, iris, wine and glass, in order to validate its generalization capability. These experimental results show that except for wine dataset the proposed RST-BatMiner yields high accuracy and comprehensible ruleset when compared to other state-of-the-art bio-inspired based fuzzy rule miners and Fuzzy Rule Based Classification Systems (FRBCS) in the literature. In the case of wine dataset, the proposed RST-BatMiner yields second highest accuracy along with comprehensible ruleset."
Rule extraction using Recursive-Rule extraction algorithm with J48graft combined with sampling selection techniques for the diagnosis of type 2 diabetes mellitus in the Pima Indian dataset,"Diabetes is a complex disease that is increasing in prevalence around the world. Type 2 diabetes mellitus (T2DM) accounts for about 90–95% of all diagnosed adult cases of diabetes. Most present diagnostic methods for T2DM are black-box models, which are unable to provide the reasons underlying diagnosis to physicians; therefore, algorithms that can provide further insight are needed. Rule extraction can provide such explanations; however, in the medical setting, extracted rules must be not only highly accurate, but also simple and easy to understand. The Recursive-Rule eXtraction (Re-RX) algorithm is a “white-box” model that provides highly accurate classification. However, due to its recursive nature, it tends to generate more rules than other algorithms. Therefore, in this study, we propose the use of a rule extraction algorithm, Re-RX with J48graft, combined with sampling selection techniques (sampling Re-RX with J48graft) to achieve highly accurate, concise, and interpretable classification rules for the Pima Indian Diabetes (PID) dataset, which comprises 768 samples with two classes (diabetes or non-diabetes) and eight continuous attributes. The use of this algorithm resulted in an average accuracy of 83.83% after 10 runs of 10-fold cross validation. Sampling Re-RX with J48 graft achieved substantially better accuracy and provided a considerably fewer average number of rules and antecedents than the original Re-RX algorithm. These results suggest that sampling Re-RX with J48graft provides more accurate, concise, and interpretable extracted rules than previous algorithms, and is therefore more suitable for medical decision making, including the diagnosis of T2DM."
Rule-based automatic diagnosis of thyroid nodules from intraoperative frozen sections using deep learning,"Frozen sections provide a basis for rapid intraoperative diagnosis that can guide surgery, but the diagnoses often challenge pathologists. Here we propose a rule-based system to differentiate thyroid nodules from intraoperative frozen sections using deep learning techniques. The proposed system consists of three components: (1) automatically locating tissue regions in the whole slide images (WSIs), (2) splitting located tissue regions into patches and classifying each patch into predefined categories using convolutional neural networks (CNN), and (3) integrating predictions of all patches to form the final diagnosis with a rule-based system. To be specific, we fine-tune the InceptionV3 model for thyroid patch classification by replacing the last fully connected layer with three outputs representing the patch's probabilities of being benign, uncertain, or malignant. Moreover, we design a rule-based protocol to integrate patches’ predictions to form the final diagnosis, which provides interpretability for the proposed system. On 259 testing slides, the system correctly predicts 95.3% (61/64) of benign nodules and 96.7% (148/153) of malignant nodules, and classify 16.2% (42/259) slides as uncertain, including 19 benign and 16 malignant slides, which are a sufficiently small number to be manually examined by pathologists or fully processed through permanent sections. Besides, the system allows the localization of suspicious regions along with the diagnosis. A typical whole slide image, with 80, 000 × 60, 000 pixels, can be diagnosed within 1 min, thus satisfying the time requirement for intraoperative diagnosis. To the best of our knowledge, this is the first study to apply deep learning to diagnose thyroid nodules from intraoperative frozen sections. The code is released at https://github.com/PingjunChen/ThyroidRule."
Scaling and contextualizing personalized healthcare: A case study of disease prediction algorithm integration,"Today, advances in medical informatics brought on by the increasing availability of electronic medical records (EMR) have allowed for the proliferation of data-centric tools, especially in the context of personalized healthcare. While these tools have the potential to greatly improve the quality of patient care, the effective utilization of their techniques within clinical practice may encounter two significant challenges. First, the increasing amount of electronic data generated by clinical processes can impose scalability challenges for current computational tools, requiring parallel or distributed implementations of such tools to scale. Secondly, as technology becomes increasingly intertwined in clinical workflows these tools must not only operate efficiently, but also in an interpretable manner. Failure to identity areas of uncertainty or provide appropriate context creates a potentially complex situation for both physicians and patients. This paper will present a case study investigating the issues associated with first scaling a disease prediction algorithm to accommodate dataset sizes expected in large medical practices. It will then provide an analysis on the diagnoses predictions, attempting to provide contextual information to convey the certainty of the results to a physician. Finally it will investigate latent demographic features of the patient’s themselves, which may have an impact on the accuracy of the diagnosis predictions."
Segmentation and Feature Extraction in Medical Imaging: A Systematic Review,"Image processing techniques being crucial towards analyzing and resolving issues in medical imaging since last two decades. Medical imaging is a process or technique to find the inner or outer construction of mortal body. The process observes medicinal diagnosis, analyze illnesses and develop data-sets of normal and abnormal imageries. Medical imaging is divided in two folds such as invisible-light medical imaging and visible-light medical imaging. The second type of medical imaging were can be understood by a common person whereas the first type can be interpreted by a radiologist. Analysis of all these require segmentation and feature extraction. In fact a lot of medical imaging techniques are available but authors restrict survey to tumor detection through mammograms or magnetic resonance imaging. In this paper, authors survey on various segmentation and feature extraction methods in medicinal images used for preprocessing."
Selecting relevant features from the electronic health record for clinical code prediction,"A multitude of information sources is present in the electronic health record (EHR), each of which can contain clues to automatically assign diagnosis and procedure codes. These sources however show information overlap and quality differences, which complicates the retrieval of these clues. Through feature selection, a denser representation with a consistent quality and less information overlap can be obtained. We introduce and compare coverage-based feature selection methods, based on confidence and information gain. These approaches were evaluated over a range of medical specialties, with seven different medical specialties for ICD-9-CM code prediction (six at the Antwerp University Hospital and one in the MIMIC-III dataset) and two different medical specialties for ICD-10-CM code prediction. Using confidence coverage to integrate all sources in an EHR shows a consistent improvement in F-measure (49.83% for diagnosis codes on average), both compared with the baseline (44.25% for diagnosis codes on average) and with using the best standalone source (44.41% for diagnosis codes on average). Confidence coverage creates a concise patient stay representation independent of a rigid framework such as UMLS, and contains easily interpretable features. Confidence coverage has several advantages to a baseline setup. In our baseline setup, feature selection was limited to a filter removing features with less than five total occurrences in the trainingset. Prediction results improved consistently when using multiple heterogeneous sources to predict clinical codes, while reducing the number of features and the processing time."
Self-mixing interferometry as a diagnostics tool for plasma characteristics in laser microdrilling,"In this work, self-mixing interferometry (SMI) was used to monitor the optical path difference induced by the ablation plasma and plume. The paper develops the analytical relationships to explain the fringe appearance in the SMI during laser microdrilling. The monitoring principle was tested under a large experimental campaign of laser microdrilling on TiAlN ceramic coating with a low-ns green fibre laser. Key process parameters namely pulse energy, number and repetition rate were varied. The effect of side gas on the SMI signal characteristic was analysed. Laser induced breakdown spectroscopy (LIBS) was used to identify the plasma temperature and electron number density. The SMI signals were correlated to the plume size and its evolution as a function of process parameters, as well as electron number density estimated by spectroscopy. In addition to proving the validity of the proposed new method, the results show insights to the micromachining of the ceramic material with low ns pulses."
Self-running bearing diagnosis based on scalar indicator using fast order frequency spectral coherence,"In vibration-based diagnosis of rolling element bearings, the complexity of the signals requires an expert to use advanced signal processing tools and to interpret the results based on his/her experience. Recently, a few autonomous methods have been proposed to alleviate the demand on the user’s expertise, yet they have been mainly focused on fault detection. This paper follows a similar direction but with wider objectives: it aims to develop an indicator that is able to detect, identify and classify typical faults on rolling elements, inner and outer-race. The indicator is based on the recently developed Fast Order-Frequency Spectral Coherence, a key tool of the theory of second-order cyclostationary processes: it condenses the whole information initially displayed in three dimensions into a scalar and provides an interpretation in terms of a probability of presence of a fault. In addition, the proposed indicator is able to return information for different levels of damages in both stationary and non-stationary operating conditions. It takes into consideration uncertainties in the bearing characteristic frequencies, which is crucial in bearing diagnosis. A new pre-processing step is provided to ensure an efficient and constant statistical threshold. The proposed indicator is intended to be used in an autonomous process without the need for visual analysis and human interpretation. The proposed indicator is compared with a recent indicator based on the Envelop Spectrum, in terms of classification and detection performance. Several applications using real and simulated data eventually illustrate the capability for self-running diagnosis."
Sensor validation and reconstruction: Experiences with commercial technology,"Detecting the failure of a sensor in industrial processes is important to avoid the use of incorrect measurements. When a sensor fails the missing measurement are reconstructed, using the measurements of other sensors and inferring the missing or incorrect measurement. Although this technology has been developed more than 20 years ago, there are few commercial solutions available today. One of these few solutions uses principal component analysis, based on an algorithm originally developed by Qin and Li (1999). In this paper, this solution is applied to operating data from a minerals processing plant with persistent sensor problems Somewhat surprisingly, poor results are obtained, despite numerous attempts to improve reconstructability. Analysis indicates that the challenges are not about the algorithm but rather about choices that need to be made in the application of data-driven analysis tools to new data sets. These include data selection, filtering and interpreting which results are useful. It is suggested that together with any new algorithm presented. researchers should provide practical guidelines in choosing appropriate data, and any pre-processing that may be required."
Shape and margin-aware lung nodule classification in low-dose CT images via soft activation mapping,"A number of studies on lung nodule classification lack clinical/biological interpretations of the features extracted by convolutional neural network (CNN). The methods like class activation mapping (CAM) and gradient-based CAM (Grad-CAM) are tailored for interpreting localization and classification tasks while they ignored fine-grained features. Therefore, CAM and Grad-CAM cannot provide optimal interpretation for lung nodule categorization task in low-dose CT images, in that fine-grained pathological clues like discrete and irregular shape and margins of nodules are capable of enhancing sensitivity and specificity of nodule classification with regards to CNN. In this paper, we first develop a soft activation mapping (SAM) to enable fine-grained lung nodule shape & margin (LNSM) feature analysis with a CNN so that it can access rich discrete features. Secondly, by combining high-level convolutional features with SAM, we further propose a high-level feature enhancement scheme (HESAM) to localize LNSM features. Experiments on the LIDC-IDRI dataset indicate that 1) SAM captures more fine-grained and discrete attention regions than existing methods, 2) HESAM localizes more accurately on LNSM features and obtains the state-of-the-art predictive performance, reducing the false positive rate, and 3) we design and conduct a visually matching experiment which incorporates radiologists study to increase the confidence level of applying our method to clinical diagnosis."
Shape-based outlier detection in multivariate functional data,"Multivariate functional data refer to a population of multivariate functions generated by a system involving dynamic parameters depending on continuous variables (e.g., multivariate time series). Outlier detection in such a context is a challenging problem because both the individual behavior of the parameters and the dynamic correlation between them are important. To address this problem, recent work has focused on multivariate functional depth to identify the outliers in a given dataset. However, most previous approaches fail when the outlyingness manifests itself in curve shape rather than curve magnitude. In this paper, we propose identifying outliers in multivariate functional data by a method whereby different outlying features are captured based on mapping functions from differential geometry. In this regard, we extract shape features reflecting the outlyingness of a curve with a high degree of interpretability. We conduct an experimental study on real and synthetic datasets and compare the proposed method with functional-depth-based methods. The results demonstrate that the proposed method, combined with state-of-the-art outlier detection algorithms, can outperform the functional-depth-based methods. Moreover, in contrast with the baseline methods, it is efficient regardless of the proportion of outliers."
Shock tube measurement of NO time-histories in nitromethane pyrolysis using a quantum cascade laser at 5.26 µm,"The pyrolysis of nitromethane, CH3NO2, was studied at ∼3.5 atm and 1013 K–1418 K in a heated shock tube by measuring the key product nitric oxide (NO) using mid-infrared laser absorption spectroscopy. We used a quantum cascade laser (QCL) at 5.26 µm to exploit the strong NO absorption at 1900.08 cm−1. With the NO absorption cross-section data characterized at 1006 K–1789 K and 2.7 atm–3.5 atm behind reflected shock waves, we measured the NO concentration time-histories during the pyrolysis of nitromethane at two different concentrations (1.05% and 0.6%). The absorption interference from other major products such as CO and H2O was analyzed to be negligible, leading to an interference-free NO diagnostic in nitromethane pyrolysis. A recent kinetic model of Shang et al. (2019) was adopted to interpret the shock tube data. All the NO time-histories measured over the entire temperature range 1013 K–1418 K were well-predicted by this model in terms of the initial NO formation rate and the final plateau level. The rate-of-production, sensitivity, and reaction flux analyses were performed to identify four important reactions (CH3NO2 = CH3 + NO2, CH3NO2 ↔ CH3ONO = CH3O + NO, CH3 + NO2 = CH3O + NO, and NO2 + H = NO + OH) that determine the NO formation during CH3NO2 pyrolysis. The satisfactory agreement between the simulation and shock tube/laser absorption measurement further validated the kinetic mechanism of nitromethane decomposition. The developed mid-infrared NO absorption sensor provides a promising diagnostic tool for studying fuel-nitrogen chemical kinetics in the shock tube experiments."
Sickle-cell disease diagnosis support selecting the most appropriate machine learning method: Towards a general and interpretable approach for cell morphology analysis from microscopy images,"In this work we propose an approach to select the classification method and features, based on the state-of-the-art, with best performance for diagnostic support through peripheral blood smear images of red blood cells. In our case we used samples of patients with sickle-cell disease which can be generalized for other study cases. To trust the behavior of the proposed system, we also analyzed the interpretability. We pre-processed and segmented microscopic images, to ensure high feature quality. We applied the methods used in the literature to extract the features from blood cells and the machine learning methods to classify their morphology. Next, we searched for their best parameters from the resulting data in the feature extraction phase. Then, we found the best parameters for every classifier using Randomized and Grid search. For the sake of scientific progress, we published parameters for each classifier, the implemented code library, the confusion matrices with the raw data, and we used the public erythrocytesIDB dataset for validation. We also defined how to select the most important features for classification to decrease the complexity and the training time, and for interpretability purpose in opaque models. Finally, comparing the best performing classification methods with the state-of-the-art, we obtained better results even with interpretable model classifiers."
Simple and effective open switch fault diagnosis of single-phase PWM rectifier,"In order to obtain lower harmonics distortion and higher power factors, single-phase pulse-width modulation (PWM) rectifiers are adopted in AC railway drive systems. Therefore, its reliability is of most importance with regard to the safe operation of the train. In this paper, a fault diagnosis method for open switch fault in single-phase PWM rectifier is proposed based on the switching system theory. It requires no additional sensor, nor extra operation states need to be set. Four observers which correspond to four kinds of open switch faults are utilized to detect and locate the faults. Real-time simulations are carried out to validate the effectiveness of this method."
Simple continuous optimal regions of the space of data,"The definition of efficient programs for both maintenance and optimization is a struggling task in many industrial sectors. In this context, data analysis can significantly improve the state-of-the-art techniques, employed, for instance, to determine if a particular component or product is showing an anomalous behavior with respect to a defined nominal state. In fact, through the analysis of data collected on field, it is possible to detect optimal operating regions and to detect anomalies in advance. In this context, we propose a multi-purpose algorithm for unsupervised or semi-supervised learning in order to determine a simple continuous region of points. This region can be adopted in order to describe a component or a product nominal behavior and can be used in order to detect anomalies which are outside it. Such a region can be defined adopting a finite ensemble of thresholds, whose value can be physically interpreted. In order to show the effectiveness of our approach, the proposed method has been tested in an Anomaly Detection problem concerning Predictive Maintenance, exploiting data coming from a naval vessel, characterized by a combined diesel-electric and gas propulsion plant."
Simplified Granger causality map for data-driven root cause diagnosis of process disturbances,"Root cause diagnosis is an important step in process monitoring, which aims to identify the sources of process disturbances. The primary challenge is that process disturbances propagate between different operating units because of the flow of material and information. Data-driven causality analysis techniques, such as Granger causality (GC) test, have been widely adopted to construct process causal maps for root cause diagnosis. However, the generated causal map is over-complicated and difficult to interpret because of the existence of process loops and the violation of statistical assumptions. In this work, a two-step procedure is proposed to solve this problem. First, a causal map is built by adopting the conditional GC analysis, which is viewed as a graph in the next step. In this graph, each vertex corresponds to a process variable under investigation, while the weight of the edge connecting two vertices is the F-value calculated by conditional GC. This graph is then simplified by computing its maximum spanning tree. Thus, the results of the causality analysis are transformed into a directed acyclic graph, which eliminates all loops, highlights the root cause variable, and facilitates the diagnosis. The feasibility of this method is illustrated with the application to the Tennessee Eastman benchmark process. In the investigated case studies, the proposed method outperforms the conditional GC test and provides an easy way to identify the root cause of process disturbances."
Simplified model of defective pile-soil interaction considering three-dimensional effect and application to integrity testing,"A new simplified model for defective pile-soil interaction (DPSI) is developed to account for the three-dimensional (3D) effect in the pile integrity test (PIT) for large diameter piles. To account for the 3D effect, the defective pile is treated as linear continuum rather than the conventional 1D rod model. The surrounding soil is simulated employing the classical plane strain model and the soil resistance at the pile base is represented by the fictitious soil pile model. Theoretical solutions for the velocity field of the defective pile under vertical half-sine excitation are derived by integral transformation technique. The developed DPSI model and solutions are then used to investigate the 3D effect in the PIT for defective pile considering four common pile defect types. The obtained results provide important insights into the high-frequency (HF) interferences on the velocity-time history of the defective pile top caused by the 3D effect. Recommendations to minimize the HF interferences in the actual PIT are consequently provided. Application of the DPSI model in interpreting PIT measurements can improve accuracy of pile defect diagnosis."
Simultaneous determination of insulin and glucose in human serum based on dual emissive fluorescent nano-aptasensor of carbon dots and CdTe/CdS/ZnS quantum dots,"A dual emissive fluorescent nano aptasensor was designed for simultaneous detection of human serum glucose and insulin levels. CdTe/CdS/ZnS quantum dots and carbon dots were synthesized and conjugated with the aptamers of glucose and insulin, respectively. The fluorescence of these conjugates was quenched by nano-graphite based on the static quenching and inner filter effect, and restored on the addition of the analytes. All reaction parameters were optimized using analysis of variance, and a polynomial regression model was constructed and verified to explain the effects of reaction parameters on the outcome. The linear ranges of insulin and glucose were 0.2–2 nM and 0.5–7 mM, respectively. The limits of detection for insulin and glucose were 0.018 nM and 0.058 mM, respectively. All the values of relative error % were within the range of −5.00−4.00%, and the values of relative standard deviation % were ≤3.77%. The changes in fluorescence intensity restoration of the probe in the presence of various interfering species were less than 5%, and all the recoveries for glucose and insulin were in the range of 90–110%. The probe was validated and applied successfully to the determination of the stated analytes in human serum. The fabricated aptasensor can be used to diagnose diabetes and its type with a single assay in clinical diagnostic labs."
"Simultaneous high speed PIV and CH PLIF using R-branch excitation in the C2Σ+-X2Π (0,0) band","Simultaneous particle image velocimetry (PIV) and planar laser-induced fluorescence (PLIF) utilizing R-branch transitions in the C-X (0,0) band were performed at a 10-kHz repetition-rate in a turbulent premixed flame. The CH lines at 310.690 nm (from the R-branch of the C-X band) used here have greater efficiency than A-X and B-X transitions, which allows for high-framerate imaging with low laser pulse energy. Most importantly, the simultaneous imaging of both CH PLIF and PIV is enabled by the use of a custom edge filter, which blocks scattering at the laser wavelength (below ∼311 nm) while efficiently transmitting fluorescence at longer wavelengths. The Hi-Pilot Bunsen burner operated with a turbulent Reynolds number of 7900 was used to demonstrate simultaneous PIV and CH PLIF utilizing this filtered detection scheme. Instances where pockets of products were observed well upstream of the mean flame brush are found to be the result of out-of-plane motion of the flame sheet. Such instances can lead to ambiguous results when interpreting the thickness of reaction layers. However, the temporally resolved nature of the present diagnostics facilitate the identification and proper treatment of such situations. The strategy demonstrated here can yield important information in the study of turbulent flames by providing temporally resolved flame dynamics in terms of flame sheet visualization and velocity fields."
Simultaneous kHz-rate temperature and velocity field measurements in the flow emanating from angled and trenched film cooling holes,"To design more efficient film cooling geometries for gas turbines, non-intrusive measurements of the flow temperature, velocity and derived quantities like the turbulent heat flux are needed in well-defined, generic flow configurations. With this aim we have applied thermographic particle image velocimetry (thermographic PIV) to investigate the flow emanating from angled and trenched cooling holes in a closed-loop optically-accessible wind tunnel facility. BAM:Eu2+ thermographic phosphor particles were seeded into the flow as a tracer. A pulsed high-speed UV laser was used to excite the particles and the luminescence was detected using two high-speed cameras to determine the temperature field by a two-colour ratiometric approach. The velocity field was measured using ordinary high-speed PIV. The simultaneously measured fields were sampled at a rate of 6kHz in a vertical plane through the centreline of the symmetrical single-row cooling holes. The flowrate and temperature of the cooling air and heated main flow were chosen to achieve density and momentum flux ratios of 1.6 and 8 respectively. For these conditions the average and RMS temperature fields show that for ordinary angled holes the jet is detached from the surface. In contrast, the trenched geometry leads to a cooling film attached to the surface. However, time-resolved image sequences show instances where hot air breaks through the cooling film and almost reaches the surface. Similar image sequences for the angled holes show that the detached coolant jet becomes unstable downstream and pockets of cold air are ejected into the main flow. This intermittency may in part explain the observation that the measured turbulent heat flux is oriented towards the cold core, but deviates from the direction of the mean temperature gradient, thereby contradicting the simple gradient diffusion hypothesis commonly used in RANS simulations."
Small lung nodules detection based on local variance analysis and probabilistic neural network,"Background and objective In medical examinations doctors use various techniques in order to provide to the patients an accurate analysis of their actual state of health. One of the commonly used methodologies is the x-ray screening. This examination very often help to diagnose some diseases of chest organs. The most frequent cause of wrong diagnosis lie in the radiologist’s difficulty in interpreting the presence of lungs carcinoma in chest X-ray. In such circumstances, an automated approach could be highly advantageous as it provides important help in medical diagnosis. Methods In this paper we propose a new classification method of the lung carcinomas. This method start with the localization and extraction of the lung nodules by computing, for each pixel of the original image, the local variance obtaining an output image (variance image) with the same size of the original image. In the variance image we find the local maxima and then by using the locations of these maxima in the original image we found the contours of the possible nodules in lung tissues. However after this segmentation stage we find many false nodules. Therefore to discriminate the true ones we use a probabilistic neural network as classifier. Results The performance of our approach is 92% of correct classifications, while the sensitivity is 95% and the specificity is 89.7%. The misclassification errors are due to the fact that network confuses false nodules with the true ones (6%) and true nodules with the false ones (2%). Conclusions Several researchers have proposed automated algorithms to detect and classify pulmonary nodules but these methods fail to detect low-contrast nodules and have a high computational complexity, in contrast our method is relatively simple but at the same time provides good results and can detect low-contrast nodules. Furthermore, in this paper is presented a new algorithm for training the PNN neural networks that allows to obtain PNNs with many fewer neurons compared to the neural networks obtained by using the training algorithms present in the literature. So considerably lowering the computational burden of the trained network and at same time keeping the same performances."
Space-time model and spectrum mechanism on vibration signal for planetary gear drive,"Planetary gear drives pose complex vibration signals due to that the excitations of the signal are the space-time varying meshing forces and the transfer paths for the excitations are time-varying systems. It brings about difficulties in machine diagnostics field to model, to predict, and to interpret the spectrum mechanism of the signal. This paper suggests a space-time model for this time-varying problem. Premised on deformable-body transfer path, the model formulates the response signal in frequency-domain as an integral transform of the excitations, where the transform kernel is formulated by the product of the Fourier kernel and the Frequency Response Function of the transfer path. A corresponding numerical method is also proposed for the numerical computation of the signal. Comparing with the existing model and method premised on rigid body transfer path, the proposed approach presents higher accuracy on signal prediction and spectral feature interpretation with respect to the frequency, amplitude and modal information."
"Specific binding of antigen-antibody in physiological environments: Measurement, force characteristics and analysis","The specific recognition of the antigen by the antibody is the crucial step in immunoassays. Measurement and analysis of the specific recognition, including the ways in which it is influenced by external factors are of paramount significance for the quality of the immunoassays. Using prostate-specific antigen (PSA)/anti-PSA antibody and α-fetoprotein (AFP) /anti-AFP antibody as examples, we have proposed a novel solution for measuring the binding forces between the antigens and their corresponding antibodies in different physiological environments by combining laminar flow control technology and optical tweezers technology. On the basis of the experimental results, the different binding forces of PSA/anti-PSA antibody and AFP/anti-AFP antibody in the same phosphate-buffered saline (PBS) environments are analysed by comparing the affinity constant of the two antibodies and the number of antigenic determinants of the two antigens. In different electrolyte environments, the changes of the binding force of antigens-antibodies are explained by the polyelectrolyte effect and hydrophobic interaction. Furthermore, in different pH environments, the changes of binding forces of antigens-antibodies are attributed to the role of the denaturation of protein. The study aims to recognise the antigen-antibody immune mechanism, thus ensuring further understanding of the biological functions of tumour markers, and it promises to be very useful for the clinical diagnosis of early-stage cancer."
"Speech intelligibility of English, Polish, Arabic and Mandarin under different room acoustic conditions","This paper examines the impact of room acoustic conditions on the speech intelligibility of four languages (English, Polish, Arabic and Mandarin). Listening test scores (diagnostic rhyme tests, phonemically balanced word tests and phonemically balanced sentence tests) of the four languages were compared under four room acoustic conditions defined by their speech transmission index (STI=0.2, 0.4, 0.6 and 0.8). The results obtained indicated that there was a statistically significant difference between the word intelligibility scores of languages under all room acoustic conditions, apart from the STI=0.8 condition. English was the most intelligible language under all conditions, and differences with other languages were larger when conditions were poor (maximum difference of 29% at STI=0.2, 33% at STI=0.4 and 14% at STI=0.6). Results also showed that Arabic and Polish were particularly sensitive to background noise, and that Mandarin was significantly more intelligible than those languages at STI=0.4. Consonant-to-vowel ratios and languages’ distinctive features and acoustical properties explained some of the scores obtained. Sentence intelligibility scores confirmed variations between languages, but these variations were statistically significant only at the STI=0.4 condition (sentence tests being less sensitive to very good and very poor room acoustic conditions). Overall, the results indicate that large variations between the speech intelligibility of different languages can occur, especially for spaces that are expected to be challenging in terms of room acoustic conditions. Recommendations solely based on room acoustic parameters (e.g. STI) might then prove to be insufficient for designing a multilingual environment."
Speed characteristics of disk–shaft system with rotating part looseness,"This study focuses on the looseness fault of rotating parts in rotor systems. A dedicated experimental setup is designed to induce looseness faults. Two particular phenomena of looseness faults are observed in the experiments under different torsional loads: when the torsional load is low, a constant difference in the rotational speed between the disk and shaft can be observed; when the torsional load is high, the rotational speed of the disk will dissociate with the rotational speed of the shaft and finally fluctuate within a range. The dynamic equations of the disk–shaft system are established, and a novel contact model of rotating part looseness is proposed. In the contact model, looseness contact stress may exist in the contact area and is described as a form with bilinear stiffness. Simulations using the equations are performed to explain the experiment results. The two phenomena, namely speed difference and speed dissociation, are satisfactorily explained by the simulation results. The results of this study may be helpful for the diagnosis of looseness faults in practical engineering."
SPIDER in the roadmap of the ITER neutral beams,"To reach fusion conditions and control plasma configuration in ITER, a suitable combination of additional heating and current drive systems is necessary. Among them, two Neutral Beam Injectors (NBI) will provide 33 MW hydrogen/deuterium particles electrostatically accelerated to 1 MeV; efficient gas-cell neutralisation at such beam energy requires negative ions, obtained by caesium-catalysed surface conversion of atoms inside the ion source. As ITER NBI requirements have never been simultaneously attained, a Neutral Beam Test Facility (NBTF) was set up at Consorzio RFX (Italy), including two experiments. MITICA is the full-scale NBI prototype with 1 MeV particle energy. SPIDER, with 100 keV particle energy, aims at testing and optimising the full-scale ion source: extracted beam uniformity, negative ion current density (for one hour) and beam optics (beam divergence <7 mrad; beam aiming direction within 2 mrad). This paper outlines the worldwide effort towards the ITER NBI realisation: the main results of the ELISE facility (IPP-Garching, Germany), equipped with a half-size source, are described along with the status of MITICA; specific issues are investigated by small specific facilities and by joint experiments at QST and NIFS (Japan). The SPIDER experiment, just come into operation, will profit from strong modelling activities, to simulate and interpret experimental scenarios, and from advanced diagnostic instruments, providing thorough plasma and beam characterisation. Finally, the results of the first experiments in SPIDER are presented, aimed at a preliminary source plasma characterisation by plasma light detectors and plasma spectroscopy."
SpiderNet: A spiderweb graph neural network for multi-view gait recognition,"Human gait is a proven biometric trait with applications in security for authentication and disease diagnosis. However, it is one-sided to express and interpret gait data from a single point of view, which cannot reflect multi-dimensional characteristics of gait changes. Moreover, if the gait pattern observed from other views has pathological or abnormal behavior, or has micro movement, it is not easy to be detected and thus affects the recognition rate of gait. In addition, the multi-view fusion of gait knowledge can be challenging due to the close correlation between various visual angles. Owing to the above facts, we propose a spiderweb graph neural network (SpiderNet) to solve the multi-view gait recognition problem, which connects the gait data of single view with that of other views concurrently and constructs an active graph convolutional neural network. The gait trajectory of each view is analyzed by the combination of a memory module and a capsule module, which accomplishes the multi-view feature fusion, as well as the spatio-temporal feature extraction of single view. The experimental results show that the SpiderNet is superior to fifteen state-of-the-art methods, such as random forest, long-short term memory and convolutional neural network, and achieves 98.54%, 98.77%, and 96.91% of the results on three challenging gait datasets: SDUgait, CASIA-B, and OU-MVLP."
Spiral FBG sensors-based contact detection for confocal laser endomicroscopy,"Endomicroscopy is an emerging non-invasive technique for real-time diagnosis of intraluminal malignancies. For accurate microscopic steering of the imaging probe in vivo, a miniature continuum manipulator has been developed to perform large-area optical biopsy. To keep images in focus, consistent contact with proper force and orientation between the imaging probe tip and the targeted tissue is required. This paper presents a spiral FBG sensors-based sensing method to simultaneously measure the force and torque exerted at the tip of the probe when contacting with the tissue. The embodiment consists of a tapered substrate with a hollow inner lumen for holding the imaging probe, and three optical fibres equally and spirally distributed on the outer surface of the substrate. Each fibre has two FBG sensors to detect small strain changes at two different cross-sections. The modelling process is explained in detail, and a learning-based measurement decoupling method is also provided. In vitro experiments are performed to collect cellular images with simultaneous force and torque sensing, demonstrating the practical value of the technique."
"Spray penetration, combustion, and soot formation characteristics of the ECN Spray C and Spray D injectors in multiple combustion facilities","In a collaborative effort to identify key aspects of heavy-duty diesel injector behavior, the Engine Combustion Network (ECN) Spray C and Spray D injectors were characterized in three independent research laboratories using constant volume pre-burn vessels and a heated constant-pressure vessel. This work reports on experiments with nominally identical injectors used in different optically accessible combustion chambers, where one of the injectors was designed intentionally to promote cavitation. Optical diagnostic techniques specifically targeted liquid- and vapor-phase penetration, combustion indicators, and sooting behavior over a large range of ambient temperatures—from 850 K to 1100 K. Because the large-orifice injectors employed in this work result in flame lengths that extend well beyond the optical diagnostics’ field-of-view, a novel method using a characteristic volume is proposed for quantitative comparison of soot under such conditions. Further, the viability of extrapolating these measurements downstream is considered. The results reported in this publication explain trends and unique characteristics of the two different injectors over a range of conditions and serve as calibration targets for numerical efforts within the ECN consortium and beyond. Building on agreement for experimental results from different institutions under inert conditions, apparent differences found in combustion indicators and sooting behavior are addressed and explained. Ignition delay and soot onset are correlated and the results demonstrate the sensitivity of soot formation to the major species of the ambient gas (i.e., carbon dioxide, water, and nitrogen in the pre-burn ambient versus nitrogen only in the constant pressure vessel) when holding ambient oxygen volume percent constant."
SRM sensorless for position control based on a frequency modulation system,"This paper presents a study of a system to detect the angular position of the shaft of an 8/6 Switched Reluctance Machine (SRM) for position control. The detection system is independent of the machine circuits and is based on the indirect measurement of the induction coefficient, using a diagnostic coil. This induction coefficient, variable with the angular position, is the variable element of an oscillator, producing a frequency modulation. The intrinsic problems that this indirect position measurement system presents, particularly related with position control, are explained. The influence of the machine temperature, of the rotor speed and of the magnetic circuit saturation are also discussed."
Stabilization of a turbulent premixed flame by a plasma filament,"The mechanism of stabilizing a turbulent premixed methane-air flame using warm filamentary plasma is investigated by using laser diagnostics. First, stabilization of a turbulent jet flame is demonstrated in a setup using a pin-to-pin plasma discharge. The coupled plasma-flame structures were visualized utilizing planar laser-induced fluorescence (PLIF) of formaldehyde (CH2O) and methylidyne radicals (CH), as well as laser Rayleigh scattering thermometry imaging. The results show that the plasma channel and the flame front are spatially separated by a layer of hot burning products attributed to the flame propagation from the plasma core. Because of this spatial separation, the impacts of plasma on combustion are primarily thermal since the energetic radical species (such as O, H), produced by the discharge, have short equilibration time and cannot spread far away from the discharge channel before reaching the equilibrium state. From this point of view, turbulence would be beneficial for promoting the transport of plasma-produced radicals and thus bridge the gap between the plasma and the flame front. The plasma is still able to stabilize the flame. Based upon the experimental results, a frequent ignition-flame propagation (FIFP) model is proposed to explain the flame stabilization process. For the contracted plasma filament, the local power density is high enough to initialize the flame kernel that propagates away from the plasma channel until extinction. The propagation process is, however, strongly affected by turbulence. Local extinction is highly probable and thus the flame front has to be close to the ignition source at strong turbulence. At such conditions, the stabilized flame can be regarded as a large number of flame pockets, repeating the three phases of ignition, propagation and extinction, which can be summarized as the FIFP model. It infers that the flame propagation phase is important for sustaining the flame to complete combustion. Hence, this phase should be extended, which is more probable to achieve if the plasma ignition pilot is located in a section of limited turbulence."
Stacking-based ensemble learning of decision trees for interpretable prostate cancer detection,"Prostate cancer is a highly incident malignant cancer among men. Early detection of prostate cancer is necessary for deciding whether a patient should receive costly and invasive biopsy with possible serious complications. However, existing cancer diagnosis methods based on data mining only focus on diagnostic accuracy, while neglecting the interpretability of the diagnosis model that is necessary for helping doctors make clinical decisions. To take both accuracy and interpretability into consideration, we propose a stacking-based ensemble learning method that simultaneously constructs the diagnostic model and extracts interpretable diagnostic rules. For this purpose, a multi-objective optimization algorithm is devised to maximize the classification accuracy and minimize the ensemble complexity for model selection. As for model combination, a random forest classifier-based stacking technique is explored for the integration of base learners, i.e., decision trees. Empirical results on real-world data from the General Hospital of PLA demonstrate that the classification performance of the proposed method outperforms that of several state-of-the-art methods in terms of the classification accuracy, sensitivity and specificity. Moreover, the results reveal that several diagnostic rules extracted from the constructed ensemble learning model are accurate and interpretable."
Stationary subspaces-vector autoregressive with exogenous terms methodology for degradation trend estimation of rolling and slewing bearings,"Degradation trend estimation (DTE) of rotating machinery plays a vital role in prognostics and health management (PHM). It enables us to foresee future conditions and avoid unexpected risks. Recently, considerable accomplishments in the field of rotating machinery PHM has achieved through regression analysis based data-driven prognostics, which assist in directly analyzing and exploring the relationships between degradation trend and characterization indicators. Internal static structures still widely exist in most of them, inevitably restricting the natural extrapolation or generalization to future moments. Thus, the autoregression theories with complete mathematical foundations are first introduced and extended the methodologies for rotating machinery DTE. Meanwhile, the characterization ability of degradation or damage information from a single indicator rather than multi-endogenous indicators considering their causality and interactions may significantly reduce in the existing regression analysis based prognostics, and it further influences the final prognostics. Therefore, the idea of exploring internal dynamic structural regression based prognostics containing establishing multi-endogenous degradation indicators with weak-stationary traits and an interpretable and lightweight vector autoregression based DTE modeling method is motivated. The above dilemmas are well addressed through the in-depth study of autoregression based prognostics, namely stationary subspaces-vector autoregressive with exogenous terms (SSVARX). To be specific, multi-channel vibration signals are first picked up, and non-stationary signals are converted into time and frequency domain based weak-stationary degradation indicators via double stationary subspace decomposition and differential operation. Then the above two domain endogenous variables are feed into our proposed DTE models after stationarity test, order determination, and impulse response analysis. Finally, promising results from two run-to-failed life tests of rolling and slewing bearings are obtained via our multi-endogenous variables based extrapolation model. Compared with existing prediction methodologies, SSVARX of this study achieves not only high-accurate prediction results but also fast-computing speed and reasonable mathematical supports."
Stem water potential estimation of drip-irrigated early-maturing peach trees under Mediterranean conditions,"In the last decade deficit irrigation strategies allowed growers to deal with water shortages, while monitoring stem water potential (Ψstem) is deemed essential for maximising fruit yield and quality. However, because of the intensive labour involved in measuring Ψstem, alternative methods are desirable. The experiment described was conducted in Murcia (Spain) with adult peach trees (Prunus persica (L.) Batsch cv. Flordastar) submitted to different drip irrigation treatments, measuring Ψstem with a pressure chamber and the soil water content with a neutron probe. Agro-meteorological variables were recorded. Seasonal patterns of stem water potential provide a useful diagnostic tool for irrigation management in peach trees. Rainfall events and the meteorological conditions prevailing in autumn pointed to the resilient nature of the peach cultivar studied. Fitting Ψstem by linear regression analysis as a function of soil and atmosphere yielded a significant correlation, with the soil water content being the main contributor to estimating Ψstem. Linear regression analysis highlighted the importance of considering plant water status as a function of the peach tree cultivar, the atmospheric conditions in which it develops and the soil water conditions resulting from irrigation. A multiple linear regression equation based on soil water content in the soil profile, mean daily air vapour pressure deficit (VPDm) and growing degree hours (GDH) data explained 72% of the variance in Ψstem, and is proposed as an alternative to the field measurement of Ψstem."
Strain rate effect on sooting characteristics in laminar counterflow diffusion flames,"The effects of strain rate, oxygen enrichment and fuel type on the sooting characteristics of counterflow diffusion flames were studied. The sooting structures and relative PAH concentrations were measured with laser diagnostics. Detailed soot modeling using recently developed PAH chemistry and surface reaction mechanism was performed and the results were compared with experimental data for ethylene flames, focusing on the effects of strain rates. The results showed that increase in strain rate reduced soot volume fraction, average size and peak number density. Increase in oxygen mole fraction increased soot loading and decreased its sensitivity on strain rate. The soot volume fractions of ethane, propene and propane flames were also measured as a function of global strain rate. The sensitivity of soot volume fraction to strain rate was observed to be fuel dependent at a fixed oxygen mole fraction, with the sensitivity being higher for more sooting fuels. However, when the soot loadings were matched at a reference strain rate for different fuels by adjusting oxygen mole fraction, the dependence of soot loading on strain rate became comparable among the tested fuels. PAH concentrations were shown to decrease with increase in strain rate and the dependence on strain rate is more pronounced for larger PAHs. Soot modeling was performed using detailed PAH growth chemistry with molecular growth up to coronene. A qualitative agreement was obtained between experimental and simulation results, which was then used to explain the experimentally observed strain rate effect on soot growth. However, quantitatively, the simulation result exhibits higher sensitivity to strain rate, especially for large PAHs and soot volume fractions."
Strain-based health monitoring and remaining life prediction of large caliber gun barrel,"When a large caliber rifled gun fires, wear and fatigue damage the gun barrel and lead to the end of its useful life. It is important to understand the gun barrel condition during its service time and to predict the remaining useful life. In this paper, three traditional life prediction methods, including heat-based, dimension-based and velocity-based method, are summarized and proved to be insufficient for predicting the remaining useful life. A novel strain-based approach to life prediction of gun barrel is presented and analyzed theoretically, in which the strain of the outside surface of gun barrel is selected as a health index of the gun barrel. A large number of laboratory experiments on engraving small caliber projectiles have been carried out to verify the reasonability and feasibility of this strain-based method. Self-made copper and brass jacketed projectiles were pushed through a short gun barrel section under quasi-static and dynamic loading. The hoop and axial strains were measured using strain gauges and stored for analysis. Experimental results are listed as follows. (1) The measured strains do reflect the interaction between projectile and gun barrel under different test conditions. (2) Material property of jacket and diameter of projectile have influence on the external strains. (3) The jacket undergoes friction and plastic deformation during the engraving process. Effects of strain rate and thermal softening play critical roles in affecting the projectile and gun barrel interaction. This finding helps to explain that higher strains at low charges and lower strains at high charges occur to a 155 mm gun during firing practice."
Stream Data Analytics for Network Attacks’ Prediction,"Nowadays stream data, flowing over the modern networks between disparate data sources, become the norm. The broadband Internet, the Internet of Things (IoT) and cloud computing require to analyze the data from streams to make data-driven decisions in real time. In today’s world of more complex and increasing in the number of network attacks, one of their most important data is the data from network security (NS) tools, ensuring their secure and resilient operations and uninterrupted provision of services to its users. At Gartner Data & Analytics Summit-2019 augmented analytics and data management, as well as continuous intelligence and explainable artificial intelligence were indicated among the top trends in data and analytics technology that have significant disruptive potential over the next 3-5 years. In practice, complexities of the modern attack scenarios often make it difficult for NS administrators to understand the current NS-related status and to recognize emerging patterns of attacks in a vast amount of raw data before they make a substantial impact. To benefit from the NS-related stream data, businesses require powerful analytics tools for ingesting and processing it. There are four consequent levels of analytics maturity - namely descriptive, diagnostic, predictive, prescriptive. In this paper, a simplified NS-related stream data architecture, suitable for predicting attacks against network assets and services provided is proposed. In turn, MITRE ATT&CK Matrix is proposed as a source for attacks’ Indicators of Compromise (IoCs) development."
Study of MHD activities in the plasma of SST-1,"Steady State Superconducting Tokamak (SST-1) is a medium size Tokamak in operation at the Institute for Plasma Research, India. SST-1 has been consistently producing plasma currents in excess of 60kA, with plasma durations above 400ms and a central magnetic field of 1.5T over last few experimental campaigns of 2014. Investigation of these experimental data suggests the presence of MHD activity in the SST-1 plasma. Further analysis clearly explains the behavior of MHD instabilities observed (i.e. tearing modes with m=2, n=1), estimating the growth rate and the island width in the SST-1 plasma. Poloidal magnetic field and Toroidal magnetic field fluctuations in SST-1 are observed using Mirnov coils. Onsets of disruptions in connection with MHD activities have been correlated with other diagnostics such as ECE, Density and Hα etc. The observations have been cross compared with the theoretical calculations and are found to be in good agreement."
Study of microwave discharge at high power density conditions in diamond chemical vapor deposition reactor by optical emission spectroscopy,"The paper presents results of measurements of spatial distributions of emission intensity of various lines of argon, atomic hydrogen, C2 and CH molecules in a wide pressure range from 40 to 500 Torr in diamond chemical vapor deposition (CVD) reactor. Microwave discharge plasma was maintained in reactor in continuous mode both in hydrogen and in a mixture of hydrogen-methane with a small addition of argon for diagnostic purposes. Intensity distribution of argon emission was characterized by a pronounced maximum near the substrate in reactor, which indicates that microwave power density (MWPD) absorbed in the plasma is also concentrated mainly near the substrate. Maximum emission intensity of hydrogen atoms is located at some distance from the substrate, and maxima of spatial distributions of emission intensity of carbon-containing species C2 and CH are shifted farther away from the substrate. This leads to the fact that the shape and size of discharge, which are observed visually or with help of a camera, significantly differ from the distribution of MWPD. There is a difference in the shape of spatial distribution of argon lines emission (750.4 nm and 811.5 nm), that can be explained by a change in the mechanism for quenching the excited levels of argon in different regions of discharge. This imposes some restrictions on the use of argon for measuring characteristics of microwave discharge."
Study on nature of crossover phenomena with application to gearbox fault diagnosis,"Detrended Fluctuation Analysis (DFA) is a robust tool for uncovering long-range correlations hidden in the non-stationary data. Recently, crossover properties of the scaling-law curve obtained by DFA have been applied to diagnose gearbox faults. However, the nature of the crossover phenomena has not been well- explained. In this paper, an explanation for the nature of crossover phenomena is specifically given, which is conducive to discovering novel features for gearbox fault diagnosis. Firstly, an explicit exposition of the crossover phenomena is provided by analyzing the gearbox vibration signal. Secondly, the nature of crossover phenomena is specifically disclosed. Thirdly, the features with clear physical meaning are proposed to describe operating conditions of a gearbox. Then, to overcome the deficiency of feature extraction through visual observation, a piecewise-linear regression model is utilized to extract the features automatically. Lastly, several combinations of these features are used to classify the fault types. As a consequence, the proposed novel features are verified that they can well- distinguish the gearbox operating conditions with different fault types and severities, and deliver a better performance than the existing method depending on the sensitive index (SI)."
Success factors in developing iHeart as a patient-centric healthcare system: A multi-group analysis,"Coronary Heart Disease is the number one killer disease not only in Malaysia and Iran but also in many countries around the world. Though early diagnosis is critical for successful treatment of the disease, immediate reaction in cases of emergency is crucial significance in saving the lives of stricken. Previous studies on patient-centric healthcare indicate that both the patient’s whereabouts and the locality of the healthcare centers play a vital role at moment of crisis. Not only it is important for a patient to be able to find the nearest healthcare points, it is also necessary for the emergency centers to know the exact location of the patient seeking for help. To this end, a Location-Based Mobile Cardiac Emergency System (iHeart) is proposed, which is a patient-centric healthcare system, to monitor and track the patient viwearable device and their mobile phones. Therefore, this study aims to find healthcare professional’s opinion and to find success factors of such a system before its full implementation. A survey was conducted in the form of questionnaire in Malaysia and Iran. In order to conduct the survey, a conceptual model was proposed based on DeLone and McLean Information Success Model. A total number of 323 data was collected from both countries. The results were analyzed using Structural Equation Modeling (SEM) with AMOS software. Multi-group comparison was applied to find differences in points of view from both countries. The results of this study revealed the success factors of iHeart before the full implementation. It was found that nationality plays a moderating role between some of the success factors and success of iHeart. The success of a patient-centric healthcare system is bound to the culture of a particular nation as well as to the technological advancements, facilities, and the needs of the target users, meaning that in order for a novel healthcare system to be publicly acknowledged and utilized; target users must be classified and assessed accordingly. The findings of the present study are highly beneficial to both patients suffering from Coronary Heart Disease and the healthcare providers that seek to utilize new patient-centric healthcare systems, devices, and electronic applications to rescue patients."
Superiority of intrinsic biopolymeric constituents in briquettes of lignocellulosic crop residues over wood: A TG-diagnosis,"The paper proves that briquettes of lignocellulosic crop residues have higher activation energy levels of intrinsic biopolymers as compared to these energy levels in wood, especially in context with the hemicellulosic and cellulosic segment. Binderless briquettes made of residues from pigeon pea and soybean crops were analyzed in comparison with wood. Thermal decompositions of these biofuels were experimented by thermogravimetry under pyrolysis environment and the transitions of the thermogravimetric signals were explained. Popular isoconversional kinetics method, namely integral Ozawa-Flynn-Wall was used to evaluate and compare the activation energies. Kinetics analyzed that wood was thermally weaker than the briquettes."
Supervised dictionary learning of EEG signals for mild cognitive impairment diagnosis,"Mild Cognitive Impairment (MCI) is an intermediate stage of memory decline between normal aging and Alzheimer’s disease or other types of dementia. MCI diagnosis is instrumental in preventing Alzheimer’s; however, its manifestations are complicated (i.e. distinguishing the symptoms is not easy) and MCI is often confused with the normal consequences of aging. To have an accurate and reliable Electroencephalogram (EEG)-based screening tool for MCI diagnosis, we are developing a new supervised dictionary-learning-based analysis of EEG signals, namely Correlation-based Label Consistent K-SVD (CLC-KSVD), which would solve the non-repeatability problem of conventional K-SVD. The proposed method is applied on both time and frequency domains, i.e. 1) the extracted patches from EEG signals recorded at resting state with eyes closed, and 2) the extracted spectral features from these EEG signals. The final label for each EEG signal (in different channels and zones) is obtained by voting between the labels of the whole of time and spectral patches. The evaluation results for the EEG signals of 61 subjects illustrate that CLC-KSVD outperforms other methods (the best accuracy of 88.9% was obtained in F7, T3 channels and the left temporal zone). Furthermore, the results are investigated using the volumetric analysis of Magnetic Resonance (MR) images, indicating that the most significant difference between healthy and MCI groups were in the superior temporal and pars triangularis in the left hemisphere. Such location matching between EEG and MR images demonstrates that the results of CLC-KSVD are anatomically interpretable."
Symbolic regression of uncertainty-resilient inferential sensors for fault diagnostics⁎⁎This work was sponsored by the United Technologies Corporation Institute for Advanced Systems Engineering (UTC-IASE) of the University of Connecticut. Any opinions expressed herein are those of the authors and do not represent those of the sponsor.,"An algorithm is presented for the design of inferential sensors for fault diagnostics in thermal management systems. The algorithm uses input and output sensed system information to improve the detection and isolation of a fault by generating inferential sensors that augment the measured information to: (i) reduce the evidence of uncertainty in the inferred variables, and thus decrease false alarm and nondetection rates; and (ii) provide distinguishable responses to faults, and thus reduce reduce the rate of misdiagnoses. The novelty of the algorithm is its use of genetic programming to evolve explainable inferential sensors that maximize information criteria specific to fault diagnostics. The chosen criteria: (i) least squares regression; and (ii) Ds -optimality (calculated from the Fisher Information Matrix), leverage symbolic mathematics and automatic differentiation to obtain parametric sensitivities of the measured outputs and inferential sensors. The algorithm is included in a standard work for fault diagnostics, where its effectiveness is assessed through k-NN classification and illustrated in an application to an aircraft cross-flow plate-fin heat exchanger."
Symptom-based patient stratification in mental illness using clinical notes,"Mental illnesses are highly heterogeneous with diagnoses based on symptoms that are generally qualitative, subjective, and documented in free text clinical notes rather than as structured data. Moreover, there exists significant variation in symptoms within diagnostic categories as well as substantial overlap in symptoms between diagnostic categories. These factors pose extra challenges for phenotyping patients with mental illness, a task that has proven challenging even for seemingly well characterized diseases. The ability to identify more homogeneous patient groups could both increase our ability to apply a precision medicine approach to psychiatric disorders and enable elucidation of underlying biological mechanism of pathology. We describe a novel approach to deep phenotyping in mental illness in which contextual term extraction is used to identify constellations of symptoms in a cohort of patients diagnosed with schizophrenia and related disorders. We applied topic modeling and dimensionality reduction to identify similar groups of patients and evaluate the resulting clusters through visualization and interrogation of clinically interpretable weighted features. Our findings show that patients diagnosed with schizophrenia may be meaningfully stratified using symptom-based clustering."
Synthetic methods for the evaluation of the State of Health (SOH) of nickel-metal hydride (NiMH) batteries,"The State of Health (SOH) of a battery is important to know the maximum energy that a battery can release while is operative and to plan the correct maintenance. In this work, we have implemented a diagnostic method to evaluate the SOH of single nickel metal hydride (NiMH) cells, that have an important role in power tools applications. Single NiMH cells of 1.2V and 1.3Ah have been characterized with Electrochemical Impedance Spectroscopy (EIS) technique to evaluate the SOH with synthetic methods. Through the study of an equivalent circuit model, we determine that three free parameters synthesize the information contained in an impedance spectrum. A new simplification allows us to construct a diagnostic diagram with only two degree of freedom. A mathematical approach based on the Dempster–Schafer Theory of Evidence has been implemented to interpret the diagnostic diagram. Combining the points obtained by the impedance spectra (IS) at different State of Charge (SOC) and SOH, the Theory of Evidence can improve the estimation of the SOH iteratively, a great advantage compared to the classic Theory of Probability."
Temperature and concentration measurements in a high-pressure gasifier enabled by cepstral analysis of dual frequency comb spectroscopy,"High-pressure gasification processes are important for conversion of solid materials into gaseous fuels and other chemicals. Laser absorption diagnostics are an important means to study these processes, but are challenging to implement due to the extreme temperatures and pressures present in the system. Here, we combine broadband high-resolution dual-comb spectroscopy with an advanced spectral absorption database and a new means for baseline-free absorption spectroscopy analysis to enable measurements of temperature and water vapor concentration in the core of an entrained flow gasifier operating at up to 1700 K and 15 bar. The dual-comb spectrometer measures the absorption of water vapor from 6800 to 7150 cm−1 with a point spacing of 0.0067 cm−1. The bandwidth is helpful for resolving the complex, congested absorption fingerprint of water vapor that is used to determine the species concentrations and temperature. We interpret the spectrum with absorption models based on a database measured under carefully controlled high-temperature conditions with the dual-comb spectrometer. The database includes the pressure broadening, shift, and temperature dependence of these parameters for water vapor in argon, which is the gasifier bath gas. Finally, fitting the absorption model to the data is enabled by modified free induction decay analysis, which is an approach for quantitatively obtaining species and temperature information without determining the baseline intensity of the spectrometer. The baseline-free approach is crucial to success in this environment, where there are no non-absorbing regions of the spectrum to anchor the normalization of the laser intensity as in traditional direct absorption spectroscopy. We demonstrate good agreement with temperatures measured on the reactor core via optical pyrometry, and show that water vapor concentrations in the reactor core did not reach the expected system set points during some experiments."
Temperature sensing during Raman spectroscopy of lead white films in different purity grades and boundary conditions,"In the field of laser-based diagnostics and treatment techniques for artwork conservation it is known that lead white, wide use as art pigment in the past, shows low photothermal stability upon laser irradiation. Thermal alterations of lead white paint films are often observed during Raman spectroscopy, although the origin of heating has not yet been exhaustively explained. Here, we approach the interpretation of this phenomenon through the preparation of high- (analytes) and low-grade (art pigment) lead white films, thorough compositional and structural characterizations, measurement of the optical parameters, and online temperature measurements during Raman spectroscopy excited at 1064 nm. Spectra and associated temperature profiles were achieved using a system equipped with an online thermal sensor. The data collected show the crucial importance of the degree of purity in heat generation upon laser exposure. Due to their higher content of Fe2O3 inclusions, low-grade films (150−250 ppm against 50 ppm of high-grade) showed absorption coefficients five times higher than high-grades films, and temperature rises up to 80−100 °C upon 207 W/cm2 irradiation and 25 s exposure. Optical absorption and heating of low-grade films determined weaker signals and higher backgrounds in the Raman spectra, whose origin was ascribed to luminescence of Fe2O3 inclusions. Moreover, the spectral and thermal influence of the substrate was also investigated thus achieving a complete picture of the photothermal behavior of lead-white art pigments during Raman spectroscopy."
Temporal-Fault Diagnosis for Critical-Decision Making in Discrete-Event Systems,"Since its appearance in AI, model-based diagnosis is intrinsically set-oriented. Given a sequence of observations, the diagnosis task generates a set of diagnoses, or candidates, each candidate complying with the observations. What all the approaches in the literature have in common is that a candidate is invariably a set of faulty elements (components, events, or otherwise). In this paper, we consider a posteriori diagnosis of discrete-event systems (DESs), which are described by networks of components that are modeled as communicating automata. The diagnosis problem consists in generating the candidates involved in the trajectories of the DES that conform with a given temporal observation. Oddly, in the literature on diagnosis of DESs, a candidate is still a set of faulty events, despite the temporal dimension of trajectories. In our view, when dealing with critical domains, such as power networks or nuclear plants, set-oriented diagnosis may be less than optimal in explaining the supposedly abnormal behavior of the DES, owing to the lack of any temporal information relevant to faults, along with the inability to discriminate between single and multiple occurrences of the same fault. Embedding temporal information in candidates may be essential for critical-decision making. This is why a temporal-oriented approach is proposed for diagnosis of DESs, where candidates are sequences of faults. This novel perspective comes with the burden of unbounded candidates and infinite collections of candidates, though. To cope with, a notation based on regular expressions on faults is adopted. The diagnosis task is supported by a temporal diagnoser, a flexible data structure that can grow over time based on new observations and domain-dependent scenarios."
Tension prediction for straight cables based on effective vibration length with a two-frequency approach,"Cost-efficient and theoretically simple vibration-based analyses are widely used to assess the axial forces of tensioned members. In the determination of cable forces, the applicability of an explicit-type formula depends on the suitability of the adopted theories and the accuracy of the known parameters. For straight cables, the sectional rigidity is generally assumed to be a known quantity and the end conditions are presumptive. Nevertheless, given data of these parameters could be vague or unreliable. Methodologies which omit the requirement of a priori data are thus preferable in practice. This paper illustrates an effective methodology for the tension prediction of cables with presumptive end conditions by making use of multiple measured mode frequencies. Simple yet accurate formulas are obtained from the exact expressions based on the Bernoulli beam theory. Due to the fact that this approach does not require the knowledge of the flexural rigidity of the testing cable and it can be performed using only a single sensor, the method is suitable to be implemented in a quick diagnosis scheme. Present research emphasizes developing an analytic basis to effectively correlate the proposed formulas with the concept of the effective vibration lengths. This concept has been widely recognized as a useful idea to interpret the dynamics of cable vibrations."
The actual measurement and analysis of transformer winding deformation fault degrees by FRA using mathematical indicators,"Since the power transformers are indispensable in power systems, their reliable operation are of significance. Winding deformation is one of the most common failures within transformers. At present, many methodologies have been proposed to detect these faults, and frequency response analysis (FRA) technique is most frequently used because of its low cost, convenience, simplicity and effectiveness. However, there is no standard and reliable code to interpret the mechanical deformations from FRA traces as so far. In this study, an actual transformer experimental platform was used to simulate variable winding deformation faults in order to standardize the interpretation of winding fault degree from FRA data. The measured FRA raw data are processed from the perspective of statistics, and some numerical indices are found to be more suitable for the diagnosis of transformer winding deformation degree."
The Bilingual Stroop Test from the View of the Information Images Theory,"The current research paper introduces the basic principles of the Information Images theory and mathematical model created through it. In order to confirm the theory experimentally, the bilingual Stroop test was used. The results of the test are interpreted through the introduced theory, then they are compared with the results of computer modeling on its basis. The authors demonstrate, that through the Information Images Theory it is possible not only to explain a number of cognitive processes of human mind, but also to make a prognosis of their dynamics in a number of isolated incidents."
The challenge of cerebral magnetic resonance imaging in neonates: A new method using mathematical morphology for the segmentation of structures including diffuse excessive high signal intensities.,"Preterm birth is a multifactorial condition associated with increased morbidity and mortality. Diffuse excessive high signal intensity (DEHSI) has been recently described on T2-weighted MR sequences in this population and thought to be associated with neuropathologies. To date, no robust and reproducible method to assess the presence of white matter hyperintensities has been developed, perhaps explaining the current controversy over their prognostic value. The aim of this paper is to propose a new semi-automated framework to detect DEHSI on neonatal brain MR images having a particular pattern due to the physiological lack of complete myelination of the white matter. A novel method for semi- automatic segmentation of neonatal brain structures and DEHSI, based on mathematical morphology and on max-tree representations of the images is thus described. It is a mandatory first step to identify and clinically assess homogeneous cohorts of neonates for DEHSI and/or volume of any other segmented structures. Implemented in a user-friendly interface, the method makes it straightforward to select relevant markers of structures to be segmented, and if needed, apply eventually manual corrections. This method responds to the increasing need for providing medical experts with semi-automatic tools for image analysis, and overcomes the limitations of visual analysis alone, prone to subjectivity and variability. Experimental results demonstrate that the method is accurate, with excellent reproducibility and with very few manual corrections needed. Although the method was intended initially for images acquired at 1.5T, which corresponds to the usual clinical practice, preliminary results on images acquired at 3T suggest that the proposed approach can be generalized."
The EU DEMO equatorial outboard limiter — Design and port integration concept,"The equatorial outboard limiters (also called outboard midplane limiters (OMLs)) are an essential part of the DEMO wall protection concept. Limiters are foreseen in different areas of the DEMO first wall, namely in the equatorial ports, on the high-field side, in vertical ports and additional protection limiters between equatorial and lower ports. The limiters shall prevent the plasma to touch the first wall of the breeding blankets during all plasma transients. The port integration concept of the OMLs, used for plasma ramp-up/-down, is explained including (i) thermal, structural and electromagnetic loads, (ii) neutronic requirements and related material properties, (iii) remote handling considerations, (iv) space and mass constraints and (v) the required alignment precision to allow equal distribution of the heat exposure amongst the individual of the plasma facing (PFC) limiter components. While the hot fusion plasma during ramp-up is impinging directly on the limiter, its PFC components temperature is rising and can be measured by means of either thermocouples or by infrared (IR) thermography an estimation of the heat flux on the contact point can be made. This is the basis for the proposed alignment strategy."
The four dimensions of contestable AI diagnostics - A patient-centric approach to explainable AI,"The problem of the explainability of AI decision-making has attracted considerable attention in recent years. In considering AI diagnostics we suggest that explainability should be explicated as ‘effective contestability’. Taking a patient-centric approach we argue that patients should be able to contest the diagnoses of AI diagnostic systems, and that effective contestation of patient-relevant aspect of AI diagnoses requires the availability of different types of information about 1) the AI system’s use of data, 2) the system’s potential biases, 3) the system performance, and 4) the division of labour between the system and health care professionals. We justify and define thirteen specific informational requirements that follows from ‘contestability’. We further show not only that contestability is a weaker requirement than some of the proposed criteria of explainability, but also that it does not introduce poorly grounded double standards for AI and health care professionals’ diagnostics, and does not come at the cost of AI system performance. Finally, we briefly discuss whether the contestability requirements introduced here are domain-specific."
The four dimensions of contestable AI diagnostics - A patient-centric approach to explainable AI,"The problem of the explainability of AI decision-making has attracted considerable attention in recent years. In considering AI diagnostics we suggest that explainability should be explicated as ‘effective contestability’. Taking a patient-centric approach we argue that patients should be able to contest the diagnoses of AI diagnostic systems, and that effective contestation of patient-relevant aspect of AI diagnoses requires the availability of different types of information about 1) the AI system’s use of data, 2) the system’s potential biases, 3) the system performance, and 4) the division of labour between the system and health care professionals. We justify and define thirteen specific informational requirements that follows from ‘contestability’. We further show not only that contestability is a weaker requirement than some of the proposed criteria of explainability, but also that it does not introduce poorly grounded double standards for AI and health care professionals’ diagnostics, and does not come at the cost of AI system performance. Finally, we briefly discuss whether the contestability requirements introduced here are domain-specific."
The gas supply and gas inlet control systems of the fusion experiment Wendelstein 7-X,"After a detailed commissioning process the fusion experiment Wendelstein 7-X (W7-X) was ready to operate in December 2015. The gas supply system and the gas inlet system are two important technical systems of the W7-X device, offering a wide spectrum of functions for plasma vessel conditioning, and plasma operation as well as for diagnostic and calibration purposes. A brief overview is given of the structure of both gas supply and gas inlet systems. The design of the control systems of the gas supply and the gas inlet are introduced and described. The application of the W7-X gas inlet system is explained during an experiment discharge."
The influence of model order reduction on the computed fractional flow reserve using parameterized coronary geometries,"Computational fluid dynamics (CFD) models combined with patient-specific imaging data are used to non-invasively predict functional significance of coronary lesions. This approach to predict the fractional flow reserve (FFR) is shown to have a high diagnostic accuracy when comparing against invasively measured FFR. However, one of the main drawbacks is the high computational effort needed for preprocessing and computations. Hence, uncertainty quantification may become unfeasible. Reduction of complexity is desirable, computationally inexpensive models with high diagnostic accuracy are preferred. We present a parametric comparison study for three types of CFD models (2D axisymmetric, Semi-3D and 3D) in which we study the impact of model reduction on three models on the predicted FFR. In total 200 coronary geometries were generated for seven geometrical characteristics e.g. stenosis severity, stenosis length and vessel curvature. The effect of time-averaged flow was investigated using unsteady, mean steady and a root mean square (RMS) steady flow. The 3D unsteady model was regarded as reference model. Results show that when using an unsteady or RMS flow, predicted FFR hardly varies between models contrary to using average flows. The 2D model with RMS flow has a high diagnostic accuracy (0.99), reduces computational time by a factor 162,000 and the introduced model error is well below the clinical relevant differences. Stenosis severity, length, curvature and tapering cause most discrepancies when using a lower order model. An uncertainty analysis showed that this can be explained by the low variability that is caused by variations in stenosis asymmetry."
The LIPAc beam dump,"The International Fusion Materials Irradiation Facility (IFMIF) aims to provide an accelerator-based, D-Li neutron source to produce high energy neutrons at sufficient intensity and irradiation volume for fusion materials qualification. The LIPAc is a 125 mA 9 MeV continuous wave deuteron accelerator whose components are under construction mainly in Europe, which is being installed in Rokkasho (Japan) with the purpose of validating the IFMIF accelerator design. The beam generated by the LIPAc accelerator will be stopped by a copper cone (2.5 m long, 6.8° angle), cooled by water flowing at high velocity along its outer surface. This piece is surrounded by a shield made of iron and low Z materials for attenuating the neutron and gamma radiation originated by the interaction of the deuterons with the copper. It incorporates dedicated diagnostics for beam dump monitoring: accelerometers for detection of localized heating due to incorrect alignment of the beam and ionization chambers, which ensure that the deuteron beam footprint remains within the beam dump design limits. A lead shutter has been designed to be inserted in the beam tube during beam-off periods to stop the gamma radiation from the activated copper cone escaping through the beam tube, allowing access inside the accelerator vault. The junction of the beam dump to the beam tube has a special design to allow its remote disconnection, enabling the end of life decommissioning operations of the facility. The design and material selection of the beam dump and neighboring elements are driven by a maintenance-free requirement after a short period of operations, as the cartridge activation precludes any maintenance activities in the beam dump and neighboring elements downstream the lead shutter. This paper describes the design and manufacturing of the beam dump and related elements explaining the interrelations between them and the reasons behind their main features."
The mechanistic link between macrozones and dwell fatigue in titanium alloys,"This paper addresses the role of macrozone crystallography and morphology in dwell fatigue in titanium alloy Ti-6Al-4V. Until now, the relationship between macrozones and dwell fatigue damage has remained mechanistically uncertain, but this paper establishes a mechanistic link between macrozones and dwell fatigue damage, and explains the preference for dwell facets to be sub-surface. It also outlines the criteria which are important in a potential definition of a macrozone (or microtextured region). High aspect ratio (>~4) macrozones are particularly damaging when their long-axes are orientated near-normal to the principal loading direction, such that their basal planes are oriented to within ~15° to the principal stress direction. These criteria may be useful in guiding the development of a diagnostic experimental measurement tool (based on EBSD or ultrasonics for example) for macrozone detection in components."
The natural language explanation algorithms for the lung cancer computer-aided diagnosis system,"Two algorithms for explaining decisions of a lung cancer computer-aided diagnosis system are proposed. Their main peculiarity is that they produce explanations of diseases in the form of special sentences via natural language. The algorithms consist of two parts. The first part is a standard local post-hoc explanation model, for example, the well-known LIME, which is used for selecting important features from a special feature representation of the segmented lung suspicious objects. This part is identical for both algorithms. The second part is a model which aims to connect selected important features and to transform them to explanation sentences in natural language. This part is implemented differently for both algorithms. The training phase of the first algorithm uses a special vocabulary of simple phrases which produce sentences and their embeddings. The second algorithm significantly simplifies some parts of the first algorithm and reduces the explanation problem to a set of simple classifiers. The basic idea behind the improvement is to represent every simple phrase from vocabulary as a class of the “sparse” histograms. An implementation of the second algorithm is shown in detail."
The natural language explanation algorithms for the lung cancer computer-aided diagnosis system,"Two algorithms for explaining decisions of a lung cancer computer-aided diagnosis system are proposed. Their main peculiarity is that they produce explanations of diseases in the form of special sentences via natural language. The algorithms consist of two parts. The first part is a standard local post-hoc explanation model, for example, the well-known LIME, which is used for selecting important features from a special feature representation of the segmented lung suspicious objects. This part is identical for both algorithms. The second part is a model which aims to connect selected important features and to transform them to explanation sentences in natural language. This part is implemented differently for both algorithms. The training phase of the first algorithm uses a special vocabulary of simple phrases which produce sentences and their embeddings. The second algorithm significantly simplifies some parts of the first algorithm and reduces the explanation problem to a set of simple classifiers. The basic idea behind the improvement is to represent every simple phrase from vocabulary as a class of the “sparse” histograms. An implementation of the second algorithm is shown in detail."
The role of computerized diagnostic proposals in the interpretation of the 12-lead electrocardiogram by cardiology and non-cardiology fellows,"Introduction Most contemporary 12-lead electrocardiogram (ECG) devices offer computerized diagnostic proposals. The reliability of these automated diagnoses is limited. It has been suggested that incorrect computer advice can influence physician decision-making. This study analyzed the role of diagnostic proposals in the decision process by a group of fellows of cardiology and other internal medicine subspecialties. Materials and methods A set of 100 clinical 12-lead ECG tracings was selected covering both normal cases and common abnormalities. A team of 15 junior Cardiology Fellows and 15 Non-Cardiology Fellows interpreted the ECGs in 3 phases: without any diagnostic proposal, with a single diagnostic proposal (half of them intentionally incorrect), and with four diagnostic proposals (only one of them being correct) for each ECG. Self-rated confidence of each interpretation was collected. Results Availability of diagnostic proposals significantly increased the diagnostic accuracy (p<0.001). Nevertheless, in case of a single proposal (either correct or incorrect) the increase of accuracy was present in interpretations with correct diagnostic proposals, while the accuracy was substantially reduced with incorrect proposals. Confidence levels poorly correlated with interpretation scores (rho≈2, p<0.001). Logistic regression showed that an interpreter is most likely to be correct when the ECG offers a correct diagnostic proposal (OR=10.87) or multiple proposals (OR=4.43). Conclusion Diagnostic proposals affect the diagnostic accuracy of ECG interpretations. The accuracy is significantly influenced especially when a single diagnostic proposal (either correct or incorrect) is provided. The study suggests that the presentation of multiple computerized diagnoses is likely to improve the diagnostic accuracy of interpreters."
The sensitizing effects of NO2 and NO on methane low temperature oxidation in a jet stirred reactor,"The oxidation of neat methane (CH4) and CH4 doped with NO2 or NO in argon has been investigated in a jet-stirred reactor at 107 kPa, temperatures between 650 and 1200 K, with a fixed residence time of 1.5 s, and for different equivalence ratios (Φ), ranging from fuel-lean to fuel-rich conditions. Four different diagnostics have been used: gas chromatography (GC), chemiluminescence NOx analyzer, continuous wave cavity ring-down spectroscopy (cw-CRDS) and Fourier transform infrared spectroscopy (FTIR). In the case of the oxidation of neat methane, the onset temperature for CH4 oxidation was above 1025 K, while it is shifted to 825 K with the addition of NO2 or NO, independently of equivalence ratio, indicating that the addition of NO2 or NO highly promotes CH4 oxidation. The consumption rate of CH4 exhibits a similar trend with the presence of both NO2 and NO. The amount of produced HCN has been quantified and a search for HONO and CH3NO2 species has been attempted. A detailed kinetic mechanism, derived from POLIMI kinetic framework, has been used to interpret the experimental data with a good agreement between experimental data and model predictions. Reaction rate and sensitivity analysis have been conducted to illustrate the kinetic regimes. The fact that the addition of NO or NO2 seems to have similar effects on promoting CH4 oxidation can be explained by the fact that both species are involved in a reaction cycle interchanging them and whose result is 2CH3 + O2 = 2CH2O + 2H. Additionally, the direct participation of NO2 in the NO2 + CH2O = HONO + HCO reaction has a notable accelerating effect on methane oxidation."
The Smart Extension approach for securing industrial control systems,"Industrial Control Devices are one of the major targets for hackers due to their exposure to threats. The principle of ""air gaps"" (disconnecting the Industrial Control Network from the operational networks) is not anymore feasible in a connected world. In this paper, a host anomaly detection system for Critical Infrastructures networks is presented. The device, called Smart Extension, also implements a filtering strategy in order to secure a single host reacting to cyber threats. Therefore, it is positioned in the network between PLC (Programmable Logic Controller) and the SCADA (Supervisory Control and Data Acquisition) control centre, more precisely just in front of the PLC. Finally, experimental results are shown in order to explain the internal working procedures in a possible case study."
The valve motion characteristics of a reciprocating pump,"In previous studies on a reciprocating pump, the state, behavior simulation, or experimental analysis of the valve was seldom reported. In the paper, taking a triplex single-acting reciprocating pump as the research object, we established an experimental system for testing valve disc׳s motion parameters to directly acquire the valve disc motion parameters (acceleration, velocity, and displacement) under actual conditions. Moreover, testing results were compared with the calculation results obtained according to U. Adolph Theory and Approximation Theory. In Approximation Theory, the valve disc motion was not fully considered, thus leading to the large deviation from the actual situation. Compared with the Approximation Theory, U. Adolph Theory is more suitable for the determination of valve disc motion parameters during different strokes and can explain the jumping and hysteresis phenomena of the valve well. A new pump testing method and an experimental system were proposed to provide a new study approach for valve design theory, disc damage mechanism, and pump failure diagnosis."
The virtual doctor: An interactive clinical-decision-support system based on deep learning for non-invasive prediction of diabetes,"Artificial intelligence (AI) will pave the way to a new era in medicine. However, currently available AI systems do not interact with a patient, e.g., for anamnesis, and thus are only used by the physicians for predictions in diagnosis or prognosis. However, these systems are widely used, e.g., in diabetes or cancer prediction. In the current study, we developed an AI that is able to interact with a patient (virtual doctor) by using a speech recognition and speech synthesis system and thus can autonomously interact with the patient, which is particularly important for, e.g., rural areas, where the availability of primary medical care is strongly limited by low population densities. As a proof-of-concept, the system is able to predict type 2 diabetes mellitus (T2DM) based on non-invasive sensors and deep neural networks. Moreover, the system provides an easy-to-interpret probability estimation for T2DM for a given patient. Besides the development of the AI, we further analyzed the acceptance of young people for AI in healthcare to estimate the impact of such a system in the future."
The VMTES: Application to the structural health monitoring and diagnosis of rotating machines,"It is a challenging task to perform the non-linear system state recognition and safety monitoring under strong noise and complex excitation, to tackle this problem, we innovatively propose the variational mode decomposition multiscale holographic transfer entropy statistics (VMTES) method based on the energy transfer relationship between non-linear system signals. The VMTES is a method that measures the information flow direction and coupling degree between non-linear systems, which can precisely measure the slight changes of energy transfer of a mechanical system, accurately assess the slight mutation of dynamical behaviors and status change of a mechanical system and therefore realize fault location and quantification of the non-linear rotating machinery system. The damage severity and direction of the measure point can be precisely described with the VMTES damage assessment indicator, providing the reliable basis for the structural health monitoring and fault diagnosis of the mechanical system. By applying the result to the state recognition of a chaotic system and the structural health monitoring of a rotating machinery system, we can see from the experimental results that the VMTES can effectively detect the fault of gears and rolling bearings under several working conditions. Based on the experiment, we also explain how this method simultaneously locate and quantify the non-linear vibration caused by the fault."
Three-dimensional morphology and spherical growth mechanisms of electrical trees in silicone rubber,"Electrical tree is an important reason of insulation failure in silicone rubber (SIR) which affects the SIR insulated electrical equipment reliability seriously. We used a confocal laser scanning microscope (CLSM) and fluorescence microscopy for the first time to obtain fluorescence three-dimensional (3D) electrical tree images. Meanwhile, other observation methods were also used to validate its microstructure. Based on the results and partial discharge (PD) theory, spherical growth pattern of electrical trees in SIR is found, which not only corresponds to the observation results, but also explains growing procedure under the PD."
Time-distanced gates in long short-term memory networks,"The Long Short-Term Memory (LSTM) network is widely used in modeling sequential observations in fields ranging from natural language processing to medical imaging. The LSTM has shown promise for interpreting computed tomography (CT) in lung screening protocols. Yet, traditional image-based LSTM models ignore interval differences, while recently proposed interval-modeled LSTM variants are limited in their ability to interpret temporal proximity. Meanwhile, clinical imaging acquisition may be irregularly sampled, and such sampling patterns may be commingled with clinical usages. In this paper, we propose the Distanced LSTM (DLSTM) by introducing time-distanced (i.e., time distance to the last scan) gates with a temporal emphasis model (TEM) targeting at lung cancer diagnosis (i.e., evaluating the malignancy of pulmonary nodules). Briefly, (1) the time distance of every scan to the last scan is modeled explicitly, (2) time-distanced input and forget gates in DLSTM are introduced across regular and irregular sampling sequences, and (3) the newer scan in serial data is emphasized by the TEM. The DLSTM algorithm is evaluated with both simulated data and real CT images (from 1794 National Lung Screening Trial (NLST) patients with longitudinal scans and 1420 clinical studied patients). Experimental results on simulated data indicate the DLSTM can capture families of temporal relationships that cannot be detected with traditional LSTM. Cross-validation on empirical CT datasets demonstrates that DLSTM achieves leading performance on both regularly and irregularly sampled data (e.g., improving LSTM from 0.6785 to 0.7085 on F1 score in NLST). In external-validation on irregularly acquired data, the benchmarks achieved 0.8350 (CNN feature) and 0.8380 (with LSTM) on AUC score, while the proposed DLSTM achieves 0.8905. In conclusion, the DLSTM approach is shown to be compatible with families of linear, quadratic, exponential, and log-exponential temporal models. The DLSTM can be readily extended with other temporal dependence interactions while hardly increasing overall model complexity."
Time-resolved in situ measurements and predictions of plasma-assisted methane reforming in a nanosecond-pulsed discharge,"This study investigates the plasma properties and chemical kinetics of plasma-assisted methane reforming in a He diluted nanosecond-pulsed plane-to-plane dielectric barrier discharge (ns-DBD) through the combination of time-resolved in situ laser diagnostics and a 1-D numerical model. Plasma-assisted fuel reforming kinetic mechanisms have predominantly been evaluated on the basis of matching reactant conversion and syngas production to steady-state measurements, which cannot describe the full range of chemistry and physics necessary to validate the model. It was found that adding 1% CH4 to a pure He ns-DBD led to a faster breakdown along the rising edge of the applied voltage pulse, thereby lowering the reduced electric field (E/N), electron number density, and electron temperature. Further addition of CH4 did not continue to alter the E/N in the model. Laser absorption spectroscopy was used to measure gas temperature, C2H2, H2O, and CH2O in a CH4/CO2/He discharge to serve as validation targets for the predicted reaction pathways. CH2O was predicted within 25% of the measured value, while H2O and C2H2 were under-predicted by a factor of two and three, respectively. From path flux analysis, the major pathway for CH2O formation was through the reaction between CH3 and O, while C2H2 formation had multi-step pathways that relied on ions like CH3+ and C2H5+. The path flux analysis also shows that CH2 is a significant intermediate for production of both CH2O and C2H2, and increased CH2 concentration could improve model predictions. The results show that the use of reaction rate constants with lower uncertainties and inclusion of He2+ are needed to improve the predictions. Finally, varying the ”equivalence ratio”, defined by the CH4 dry reforming reaction to H2 and CO, from 0.5 to 2 was shown to have a weak effect on measured product species and experimental trends were explained based on pathways extracted from the model."
Time-stability of a Single-crystal Diamond Detector for fast neutron beam diagnostic under alpha and neutron irradiation,"Single-crystal Diamond Detectors (SDDs), due to their good charge carrier transport properties, low leakage and therefore good energy resolution, are good candidates for fast neutron measurement on pulsed spallation sources and fusion plasma experiments. Moreover, diamonds are known to be resistant to neutron irradiation. Nevertheless, measurements show transient effects during irradiation with ionizing particles, as the alpha particle calibration sources. The decrease of the detector counting rate of a counting chain and the pulse height are interpreted as due to a charge trapping inside the detector, which modifies the drift electric field. These instabilities are strongly dependent on the specific type of the interaction. Measurements have been carried out with both alpha particles in the laboratory and neutrons at the ISIS neutron spallation source. We show that these polarization effects are not permanent: the detector performances can be restored by simply inverting the detector bias high voltage. Prime Novelty Statement The measurements described in the paper were performed in order to study the polarization effect in Single-crystal Diamond Detector. This effect was observed under alpha particle and neutron irradiation. With the Transient Current Technique an interpretation of the effect is given."
Time-varying mesh stiffness calculation of spur gears with spalling defect,"Considering the effects of extended tooth contact (ETC), revised fillet-foundation stiffness under double-tooth engagement region, nonlinear contact stiffness and tooth spalling defect, an analytical model for time-varying mesh stiffness (TVMS) calculation of spur gears is established. In addition, the analytical model is also verified by comparing the TVMS under different spalling widths, lengths and locations with that obtained from finite element method. The results show that gear mesh stiffness decreases sharply with the increase of spalling width, especially during the single-tooth engagement; the spalling length only has an effect on the beginning and ending of gear mesh stiffness reduction; the spalling location can affect the range of gear mesh stiffness reduction, and the range will reduce when the spalling location is close to the addendum. This study can provide a theoretical basis for spalling defect diagnosis."
Tissue-specific and interpretable sub-segmentation of whole tumour burden on CT images by unsupervised fuzzy clustering,"Background: Cancer typically exhibits genotypic and phenotypic heterogeneity, which can have prognostic significance and influence therapy response. Computed Tomography (CT)-based radiomic approaches calculate quantitative features of tumour heterogeneity at a mesoscopic level, regardless of macroscopic areas of hypo-dense (i.e., cystic/necrotic), hyper-dense (i.e., calcified), or intermediately dense (i.e., soft tissue) portions. Method: With the goal of achieving the automated sub-segmentation of these three tissue types, we present here a two-stage computational framework based on unsupervised Fuzzy C-Means Clustering (FCM) techniques. No existing approach has specifically addressed this task so far. Our tissue-specific image sub-segmentation was tested on ovarian cancer (pelvic/ovarian and omental disease) and renal cell carcinoma CT datasets using both overlap-based and distance-based metrics for evaluation. Results: On all tested sub-segmentation tasks, our two-stage segmentation approach outperformed conventional segmentation techniques: fixed multi-thresholding, the Otsu method, and automatic cluster number selection heuristics for the K-means clustering algorithm. In addition, experiments showed that the integration of the spatial information into the FCM algorithm generally achieves more accurate segmentation results, whilst the kernelised FCM versions are not beneficial. The best spatial FCM configuration achieved average Dice similarity coefficient values starting from 81.94±4.76 and 83.43±3.81 for hyper-dense and hypo-dense components, respectively, for the investigated sub-segmentation tasks. Conclusions: The proposed intelligent framework could be readily integrated into clinical research environments and provides robust tools for future radiomic biomarker validation."
Towards a user-friendly sleep staging system for polysomnography part I: Automatic classification based on medical knowledge,"Manual sleep scoring is a time-consuming task that requires a high level of medical expertise. For this reason, a number of automatic sleep scoring algorithms have recently been implemented. However, their use by physicians remains limited for various reasons: a lack of transparency of the approach used, insufficient heterogeneity among the patients used for testing, or a lack of practicality. This paper presents a system for facilitated sleep scoring that will overcome these limitations. The proposed system, a user-friendly tool based on electrophysiological channels, was trained and tested on large datasets of 300 and 100 distinct recordings from patients with various sleep disorders. The method replicates the manual sleep scoring process, in accordance with the American Academy of Sleep Medicine (AASM) guidelines and generates patient-dependent sleep scoring (using the SATUD system). For an improved level of precision and confidence with regard to scoring, our approach also provides a table that gives indications about the confidence level of the algorithm when scoring sleep. In contrast to recent deep learning approaches, the algorithms used were chosen for their resilience and as they are easy to understand. Medical knowledge was included in the process as much as possible. Results showed that the system is consistent with manual scoring (mean Cohen's Kappa of 0.69 and accuracy rate of 77.8%). It proves that a facilitated interpretation of the model, crucial in such fields as sleep diagnosis, can be provided when using automatic tools. This new system thereby generates sleep scoring decision support tools, which should easily contribute to significant time-saving and help sleep specialists to perform sleep diagnosis."
Towards effective machine learning in medical imaging analysis: A novel approach and expert evaluation of high-grade glioma ‘ground truth’ simulation on MRI,"Purpose/objective(s) Gliomas are uniformly fatal brain tumours with significant neurological and quality of life detriment to patients. Improvement in outcomes has remained largely unchanged in nearly 20 years. MRI (magnetic resonance imaging) is often used in diagnosis and management. Machine learning analyses of large-scale MRI data are pivotal in advancing the diagnosis, management and improve outcomes in neuro-oncology. A common challenge to robust machine learning approaches is the lack of large ‘ground truth’ datasets in supervised learning for building classification and prediction models. The creation of these datasets relies on human-expert input and is time-consuming and subjective error-prone, limiting effective machine learning applications. Simulation of mechanistic aspects such as geometry, location and physical properties of brain tumours can generate large-scale ground-truth datasets allowing for comparison of analysis techniques in clinical applications. We aimed to develop a transparent and convenient method for building ‘ground truth’ presentations of simulated glioma lesions on anatomical MRI. Materials/methods The simulation workflow was created using the Feature Manipulation Engine (FME®), a data integration platform specializing in the spatial data processing. By compiling and integrating FME’s functions to read, integrate, transform, validate, save, and display MRI data, and experimenting with ways to manipulate the parameters concerning location, size, shape, and signal intensity with the presentations of glioma, we were able to generate simulated appearances of high-grade gliomas on gadolinium-based high-resolution 3D T1-weighted MRI (1 mm3). Data of patients with canonical high-grade tumours were used as real-world tumours for validating the accuracy of the simulation. Twenty raters who are experienced with brain tumour interpretation on MRI independently completed a survey, designed to distinguish simulated and real-world brain tumours. Sensitivity and specificity were calculated for assessing the performance of the approach with the binary classification of simulated vs real-world tumours. Correlation and regression were used in run time analysis, assessing the software toolset’s efficiency in producing different numbers of simulated lesions. Differences in the group means were examined using the non-parametric Kruskal-Wallis test. Results The simulation method was developed as an interpretable and useful workflow for the easy creation of tumour simulations and incorporation into 3D MRI. A linear increase in the running time and memory usage was observed with an increasing number of generated lesions. The respondents' accuracy rate ranged between 33.3 and 83.3 %. The sensitivity and specificity were low for a human expert to differentiate simulated lesions from real gliomas (0.43 and 0.58) or vice versa (0.65 and 0.62). The mean scores ranking the real-world gliomas did not differ between the simulated and real tumours. Conclusion The reliable and user-friendly software method can allow for robust simulation of high-grade glioma on MRI. Ongoing research efforts include optimizing the workflow for generating glioma datasets as well as adapting it to simulating additional MRI brain changes."
Towards localization of malignant sites of asymmetry across bilateral mammograms,"Background and objectives The analysis of patterns of asymmetry between the left and right mammograms of a patient can provide meaningful insights into the presence of an underlying tumor in its early stage. However, the identification of breast cancer by investigating bilateral asymmetry is difficult to perform due to the indistinct and borderline nature of the asymmetric signs as they appear on mammograms. Methods In this study, to increase the positive-predictive value of asymmetry in mammographic screening, a novel computerized approach for the automatic localization of malignant sites of asymmetry in mammograms is proposed. The sites of anatomical correspondence between the right and left regions of each radiographic projection were extracted by means of two bilateral masking procedures, inspired by radiologists’ criteria in interpreting mammograms and based on the use of detected landmarking structures. Relative variations of spatial patterns of intensity values and of orientations of directional components within each site were quantified by combining multidirectional Gabor filters and indices of structural similarity. The localization of the sites of malignant asymmetry was performed by coupling two quadratic discriminant analysis classifiers, one for each masking procedure, that assigned the likelihood of malignancy to each site of correspondence. Results The performance of the proposed method was assessed on 94 mammographic images from two publicly available databases and containing at least one asymmetric site. Sensitivity, specificity and balanced accuracy levels of 0.83 (0.09), 0.75 (0.06), and 0.79 (0.04), respectively were obtained in the classification of malignant asymmetric sites vs benign/normal sites using cross-validation. In addition, a further blind test on a dataset of Full Field Digital Mammograms achieved levels of sensitivity, specificity, and balanced accuracy of 0.86, 0.65, and 0.75, respectively. Conclusions The achieved performance indicates that the proposed system is effective in localizing sites of malignant asymmetry and it is expected to improve computer-aided diagnosis of breast cancer."
Towards the swift prediction of the remaining useful life of lithium-ion batteries with end-to-end deep learning,"This paper presents the first full end-to-end deep learning framework for the swift prediction of lithium-ion battery remaining useful life. While lithium-ion batteries offer advantages of high efficiency and low cost, their instability and varying lifetimes remain challenges. To prevent the sudden failure of lithium-ion batteries, researchers have worked to develop ways of predicting the remaining useful life of lithium-ion batteries, especially using data-driven approaches. In this study, we sought a higher resolution of inter-cycle aging for faster and more accurate predictions, by considering temporal patterns and cross-data correlations in the raw data, specifically, terminal voltage, current, and cell temperature. We took an in-depth analysis of the deep learning models using the uncertainty metric, t-SNE of features, and various battery related tasks. The proposed framework significantly boosted the remaining useful life prediction (25X faster) and resulted in a 10.6% mean absolute error rate."
Transforming the ASDEX Upgrade discharge control system to a general-purpose plasma control platform,"The ASDEX Upgrade Discharge Control System DCS is a modern and mature product, originally designed to regulate and supervise ASDEX Upgrade Tokamak plasma operation. In its core DCS is based on a generic, versatile real-time software framework with a plugin architecture that allows to easily combine, modify and extend control function modules in order to tailor the system to required features and let it continuously evolve with the progress of an experimental fusion device. Due to these properties other fusion experiments like the WEST project have expressed interest in adopting DCS. For this purpose, essential parts of DCS must be unpinned from the ASDEX Upgrade environment by exposure or introduction of generalised interfaces. Re-organisation of DCS modules allows distinguishing between intrinsic framework core functions and device-specific applications. In particular, DCS must be prepared for deployment in different system environments with their own realisations for user interface, pulse schedule preparation, parameter server, time and event distribution, diagnostic and actuator systems, network communication and data archiving. The article explains the principles of the revised DCS structure, derives the necessary interface definitions and describes major steps to achieve the separation between general-purpose framework and fusion device specific components."
Transitive Sequencing Medical Records for Mining Predictive and Interpretable Temporal Representations,"Electronic health records (EHRs) contain important temporal information about the progression of disease and treatment outcomes. This paper proposes a transitive sequencing approach for constructing temporal representations from EHR observations for downstream machine learning. Using clinical data from a cohort of patients with congestive heart failure, we mined temporal representations by transitive sequencing of EHR medication and diagnosis records for classification and prediction tasks. We compared the classification and prediction performances of the transitive sequential representations (bag-of-sequences approach) with the conventional approach of using aggregated vectors of EHR data (aggregated vector representation) across different classifiers. We found that the transitive sequential representations are better phenotype “differentiators” and predictors than the “atemporal” EHR records. Our results also demonstrated that data representations obtained from transitive sequencing of EHR observations can present novel insights about the progression of the disease that are difficult to discern when clinical data are treated independently of the patient's history."
Tumor tissues diagnosis with PIEE lipid droplet vesicles,"A molecule ANB (6-(4-aminostyryl)-1,3-dioxo-1H-benzo[de]isoquinolin -2(3 H)-yl)-n-butane), was developed for fluorescence turn-on imaging of cancer cells lipid droplets (LDs). ANB exercised synergistically aggregation induced emission enhancement and photochromism, which are essential to construct a photoinduced emission enhancement (PIEE) LDs. The fluorescent nanostructures PIEE-LDs extremely improve intracellular imaging contrast with high emission intensity, high emission contrast to cytosol and high photostability. Following these unique optical properties, we statistically analyzed the differences of LDs between cancer and normal cells as well as successfully observed LDs progression and fusion in living cells over a long period. Consequently, the xenograft tumor tissues were favorably distinguished from normal based on this superior PIEE-LDs. It is known that ANB exhibited excited-state conformational conversion between the twist intramolecular charge transfer (TICT) and local excited (LE) states. Based on the results of our studies, we found that LE state became dominate and presented aggregation induced emission enhancement when ANB stay in LDs, which allows us to build a fluorescent nanovesicle model to explain the distinct fluorescence imaging of ANB in LDs. Furthermore, photochromism occurred when irradiation that could promoted emission behavior of bright nanovesicles LDs to further level, and achieve PIEE-LDs."
Two-layer fault diagnosis method for blast furnace based on evidence-conflict reduction on multiple time scales,"In a blast furnace (BF), faults are difficult to be diagnosed because of the severe environment inside a BF. Most proposed methods for BF fault diagnosis just diagnose states of a BF by short-time-scale data. However, the probability of faults is also related to the long-time-scale running state of a BF. This paper presents a two-layer fault diagnosis method for BFs on multiple time scales. First, the deterioration trend of a BF is analyzed and the deterioration degree is calculated on a long time scale. Then, an improved Dempster–Shafer (D–S) evidence method is designed to diagnose the faults of the BF on a short time scale. The conflicts of evidence are reduced by incorporating the deterioration degree. The paper explains how D–S theory can be extended to reduce the conflicts of evidence by incorporating the deterioration degree of a BF. The improved D–S method accounts for the different time scales compared with the conventional one. Finally, experiment results based on real running data show that the method reduces the conflict of evidence and improves diagnosis accuracy."
Two-tier anomaly detection based on traffic profiling of the home automation system,"Smart building equipment and automation systems often become a target of attacks and are used for attacking other targets located out of the Home Area Network. Attacks are often related to changes in traffic volume, disturbed packet flow or excessive energy consumption. Their symptoms can be recognized and interpreted locally, using software agent at Home Gateway. Although anomalies are detected locally at the Home Gateway, they can be exploited globally. Thus, it is significantly important to detect global attack attempts through anomalies correlation. Our proposal in this paper is the involvement of the Network Operator in Home Area Network security. Our paper describes a novel strategy for anomaly detection that consists of shared responsibilities between user and network provider. The proposed two-tier Intrusion Detection System uses a machine learning method for classifying the monitoring records and searching suspicious anomalies across the network at the service provider's data center. Result show that local anomaly detection combined with anomaly correlation at the service providers level can provide reliable information on the most frequent IoT devices misbehavior which may be caused by infection."
Ultrasonic signal classification and imaging system for composite materials via deep convolutional neural networks,"Automated ultrasonic signal classification systems are finding increasing use in many applications for the recognition of large volumes of inspection signals. Wavelet transform is a well-known signal processing technique in fault signal diagnosis system. Most of the proposed approaches have mainly used low-level handcraft features based on wavelet transform to encode the information for different defect classes. In this paper, we proposed a deep learning based framework to classify ultrasonic signals from carbon fiber reinforced polymer (CFRP) specimens with void and delamination. In our proposed algorithm, deep Convolutional Neural Networks (CNNs) are used to learn a compact and effective representation for each signal from wavelet coefficients. To yield superior results, we proposed to use a linear SVM top layer in the training process of signal classification task. The experimental results demonstrated the excellent performance of our proposed algorithm against the classical classifier with manually generated attributes. In addition, a post processing scheme is developed to interpret the classifier outputs with a C-scan imaging process and visualize the locations of defects using a 3D model representation."
Ultrasound simulation with animated anatomical models and on-the-fly fusion with real images via path-tracing,"Ultrasound is an essential imaging modality in clinical screening and diagnosis, for reducing morbidity and improving quality of life. Successfully performing ultrasound imaging, however, requires extensive training and expertise in navigating a hand-held probe to a correct anatomical location as well as subsequently interpreting the acquired image. Computer-generated simulations can offer a safe, flexible, and standardized environment to train such skills. Data-based simulations display interpolated slices from a-priori-acquired real ultrasound volumes, whereas generative simulations aim to reproduce the complex ultrasound interactions with comprehensive, geometric anatomical models, such as using ray-tracing to mimic acoustic propagation. Although sonographers typically focus on relatively smaller structures of interest in ultrasound images, the fidelity of the background anatomy may still play a role in contributing to the realism of a generated US image; e.g. when imaging a relatively smaller fetus within large abdominal background. It was proposed earlier to compose ray-traced images with acquired volumes in a preprocessing step. Despite its simplicity, this prevents any view-dependent artifacts and interactive model changes, such as those induced by animations, which can, for instance, model fetal motion. To fully leverage the flexibility of the model-based generative approach, we propose herein an on-the-fly image fusion, based on the two techniques, by moving the interpolation stage within the ray-tracer, such that the pre-acquired image data can be referred to in the background, while the acoustic interactions with the model can be resolved in the foreground. This allows for animated anatomical models, which we realize during simulation runtime via scene-hierarchy subtree switching between precomputed acceleration structure graphs. We demonstrate our proposed techniques on ultrasound sequences of fetal and heart motion, where only animated models can afford to meet realism requirements entailed by the temporal domain."
Uncertainty handling in safety instrumented systems according to IEC 61508 and new proposal based on coupling Monte Carlo analysis and fuzzy sets,"Safety instrumented systems must be designed, built and operated to meet tolerable risk level as required regulatory agencies. This requirement is closely related to their probabilistic performance measures which are either their average probability of dangerous failure on demand (PFDavg) or their average frequency of failure (PFH: Probability of Failure per Hour). The object of this work is the SIS performances evaluation taking into account the uncertainties related to the different parameters that come into play: failure rate (λ), common cause failure proportion (β), diagnostic coverage (DC), etc. This leads to an accurate and therefore safe assessment of the safety integrity level (SIL) inherent to safety functions performed by such systems. This aim is in keeping with the requirement of the IEC 61508 standard with respect to handling uncertainty. In this paper we first explain in detail the IEC 61508 approach for handling uncertainty. Afterwards, we propose an approach that combines (i) Monte Carlo analysis (MCA) and (ii) fuzzy sets. Indeed, the first method is appropriate when representative statistical data are available (using pdf of the relating parameters), while the latter applies in the case characterized by vague and subjective information (using membership function). The proposed approach is fully supported with a suitable computer code developed under the MATLAB environment."
Understanding adversarial attacks on deep learning based medical image analysis systems,"Deep neural networks (DNNs) have become popular for medical image analysis tasks like cancer diagnosis and lesion detection. However, a recent study demonstrates that medical deep learning systems can be compromised by carefully-engineered adversarial examples/attacks with small imperceptible perturbations. This raises safety concerns about the deployment of these systems in clinical settings. In this paper, we provide a deeper understanding of adversarial examples in the context of medical images. We find that medical DNN models can be more vulnerable to adversarial attacks compared to models for natural images, according to two different viewpoints. Surprisingly, we also find that medical adversarial attacks can be easily detected, i.e., simple detectors can achieve over 98% detection AUC against state-of-the-art attacks, due to fundamental feature differences compared to normal examples. We believe these findings may be a useful basis to approach the design of more explainable and secure medical deep learning systems."
Understanding fuel anti-knock performances in modern SI engines using fundamental HCCI experiments,"Modern spark-ignition (SI) engine technologies have considerably changed in-cylinder conditions under which fuel autoignition and engine knock take place. In this paper, fundamental HCCI engine experiments are proposed as a means for characterizing the impact of these technologies on the knock propensity of different fuels. In particular, the impacts of turbocharging, direct injection (DI), and downspeeding on operation with ethanol and gasoline are investigated to demonstrate this approach. Results reported earlier for ethanol and gasoline on HCCI combustion are revisited with the new perspective of how their autoignition characteristics fit into the anti-knock requirement in modern SI engines. For example, the weak sensitivity to pressure boost demonstrated by ethanol in HCCI autoignition can be used to explain the strong knock resistance of ethanol fuels for turbocharged SI engines. Further, ethanol's high sensitivity to charge temperature makes charge cooling, which can be produced by fuel vaporization via direct injection or by piston expansion via spark-timing retard, very effective for inhibiting knock. On the other hand, gasoline autoignition shows a higher sensitivity to pressure, so only very low pressure boost can be applied before knock occurs. Gasoline also demonstrates low temperature sensitivity, so it is unable to make as effective use of the charge cooling produced by fuel vaporization or spark retard. These arguments comprehensively explain literature results on ethanol's substantially better anti-knock performance over gasoline in modern turbocharged DISI engines. Fundamental HCCI experiments such as these can thus be used as a diagnostic and predictive tool for knock-limited SI engine performance for various fuels. Examples are presented where HCCI experiments are used to identify biofuel compounds with good potential for modern SI-engine applications."
Understanding the ignition mechanism of high-pressure spray flames,"A conceptual model for turbulent ignition in high-pressure spray flames is presented. The model is motivated by first-principles simulations and optical diagnostics applied to the Sandia n-dodecane experiment. The Lagrangian flamelet equations are combined with full LLNL kinetics (2755 species; 11,173 reactions) to resolve all time and length scales and chemical pathways of the ignition process at engine-relevant pressures and turbulence intensities unattainable using classic DNS. The first-principles value of the flamelet equations is established by a novel chemical explosive mode-diffusion time scale analysis of the fully-coupled chemical and turbulent time scales. Contrary to conventional wisdom, this analysis reveals that the high Damköhler number limit, a key requirement for the validity of the flamelet derivation from the reactive Navier–Stokes equations, applies during the entire ignition process. Corroborating Rayleigh-scattering and formaldehyde PLIF with simultaneous schlieren imaging of mixing and combustion are presented. Our combined analysis establishes a characteristic temporal evolution of the ignition process. First, a localized first-stage ignition event consistently occurs in highest temperature mixture regions. This initiates, owed to the intense scalar dissipation, a turbulent cool flame wave propagating from this ignition spot through the entire flow field. This wave significantly decreases the ignition delay of lower temperature mixture regions in comparison to their homogeneous reference. This explains the experimentally observed formaldehyde formation across the entire spray head prior to high-temperature ignition which consistently occurs first in a broad range of rich mixture regions. There, the combination of first-stage ignition delay, shortened by the cool flame wave, and the subsequent delay until second-stage ignition becomes minimal. A turbulent flame subsequently propagates rapidly through the entire mixture over time scales consistent with experimental observations. We demonstrate that the neglect of turbulence-chemistry-interactions fundamentally fails to capture the key features of this ignition process."
"Understanding the impact of prior reviews on subsequent reviews: The role of rating volume, variance and reviewer characteristics","Extant literature has recognized the impact of previously posted ratings on subsequent ratings but reports inconsistent findings. To reconcile the mixed findings in the literature, this study draws on an integrative view of information diagnosticity and social influence theory to investigate the moderating effect of rating context (characterized by volume and variance of prior ratings) and its interaction with subsequent reviewer characteristics on the relationship between the average of prior ratings and a subsequent rating. The empirical analyses, using a dataset of 70,410 restaurant reviews collected from Yelp.com, reveal that both volume and variance of prior ratings exert negative moderating effects and such moderating effects are contingent on subsequent reviewer connectedness and expertise."
Unifying neural learning and symbolic reasoning for spinal medical report generation,"Automated medical report generation in spine radiology, i.e., given spinal medical images and directly create radiologist-level diagnosis reports to support clinical decision making, is a novel yet fundamental study in the domain of artificial intelligence in healthcare. However, it is incredibly challenging because it is an extremely complicated task that involves visual perception and high-level reasoning processes. In this paper, we propose the neural-symbolic learning (NSL) framework that performs human-like learning by unifying deep neural learning and symbolic logical reasoning for the spinal medical report generation. Generally speaking, the NSL framework firstly employs deep neural learning to imitate human visual perception for detecting abnormalities of target spinal structures. Concretely, we design an adversarial graph network that interpolates a symbolic graph reasoning module into a generative adversarial network through embedding prior domain knowledge, achieving semantic segmentation of spinal structures with high complexity and variability. NSL secondly conducts human-like symbolic logical reasoning that realizes unsupervised causal effect analysis of detected entities of abnormalities through meta-interpretive learning. NSL finally fills these discoveries of target diseases into a unified template, successfully achieving a comprehensive medical report generation. When employed in a real-world clinical dataset, a series of empirical studies demonstrate its capacity on spinal medical report generation and show that our algorithm remarkably exceeds existing methods in the detection of spinal structures. These indicate its potential as a clinical tool that contributes to computer-aided diagnosis."
Unsupervised classification of multichannel profile data using PCA: An application to an emission control system,"Modern sensing technologies have facilitated real-time data collection for process monitoring and fault diagnosis in several research fields of industrial engineering. The challenges associated with diagnosis of multichannel (multiple) profiles are yet to be addressed in the literature. Motivated by an application of fault diagnosis of an emission control system, this paper proposes an approach for efficient and interpretable modeling of multichannel profile data in high-dimensional spaces. The method is based on unsupervised classification of multichannel profile data provided by several sensors related to a fault event. The final goal is to isolate fault events in a restricted number of clusters (scenarios), each one described by a reference pattern. This can provide practitioners with useful information to support the diagnosis and to find root cause. Two multilinear extensions of principal component analysis (PCA), which can analyze the multichannel profiles without unfolding the original data set, are investigated and compared to regular PCA applied to vectors generated by unfolding the original data set. The effectiveness of multilinear extensions of PCA is demonstrated using an experimental campaign carried out on an emission control system. Results of unsupervised classification show that the multilinear extension of PCA may lead to a classification with better compactness and separation of clusters."
Unsupervised intrusion detection through skip-gram models of network behavior,"Detecting intrusions is one of the main objectives of computer security. Attacks have become overly sophisticated over the years in order to remain effective and stealthy. Major breaches are typically perpetrated using techniques that are polymorphic, multi-vector, multi-stage and targeted, that is, adopting forms that were never seen before. Anomaly detection, which does not make any assumption about the shape of a potential attack but instead on legitimate behavior, seems to be a suitable approach in order to defeat sophisticated intrusions. Skip-gram modeling, a word2vec algorithm variant, was leveraged to model systems’ legitimate network behavior. The resulting model was then used to spot intrusions in a test dataset. The optimal configuration led to 99.20% precision, 82.07% recall, and 91.02% accuracy, with a false positive rate of 0.61%, which is significantly lower than most state-of-the-art methods. These metrics were achieved under a fully unsupervised setting, that is, without any prior knowledge of what constitutes an attack. Furthermore, the approach provides benefits in terms of interpretability and log storage requirements, as it requires a small amount of input features. It also produces information about systems behavior and their relationships, that can be reused by other analysis techniques to obtain further insights."
Unsupervised Machine Learning by Graph Analytics on Heterogeneous Network Device Data,"We explored unsupervised machine learning algorithms, specifically graph analytics, applied to behaviors observed in heterogeneous network sensor data for discovering anomalous behavior that could include novel attacks. In addition, we explored the potential difficulties with applying unsupervised machine learning approaches to anomaly detection in a network-defense context to understand how to integrate inherently imperfect anomaly-detection approaches into the workflow of a cyber defense infrastructure. Two general approaches can be used to discover anomalies: (1.) detecting rarity, i.e., finding those activities that are observed the least frequently in a set of observations, and (2.) detecting novelty, i.e., finding activities with the lowest estimated probability of observation based on prior observations of baseline (presumably “normal”) data. This effort will describe the case of detecting rarity. In this paper, we describe the entire pipeline starting from explaining the data used, the data ingest, the quantization of features, application of graph analytics to the data, post-processing to reduce results, and measuring the performance. A network-penetration experiment was setup to conduct the network attacks and generate the data that is the input to this work. Baseline methods are proposed and compared to the main method that is described in this paper."
"Unusual defects, generated by wafer sawing: An update, including pick&place processing","At ESREF 2008, the paper “Unusual defects, generated by wafer sawing: Diagnosis, mechanisms and how to distinguish from related failures” had won the Best Paper Award. In the meantime, new experiences were collected, related to new methods as laser sawing and its specific ESD risks and additional failure mechanisms as backside damage, charging of foils, pad corrosion and sawing residue damages. This paper explains in detail these failure sources, including detailed explanations on root causes and physical mechanisms as well as important hints for failure analysts how to distinguish related failure signatures from those, which look similar but are of other origin."
Urban spatial structure and travel patterns: Analysis of workday and holiday travel using inhomogeneous Poisson point process models,"City land-use features and travel behavior are mutually related and restricted. This research attempts to model the spatial-temporal travel patterns based on spatial point pattern theory. Using a twelve-day private automobile data set collected in Beijing, we systematically investigate the temporal variations of trip-destination distributions, and their association with city spatial structure. The availability of detailed POIs (Points of Interest) data enables us to study the effect of city structure on travel pattern at a refined level. Four types of inhomogeneous Poisson point process models are built to capture the impacts on human mobility posed by spatial covariates. Residual analysis, inhomogeneous K function and leverage diagnostic tools are further adopted to validate the model performance and determine the best fitted model. The validation results indicate that the proposed model reasonably explains the travel patterns in both holiday and workday throughout the city. The inclusion of Cartesian coordinates, population distribution, and city subdivision-category improves the model performance. The empirical results based on the dataset also reveal the differences in impacts on travel patterns posed by underlying city structure between holidays and weekdays as well as between citywide districts. The modeling method and the exploratory spatial–temporal analysis in this study can offer complementary techniques for traffic management and urban planning."
Use of a Recursive-Rule eXtraction algorithm with J48graft to achieve highly accurate and concise rule extraction from a large breast cancer dataset,"To assist physicians in the diagnosis of breast cancer and thereby improve survival, a highly accurate computer-aided diagnostic system is necessary. Although various machine learning and data mining approaches have been devised to increase diagnostic accuracy, most current methods are inadequate. The recently developed Recursive-Rule eXtraction (Re-RX) algorithm provides a hierarchical, recursive consideration of discrete variables prior to analysis of continuous data, and can generate classification rules that have been trained on the basis of both discrete and continuous attributes. The objective of this study was to extract highly accurate, concise, and interpretable classification rules for diagnosis using the Re-RX algorithm with J48graft, a class for generating a grafted C4.5 decision tree. We used the Wisconsin Breast Cancer Dataset (WBCD). Nine research groups provided 10 kinds of highly accurate concrete classification rules for the WBCD. We compared the accuracy and characteristics of the rule set for the WBCD generated using the Re-RX algorithm with J48graft with five rule sets obtained using 10-fold cross validation (CV). We trained the WBCD using the Re-RX algorithm with J48graft and the average classification accuracies of 10 runs of 10-fold CV for the training and test datasets, the number of extracted rules, and the average number of antecedents for the WBCD. Compared with other rule extraction algorithms, the Re-RX algorithm with J48graft resulted in a lower average number of rules for diagnosing breast cancer, which is a substantial advantage. It also provided the lowest average number of antecedents per rule. These features are expected to greatly aid physicians in making accurate and concise diagnoses for patients with breast cancer."
Use of mesh phasing to locate faulty planet gears,"It is a challenge to determine by vibration analysis which planet gear carries a fault due to the complex layout of planetary gearboxes. In this study, we explain for the first time the possibility of using mesh phasing for distinguishing between faults on different planet gears. It has been found that, due to mesh phasing relationships, the vibration signals recorded on a planetary gearbox test rig exhibit different characteristics depending on the position of the faulty gear. These tests used localised seeded spalls giving impulsive signals, and it was found that due to the sequential mesh phasing arrangement, the timing of the fault-related impulse responses, as measured relative to the phase of the gearmesh component, depends on which planet is faulty. These timing differences can also give rise to different levels of asymmetry in the signal. This paper proposes a phase-based approach to differentiate and locate the faulty planet gear position using vibration response signals, and an indicator is developed for this purpose. With the assistance of absolute phase information, acquired through a tachometer on the planet carrier, the phase indicator is able to locate the position of the faulty gear. The approach has been applied to recorded experimental signals and it is found that the method can diagnose the position of the faulty gear effectively."
Use of the recursive-rule extraction algorithm with continuous attributes to improve diagnostic accuracy in thyroid disease,"Thyroid diseases, which often lead to thyroid dysfunction involving either hypo- or hyperthyroidism, affect hundreds of millions of people worldwide, many of whom remain undiagnosed; however, diagnosis is difficult because symptoms are similar to those seen in a number of other conditions. The objective of this study was to assess the effectiveness of the Recursive-Rule Extraction (Re-RX) algorithm with continuous attributes (Continuous Re-RX) in extracting highly accurate, concise, and interpretable classification rules for the diagnosis of thyroid disease. We used the 7200-sample Thyroid dataset from the University of California Irvine Machine Learning Repository, a large and highly imbalanced dataset that comprises both discrete and continuous attributes. We trained the dataset using Continuous Re-RX, and after obtaining the maximum training and test accuracies, the number of extracted rules, and the average number of antecedents, we compared the results with those of other extraction methods. Our results suggested that Continuous Re-RX not only achieved the highest accuracy for diagnosing thyroid disease compared with the other methods, but also provided simple, concise, and interpretable rules. Based on these results, we believe that the use of Continuous Re-RX in machine learning may assist healthcare professionals in the diagnosis of thyroid disease."
Using big data analytics to extract disease surveillance information from point of care diagnostic machines,"This paper explains a novel approach for knowledge discovery from data generated by Point of Care (POC) devices. A very important element of this type of knowledge extraction is that the POC generated data would never be identifiable, thereby protecting the rights and the anonymity of the individual, whilst still allowing for vital population-level evidence to be obtained. This paper also reveals a real-world implementation of the novel approach in a big data analytics system. Using Internet of Things (IoT) enabled POC devices and the big data analytics system, the data can be collected, stored, and analyzed in batch and real-time modes to provide a detailed picture of a healthcare system as well to identify high-risk populations and their locations. In addition, the system offers benefits to national health authorities in forms of optimized resource allocation (from allocating consumables to finding the best location for new labs) thus supports efficient and timely decision-making processes."
Using game theory and decision decomposition to effectively discern and characterise bi-locus diseases,"In order to gain insight into oligogenic disorders, understanding those involving bi-locus variant combinations appears to be key. In prior work, we showed that features at multiple biological scales can already be used to discriminate among two types, i.e. disorders involving true digenic and modifier combinations. The current study expands this machine learning work towards dual molecular diagnosis cases, providing a classifier able to effectively distinguish between these three types. To reach this goal and gain an in-depth understanding of the decision process, game theory and tree decomposition techniques are applied to random forest predictors to investigate the relevance of feature combinations in the prediction. A machine learning model with high discrimination capabilities was developed, effectively differentiating the three classes in a biologically meaningful manner. Combining prediction interpretation and statistical analysis, we propose a biologically meaningful characterization of each class relying on specific feature strengths. Figuring out how biological characteristics shift samples towards one of three classes provides clinically relevant insight into the underlying biological processes as well as the disease itself."
Using Planar Laser Induced Fluorescence to explain the mechanism of heterogeneous water droplet boiling and explosive breakup,"Over the recent years, the research community has taken an increasing interest in high-temperature gas-steam-droplet systems. This promotes emerging technologies in thermal or flame water cleaning from unspecified impurities, and firefighting by water slurry aerosols. Unfortunately, these technologies have yet to become mainstream, although they have been regarded as extremely important and promising for several years already. The fact is that there are too few experimental data on the physical processes intensifying the evaporation of water slurries in hot gaseous media. The results of such experiments with temperatures over 1000°C are virtually impossible to find. These data are so evasive due to fast-paced processes and difficulties in measuring the temperature in evaporating heterogeneous water droplets. The typical durations of high-temperature heating and evaporation do not usually exceed several seconds. In this work, we conduct experiments using a heterogeneous water droplet with a single nontransparent solid inclusion to determine the unsteady temperature field of the latter. We use an optical diagnostic technique, Planar Laser Induced Fluorescence, to study the conditions, mechanism, reasons and characteristics of water boiling leading to an explosive breakup (disintegration) of water slurry droplets. Rhodamine B acts as a fluorophore. The typical temperatures are determined in the depth of a droplet, near its free (outer) surface, and at the interface. The water temperature at the water – solid inclusion interface is shown to be higher than at the outer surface of a droplet. Furthermore, we compare the temperature fields of a homogeneous and heterogeneous water droplet under identical heating conditions."
Using the PARAFAC2 tensor factorization on EHR audit data to understand PCP desktop work,"Background Activity or audit log data are required for EHR privacy and security management but may also be useful for understanding desktop workflow. Objective We determined if the EHR audit log file, a rich source of complex time-stamped data on desktop activities, could be processed to derive primary care provider (PCP) level workflow measures. Methods We analyzed audit log data on 876 PCPs across 17,455 ambulatory care encounters that generated 578,394 time-stamped records. Each individual record represents a user interaction (e.g., point and click) that reflects all or part of a specific activity (e.g., order entry access). No dictionary exists to define how to combine clusters of sequential audit log records to represent identifiable PCP tasks. We determined if PARAFAC2 tensor factorization could: (1) learn to identify audit log record clusters that specifically represent defined PCP tasks; and (2) identify variation in how tasks are completed without the need for ground-truth labels. To interpret the result, we used the following PARAFAC2 factors: a matrix representing the task definitions and a matrix containing the frequency measure of each task for each encounter. Results PARAFAC2 automatically identified 4 clusters of audit log records that represent 4 common clinical encounter tasks: (1) medications’ access, (2) notes’ access, (3) order entry access, and (4) diagnosis modification. PARAFAC2 also identified the most common variants in how PCPs accomplish these tasks. It discovered variation in how the notes’ access task was done, including identification of 9 distinct variants of notes access that explained 77% of the input data variation for notes. The discovered variants mapped to two known workflows for notes’ access and to two distinct PCP user groups who accessed notes by either using the Visit Navigator or the Wrap-Up option. Conclusions Our results demonstrate that EHR audit log data can be rapidly processed to create higher-level constructed features that represent time-stamped PCP tasks."
Validation of equilibrium tools on the COMPASS tokamak,"Various MHD (magnetohydrodynamic) equilibrium tools, some of which being recently developed or considerably updated, are used on the COMPASS tokamak at IPP Prague. MHD equilibrium is a fundamental property of the tokamak plasma, whose knowledge is required for many diagnostics and modelling tools. Proper benchmarking and validation of equilibrium tools is thus key for interpreting and planning tokamak experiments. We present here benchmarks and comparisons to experimental data of the EFIT++ reconstruction code (Appel et al., 2006), the free-boundary equilibrium code FREEBIE (Artaud and Kim, 2012), and a rapid plasma boundary reconstruction code VacTH (Faugeras et al., 2014). We demonstrate that FREEBIE can calculate the equilibrium and corresponding poloidal field (PF) coils currents consistently with EFIT++ reconstructions from experimental data. Both EFIT++ and VacTH can reconstruct equilibria generated by FREEBIE from synthetic, optionally noisy diagnostic data. Hence, VacTH is suitable for real-time control. Optimum reconstruction parameters are estimated."
Vertically aligned multi-walled carbon nanotubes based flexible immunosensor for extreme low level detection of multidrug resistant leukemia cells,"We report an advanced flexible immunosensor for extreme low level detection of multidrug resistant myeloid leukemia cells. The detection mechanism is analyzed in terms of novel nanoscale interactions of target molecules with vertically aligned multi-walled carbon nanotubes (VA-MWNT). The immunosensor is fabricated by transferring VA-MWNTs on a polyethylene terephthalate substrate by hot press technique without losing CNTs’ pristine character. Sensing is performed using doxorubicin treated leukemia K562 cells in varying concentrations from 1.5 × 102 cells mL–1 to 1.5 × 107 cells mL–1 and sensor showed detection limit of 10 cells mL–1. The calibration plot of peak current versus logarithmic concentration of DOX/leukemia K562 cells exhibited good linearity with a regression coefficient of ˜0.98. Sensing mechanism is explained in terms of charge transfer induced Fermi level shift of sub μeV order, causing band bending at the interface of CNT-molecular species; and it is reflected in lowering detection limit of the fabricated sensor. Theoretical analysis is done to correlate Fermi energy shift with sensitivity of the device on cancer cell immobilization. Additionally, the developed immunosensor shows good stability, reproducibility and fast detection vis-à-vis the devices reported so far. The notable advantages of proposed flexible sensor are its durability, chemical and moisture resistance, making it a potentially competitive device for point-of-care diagnostics."
Visible-light excited red-emitting vacancies at carbon interstitials as indicators of irradiated and annealed Type Ia diamonds,"During the last decades many studies have been carried out to investigate how point defects and aggregates respond and evolve in natural Type Ia diamonds as a result of treatments, and a number of underlying mechanisms have been identified and interpreted. However, the analysis of radiation-induced creation/ionization of defects, as well as their migration and aggregation in secondary defect structures, often requires experimental approaches which can hardly constitute a simple-to-use diagnostic tool for the identification of artificially treated diamonds. Here we disclose a novel simple indicator of artificial exposure of Type Ia diamonds to ionizing radiations and subsequent annealing. This indicator consists in narrow photoluminescence lines in the red region, between 681 and 725 nm, we recently found to result from vacancies trapped by interstitial carbon aggregates and platelets. Our results demonstrate that interstitial structures become sites of vacancy trapping – by thermal migration of radiation-induced vacancies – only when diamond undergoes treatments. We give the rigorous validation of the new spectroscopic probe of artificial treatments analysing photoluminescence and infrared absorption spectra of well-known H1b and H1c centres in a hundred samples. Importantly, the method is based on emission lines which do not require neither high photon-energy excitation nor cryogenic temperatures."
Volumetric relief map for intracranial cerebrospinal fluid distribution analysis,"Cerebrospinal fluid imaging plays a significant role in the clinical diagnosis of brain disorders, such as hydrocephalus and Alzheimer's disease. While three-dimensional images of cerebrospinal fluid are very detailed, the complex structures they contain can be time-consuming and laborious to interpret. This paper presents a simple technique that represents the intracranial cerebrospinal fluid distribution as a two-dimensional image in such a way that the total fluid volume is preserved. We call this a volumetric relief map, and show its effectiveness in a characterization and analysis of fluid distributions and networks in hydrocephalus patients and healthy adults."
Walking pattern classification using a granular linguistic analysis,"Classifying walking patterns helps the diagnosis of health status, disease progression and the effect of interventions. In this paper, we develop previous research on human gait to extract a meaningful set of parameters that allow us to design a highly interpretable system capable of identifying different gait styles with linguistic fuzzy if-then rules. The model easily discriminates among five different walking patterns, namely: normal walk, on tiptoes, dragging left limb, dragging right limb, and dragging both limbs. We have carried out a complete experimentation to test the performance of the extracted parameters to correctly classify these five chosen gait styles."
When to re-order laboratory tests? Learning laboratory test shelf-life,"Most laboratory results are valid for only a certain time period (laboratory tests shelf-life), after which they are outdated and the test needs to be re-administered. Currently, laboratory test shelf-lives are not centrally available anywhere but the implicit knowledge of doctors. In this work we propose an automated method to learn laboratory test-specific shelf-life by identifying prevalent laboratory test order patterns in electronic health records. The resulting shelf-lives performed well in the evaluation of internal validity, clinical interpretability, and external validity."
White learning methodology: A case study of cancer-related disease factors analysis in real-time PACS environment,"Background and Objective Bayesian network is a probabilistic model of which the prediction accuracy may not be one of the highest in the machine learning family. Deep learning (DL) on the other hand possess of higher predictive power than many other models. How reliable the result is, how it is deduced, how interpretable the prediction by DL mean to users, remain obscure. DL functions like a black box. As a result, many medical practitioners are reductant to use deep learning as the only tool for critical machine learning application, such as aiding tool for cancer diagnosis. Methods In this paper, a framework of white learning is being proposed which takes advantages of both black box learning and white box learning. Usually, black box learning will give a high standard of accuracy and white box learning will provide an explainable direct acyclic graph. According to our design, there are 3 stages of White Learning, loosely coupled WL, semi coupled WL and tightly coupled WL based on degree of fusion of the white box learning and black box learning. In our design, a case of loosely coupled WL is tested on breast cancer dataset. This approach uses deep learning and an incremental version of Naïve Bayes network. White learning is largely defied as a systemic fusion of machine learning models which result in an explainable Bayes network which could find out the hidden relations between features and class and deep learning which would give a higher accuracy of prediction than other algorithms. We designed a series of experiments for this loosely coupled WL model. Results The simulation results show that using WL compared to standard black-box deep learning, the levels of accuracy and kappa statistics could be enhanced up to 50%. The performance of WL seems more stable too in extreme conditions such as noise and high dimensional data. The relations by Bayesian network of WL are more concise and stronger in affinity too. Conclusion The experiments results deliver positive signals that WL is possible to output both high classification accuracy and explainable relations graph between features and class."
With a little help from my friends: Cultivating serendipity in online shopping environments,"Many important findings and discoveries in science and everyday life are the result of serendipity, that is, the unanticipated occurrence of happy events such as the finding of valuable information. Consumers are increasingly seeking serendipity in online shopping, where information clutter and preprogramed recommendation systems can make product choice frustrating or mundane. However, it is notoriously difficult to design online shopping environments that induce it. In this study, we explore how social media affordances such as obtaining access to peer-generated content and being connected to online friends can help create the right conditions for serendipity in online shopping. We supplement this analysis with an account of two individual factors that are also likely to be instrumental in a shopping context, namely, the intensity of shoppers’ information search and their aversion to risk when faced with a product choice. Our investigation relies on a conceptualization of serendipity that has two defining elements: unexpectedness and informational value. The results of an experimental study in which we manipulated an online product search environment reveal the superiority of designs that incorporate online friendships, and these results support the positive effects of search effort and risk aversion on serendipity. This study contributes by developing a theoretical framework for the analysis of serendipity and by explaining how social commerce, that is, the integration of social media and electronic commerce, can cultivate serendipity."
Zeff profile measurement system using a Thomson polychromator,"To measure the effective ion charge Zeff profile, most plasma machines are equipped with a bremsstrahlung measurement system as a filter-scope diagnostic device. In Korean Superconducting Tokamak Advanced Research (KSTAR) machine, a new type of bremsstrahlung measurement system, called hybrid-type polychromator system, was developed and tested at a single position during the 10th KSTAR plasma campaign [1]. The main idea of this hybrid-type polychromator system is to combine a polychromator system, which can measure the Thomson scattering signal, with a bremsstrahlung signal measurement system. The advantages of this hybrid-type polychromator system include the reduction of diagnostic vacuum windows in the Tokamak or Stellarator, and the measurement of the Zeff parameter with electron temperature (Te) and electron density (ne) at a same position. In this study, we improve last year's hybrid-type polychromator system. A major improvement is the change in the structure of the optical system. This allows measuring the bremsstrahlung signals more clearly, and four hybrid-type polychromators were equipped to measure the Zeff profile preliminarily. In this paper, we explain the structure of the hybrid-type polychromator and the system's measurement results."
