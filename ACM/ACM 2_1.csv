"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"6V4HJJAY","journalArticle","2016","Zhu, Yada; He, Jingrui","Co-Clustering Structural Temporal Data with Applications to Semiconductor Manufacturing","ACM Transactions on Knowledge Discovery from Data","","1556-4681","10.1145/2875427","https://doi.org/10.1145/2875427","Recent years have witnessed data explosion in semiconductor manufacturing due to advances in instrumentation and storage techniques. The large amount of data associated with process variables monitored over time form a rich reservoir of information, which can be used for a variety of purposes, such as anomaly detection, quality control, and fault diagnostics. In particular, following the same recipe for a certain Integrated Circuit device, multiple tools and chambers can be deployed for the production of this device, during which multiple time series can be collected, such as temperature, impedance, gas flow, electric bias, etc. These time series naturally fit into a two-dimensional array (matrix), i.e., each element in this array corresponds to a time series for one process variable from one chamber. To leverage the rich structural information in such temporal data, in this article, we propose a novel framework named C-Struts to simultaneously cluster on the two dimensions of this array. In this framework, we interpret the structural information as a set of constraints on the cluster membership, introduce an auxiliary probability distribution accordingly, and design an iterative algorithm to assign each time series to a certain cluster on each dimension. Furthermore, we establish the equivalence between C-Struts and a generic optimization problem, which is able to accommodate various distance functions. Extensive experiments on synthetic, benchmark, as well as manufacturing datasets demonstrate the effectiveness of the proposed method.","2016-05-24","2021-06-14 02:55:00","2021-06-14 06:35:49","2021-06-14 02:53:53","43:1–43:18","","4","10","","ACM Trans. Knowl. Discov. Data","","","","","","","","","","","","","July 2016","","","","","","","Co-clustering; semiconductor; structural; temporal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YKC9DAN3","journalArticle","2016","Kumar, Dheeraj; Bezdek, James C.; Rajasegarar, Sutharshan; Palaniswami, Marimuthu; Leckie, Christopher; Chan, Jeffrey; Gubbi, Jayavardhana","A A Adaptive Cluster Tendency Visualization and Anomaly Detection for Streaming Data","ACM Transactions on Knowledge Discovery from Data","","1556-4681","10.1145/2997656","https://doi.org/10.1145/2997656","The growth in pervasive network infrastructure called the Internet of Things (IoT) enables a wide range of physical objects and environments to be monitored in fine spatial and temporal detail. The detailed, dynamic data that are collected in large quantities from sensor devices provide the basis for a variety of applications. Automatic interpretation of these evolving large data is required for timely detection of interesting events. This article develops and exemplifies two new relatives of the visual assessment of tendency (VAT) and improved visual assessment of tendency (iVAT) models, which uses cluster heat maps to visualize structure in static datasets. One new model is initialized with a static VAT/iVAT image, and then incrementally (hence inc-VAT/inc-iVAT) updates the current minimal spanning tree (MST) used by VAT with an efficient edge insertion scheme. Similarly, dec-VAT/dec-iVAT efficiently removes a node from the current VAT MST. A sequence of inc-iVAT/dec-iVAT images can be used for (visual) anomaly detection in evolving data streams and for sliding window based cluster assessment for time series data. The method is illustrated with four real datasets (three of them being smart city IoT data). The evaluation demonstrates the algorithms’ ability to successfully isolate anomalies and visualize changing cluster structure in the streaming data.","2016-12-03","2021-06-14 02:55:00","2021-06-15 08:17:36","2021-06-14 02:53:53","24:1–24:40","","2","11","","ACM Trans. Knowl. Discov. Data","","","","","","","","","","","","","December 2016","","","","","","","cluster heat maps; internet of things (IoT); online anomaly detection; sliding window based time series clustering; smart city streaming data analysis; Visual assessment of clusters in streaming data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"52BP6BC7","journalArticle","2019","Siddiqui, Md Amran; Fern, Alan; Dietterich, Thomas G.; Wong, Weng-Keen","Sequential Feature Explanations for Anomaly Detection","ACM Transactions on Knowledge Discovery from Data","","1556-4681","10.1145/3230666","https://doi.org/10.1145/3230666","In many applications, an anomaly detection system presents the most anomalous data instance to a human analyst, who then must determine whether the instance is truly of interest (e.g., a threat in a security setting). Unfortunately, most anomaly detectors provide no explanation about why an instance was considered anomalous, leaving the analyst with no guidance about where to begin the investigation. To address this issue, we study the problems of computing and evaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE of an anomaly is a sequence of features, which are presented to the analyst one at a time (in order) until the information contained in the highlighted features is enough for the analyst to make a confident judgement about the anomaly. Since analyst effort is related to the amount of information that they consider in an investigation, an explanation’s quality is related to the number of features that must be revealed to attain confidence. In this article, we first formulate the problem of optimizing SFEs for a particular density-based anomaly detector. We then present both greedy algorithms and an optimal algorithm, based on branch-and-bound search, for optimizing SFEs. Finally, we provide a large scale quantitative evaluation of these algorithms using a novel framework for evaluating explanations. The results show that our algorithms are quite effective and that our best greedy algorithm is competitive with optimal solutions.","2019-01-09","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:53:54","1:1–1:22","","1","13","","ACM Trans. Knowl. Discov. Data","","","","","","","","","","","","","January 2019","","","","C:\Users\Asus\Zotero\storage\PZHI5F3V\Siddiqui et al. - 2019 - Sequential Feature Explanations for Anomaly Detect.pdf","","","Anomaly detection; anomaly explanation; anomaly interpretation; explanation evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXVAYJ5T","journalArticle","2020","Lin, Zongyu; Lyu, Shiqing; Cao, Hancheng; Xu, Fengli; Wei, Yuqiong; Samet, Hanan; Li, Yong","HealthWalks: Sensing Fine-grained Individual Health Condition via Mobility Data","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3432229","https://doi.org/10.1145/3432229","Can health conditions be inferred from an individual's mobility pattern? Existing research has discussed the relationship between individual physical activity/mobility and well-being, yet no systematic study has been done to investigate the predictability of fine-grained health conditions from mobility, largely due to the unavailability of data and unsatisfactory modelling techniques. Here, we present a large-scale longitudinal study, where we collect the health conditions of 747 individuals who visit a hospital and tracked their mobility for 2 months in Beijing, China. To facilitate fine-grained individual health condition sensing, we propose HealthWalks, an interpretable machine learning model that takes user location traces, the associated points of interest, and user social demographics as input, at the core of which a Deterministic Finite Automaton (DFA) model is proposed to auto-generate explainable features to capture useful signals. We evaluate the effectiveness of our proposed model, which achieves 40.29% in micro-F1 and 31.63% in Macro-F1 for the 8-class disease category prediction, and outperforms the best baseline by 22.84% in Micro-F1 and 31.79% in Macro-F1. In addition, deeper analysis based on the SHapley Additive exPlanations (SHAP) showcases that HealthWalks can derive meaningful insights with regard to the correlation between mobility and health conditions, which provide important research insights and design implications for mobile sensing and health informatics.","2020-12-17","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:53:56","138:1–138:26","","4","4","","Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","HealthWalks","","","","","","","","","","","","December 2020","","","","","","","Health Sensing; Human Mobility","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V3DVXHJE","journalArticle","2018","Zheng, Pengfei; Lee, Benjamin C.","Hound: Causal Learning for Datacenter-scale Straggler Diagnosis","Proceedings of the ACM on Measurement and Analysis of Computing Systems","","","10.1145/3179420","https://doi.org/10.1145/3179420","Stragglers are exceptionally slow tasks within a job that delay its completion. Stragglers, which are uncommon within a single job, are pervasive in datacenters with many jobs. A large body of research has focused on mitigating datacenter stragglers, but relatively little research has focused on systematically and rigorously identifying their root causes. We present Hound, a statistical machine learning framework that infers the causes of stragglers from traces of datacenter-scale jobs. Hound is designed to achieve several objectives: datacenter-scale diagnosis, interpretable models, unbiased inference, and computational efficiency. We demonstrate Hound's capabilities for a production trace from Google's warehouse-scale datacenters and two Spark traces from Amazon EC2 clusters.","2018-04-03","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:53:57","17:1–17:36","","1","2","","Proc. ACM Meas. Anal. Comput. Syst.","Hound","","","","","","","","","","","","March 2018","","","","C:\Users\Asus\Zotero\storage\JR5WLQIK\Zheng and Lee - 2018 - Hound Causal Learning for Datacenter-scale Stragg.pdf","","","machine learning; causal reasoning; datacenter; distributed system; performance diagnosis; performance modeling; topic modeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VS7UI5YI","journalArticle","2020","Kim, Youngwoo; Jang, Myungha; Allan, James","Explaining Text Matching on Neural Natural Language Inference","ACM Transactions on Information Systems","","1046-8188","10.1145/3418052","https://doi.org/10.1145/3418052","Natural language inference (NLI) is the task of detecting the existence of entailment or contradiction in a given sentence pair. Although NLI techniques could help numerous information retrieval tasks, most solutions for NLI are neural approaches whose lack of interpretability prohibits both straightforward integration and diagnosis for further improvement. We target the task of generating token-level explanations for NLI from a neural model. Many existing approaches for token-level explanation are either computationally costly or require additional annotations for training. In this article, we first introduce a novel method for training an explanation generator that does not require additional human labels. Instead, the explanation generator is trained with the objective of predicting how the model’s classification output will change when parts of the inputs are modified. Second, we propose to build an explanation generator in a multi-task learning setting along with the original NLI task so the explanation generator can utilize the model’s internal behavior. The experiment results suggest that the proposed explanation generator outperforms numerous strong baselines. In addition, our method does not require excessive additional computation at prediction time, which renders it an order of magnitude faster than the best-performing baseline.","2020-09-16","2021-06-14 02:55:00","2021-06-14 06:25:06","2021-06-14 02:53:58","39:1–39:23","","4","38","","ACM Trans. Inf. Syst.","","","","","","","","","","","","","October 2020","","","","","","","interpretable machine learning; Natural language inference; neural network explanation; rationale","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RN6BXADH","journalArticle","2020","Huang, Zhenya; Liu, Qi; Chen, Yuying; Wu, Le; Xiao, Keli; Chen, Enhong; Ma, Haiping; Hu, Guoping","Learning or Forgetting? A Dynamic Approach for Tracking the Knowledge Proficiency of Students","ACM Transactions on Information Systems","","1046-8188","10.1145/3379507","https://doi.org/10.1145/3379507","The rapid development of the technologies for online learning provides students with extensive resources for self-learning and brings new opportunities for data-driven research on educational management. An important issue of online learning is to diagnose the knowledge proficiency (i.e., the mastery level of a certain knowledge concept) of each student. Considering that it is a common case that students inevitably learn and forget knowledge from time to time, it is necessary to track the change of their knowledge proficiency during the learning process. Existing approaches either relied on static scenarios or ignored the interpretability of diagnosis results. To address these problems, in this article, we present a focused study on diagnosing the knowledge proficiency of students, where the goal is to track and explain their evolutions simultaneously. Specifically, we first devise an explanatory probabilistic matrix factorization model, Knowledge Proficiency Tracing (KPT), by leveraging educational priors. KPT model first associates each exercise with a knowledge vector in which each element represents a specific knowledge concept with the help of Q-matrix. Correspondingly, at each time, each student can be represented as a proficiency vector in the same knowledge space. Then, our KPT model jointly applies two classical educational theories (i.e., learning curve and forgetting curve) to capture the change of students’ proficiency level on concepts over time. Furthermore, for improving the predictive performance, we develop an improved version of KPT, named Exercise-correlated Knowledge Proficiency Tracing (EKPT), by considering the connectivity among exercises with the same knowledge concepts. Finally, we apply our KPT and EKPT models to three important diagnostic tasks, including knowledge estimation, score prediction, and diagnosis result visualization. Extensive experiments on four real-world datasets demonstrate that both of our models could track the knowledge proficiency of students effectively and interpretatively.","2020-02-20","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:00","19:1–19:33","","2","38","","ACM Trans. Inf. Syst.","Learning or Forgetting?","","","","","","","","","","","","March 2020","","","","","","","Diagnosis; educational theories; knowledge proficiency levels","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WL894H78","journalArticle","2020","Ma, Minghua; Yin, Zheng; Zhang, Shenglin; Wang, Sheng; Zheng, Christopher; Jiang, Xinhao; Hu, Hanwen; Luo, Cheng; Li, Yilin; Qiu, Nengjun; Li, Feifei; Chen, Changcheng; Pei, Dan","A A Diagnosing root causes of intermittent slow queries in cloud databases","Proceedings of the VLDB Endowment","","2150-8097","10.14778/3389133.3389136","https://doi.org/10.14778/3389133.3389136","With the growing market of cloud databases, careful detection and elimination of slow queries are of great importance to service stability. Previous studies focus on optimizing the slow queries that result from internal reasons (e.g., poorly-written SQLs). In this work, we discover a different set of slow queries which might be more hazardous to database users than other slow queries. We name such queries Intermittent Slow Queries (iSQs), because they usually result from intermittent performance issues that are external (e.g., at database or machine levels). Diagnosing root causes of iSQs is a tough but very valuable task. This paper presents iSQUAD, Intermittent Slow QUery Anomaly Diagnoser, a framework that can diagnose the root causes of iSQs with a loose requirement for human intervention. Due to the complexity of this issue, a machine learning approach comes to light naturally to draw the interconnection between iSQs and root causes, but it faces challenges in terms of versatility, labeling overhead and interpretability. To tackle these challenges, we design four components, i.e., Anomaly Extraction, Dependency Cleansing, Type-Oriented Pattern Integration Clustering (TOPIC) and Bayesian Case Model. iSQUAD consists of an offline clustering & explanation stage and an online root cause diagnosis & update stage. DBAs need to label each iSQ cluster only once at the offline stage unless a new type of iSQs emerges at the online stage. Our evaluations on real-world datasets from Alibaba OLTP Database show that iSQUAD achieves an iSQ root cause diagnosis average F1-score of 80.4%, and outperforms existing diagnostic tools in terms of accuracy and efficiency.","2020-04-01","2021-06-14 02:55:00","2021-06-15 08:17:46","2021-06-14 02:54:02","1176–1189","","8","13","","Proc. VLDB Endow.","","","","","","","","","","","","","April 2020","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N6796KVD","journalArticle","2018","Liu, Qi; Wu, Runze; Chen, Enhong; Xu, Guandong; Su, Yu; Chen, Zhigang; Hu, Guoping","Fuzzy Cognitive Diagnosis for Modelling Examinee Performance","ACM Transactions on Intelligent Systems and Technology","","2157-6904","10.1145/3168361","https://doi.org/10.1145/3168361","Recent decades have witnessed the rapid growth of educational data mining (EDM), which aims at automatically extracting valuable information from large repositories of data generated by or related to people’s learning activities in educational settings. One of the key EDM tasks is cognitive modelling with examination data, and cognitive modelling tries to profile examinees by discovering their latent knowledge state and cognitive level (e.g. the proficiency of specific skills). However, to the best of our knowledge, the problem of extracting information from both objective and subjective examination problems to achieve more precise and interpretable cognitive analysis remains underexplored. To this end, we propose a fuzzy cognitive diagnosis framework (FuzzyCDF) for examinees’ cognitive modelling with both objective and subjective problems. Specifically, to handle the partially correct responses on subjective problems, we first fuzzify the skill proficiency of examinees. Then we combine fuzzy set theory and educational hypotheses to model the examinees’ mastery on the problems based on their skill proficiency. Finally, we simulate the generation of examination score on each problem by considering slip and guess factors. In this way, the whole diagnosis framework is built. For further comprehensive verification, we apply our FuzzyCDF to three classical cognitive assessment tasks, i.e., predicting examinee performance, slip and guess detection, and cognitive diagnosis visualization. Extensive experiments on three real-world datasets for these assessment tasks prove that FuzzyCDF can reveal the knowledge states and cognitive level of the examinees effectively and interpretatively.","2018-01-31","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:03","48:1–48:26","","4","9","","ACM Trans. Intell. Syst. Technol.","","","","","","","","","","","","","February 2018","","","","C:\Users\Asus\Zotero\storage\M6BRNYRU\Liu et al. - 2018 - Fuzzy Cognitive Diagnosis for Modelling Examinee P.pdf","","","Cognitive; educational data mining; graphic model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EJPDFZ6E","journalArticle","2018","Bornholt, James; Torlak, Emina","Finding code that explodes under symbolic evaluation","Proceedings of the ACM on Programming Languages","","","10.1145/3276519","https://doi.org/10.1145/3276519","Solver-aided tools rely on symbolic evaluation to reduce programming tasks, such as verification and synthesis, to satisfiability queries. Many reusable symbolic evaluation engines are now available as part of solver-aided languages and frameworks, which have made it possible for a broad population of programmers to create and apply solver-aided tools to new domains. But to achieve results for real-world problems, programmers still need to write code that makes effective use of the underlying engine, and understand where their code needs careful design to elicit the best performance. This task is made difficult by the all-paths execution model of symbolic evaluators, which defies both human intuition and standard profiling techniques. This paper presents symbolic profiling, a new approach to identifying and diagnosing performance bottlenecks in programs under symbolic evaluation. To help with diagnosis, we develop a catalog of common performance anti-patterns in solver-aided code. To locate these bottlenecks, we develop SymPro, a new profiling technique for symbolic evaluation. SymPro identifies bottlenecks by analyzing two implicit resources at the core of every symbolic evaluation engine: the symbolic heap and symbolic evaluation graph. These resources form a novel performance model of symbolic evaluation that is general (encompassing all forms of symbolic evaluation), explainable (providing programmers with a conceptual framework for understanding symbolic evaluation), and actionable (enabling precise localization of bottlenecks). Performant solver-aided code carefully manages the shape of these implicit structures; SymPro makes their evolution explicit to the programmer. To evaluate SymPro, we implement profilers for the Rosette solver-aided language and the Jalangi program analysis framework. Applying SymPro to 15 published solver-aided tools, we discover 8 previously undiagnosed performance issues. Repairing these issues improves performance by orders of magnitude, and our patches were accepted by the tools' developers. We also conduct a small user study with Rosette programmers, finding that SymPro helps them both understand what the symbolic evaluator is doing and identify performance issues they could not otherwise locate.","2018-10-24","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:04","149:1–149:26","","OOPSLA","2","","Proc. ACM Program. Lang.","","","","","","","","","","","","","November 2018","","","","C:\Users\Asus\Zotero\storage\Q47KM9DN\Bornholt and Torlak - 2018 - Finding code that explodes under symbolic evaluati.pdf","","","profiling; solver-aided programming; symbolic execution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMHZMJ8C","journalArticle","2019","Raj, Shriti; Lee, Joyce M.; Garrity, Ashley; Newman, Mark W.","Clinical Data in Context: Towards Sensemaking Tools for Interpreting Personal Health Data","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3314409","https://doi.org/10.1145/3314409","Clinical data augmented with contextual data can help patients with chronic conditions make sense of their disease. However, existing tools do not support interpretation of multiple data streams. To better understand how individuals make sense of clinical and contextual data, we interviewed patients with Type 1 diabetes and their caregivers using context-enhanced visualizations of patients' data as probes to facilitate interpretation activities. We observed that our participants performed four analytical activities when interpreting their data -- finding context-based trends and explaining them, triangulating multiple factors, suggesting context-specific actions, and hypothesizing about alternate contextual factors affecting outcomes. We also observed two challenges encountered during analysis -- the inability to identify clear trends challenged action planning and counterintuitive insights compromised trust in data. Situating our findings within the existing sensemaking frameworks, we demonstrate that sensemaking can not only inform action but can guide the discovery of information needs for exploration. We further argue that sensemaking is a valuable approach for exploring contextual data. Informed by our findings and our reflection on existing sensemaking frameworks, we provide design guidelines for sensemaking tools to improve awareness of contextual factors affecting patients and to support patients' agency in making sense of health data.","2019-03-29","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:05","22:1–22:20","","1","3","","Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","Clinical Data in Context","","","","","","","","","","","","March 2019","","","","","","","diabetes; chronic disease management; context; data interpretation; patient-generated data; Personal informatics; reflection; sensemaking; visualizations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J9HR33HK","journalArticle","2020","Shneiderman, Ben","Bridging the Gap Between Ethics and Practice: Guidelines for Reliable, Safe, and Trustworthy Human-centered AI Systems","ACM Transactions on Interactive Intelligent Systems","","2160-6455","10.1145/3419764","https://doi.org/10.1145/3419764","This article attempts to bridge the gap between widely discussed ethical principles of Human-centered AI (HCAI) and practical steps for effective governance. Since HCAI systems are developed and implemented in multiple organizational structures, I propose 15 recommendations at three levels of governance: team, organization, and industry. The recommendations are intended to increase the reliability, safety, and trustworthiness of HCAI systems: (1) reliable systems based on sound software engineering practices, (2) safety culture through business management strategies, and (3) trustworthy certification by independent oversight. Software engineering practices within teams include audit trails to enable analysis of failures, software engineering workflows, verification and validation testing, bias testing to enhance fairness, and explainable user interfaces. The safety culture within organizations comes from management strategies that include leadership commitment to safety, hiring and training oriented to safety, extensive reporting of failures and near misses, internal review boards for problems and future plans, and alignment with industry standard practices. The trustworthiness certification comes from industry-wide efforts that include government interventions and regulation, accounting firms conducting external audits, insurance companies compensating for failures, non-governmental and civil society organizations advancing design principles, and professional organizations and research institutes developing standards, policies, and novel ideas. The larger goal of effective governance is to limit the dangers and increase the benefits of HCAI to individuals, organizations, and society.","2020-10-16","2021-06-14 02:55:00","2021-06-14 06:19:03","2021-06-14 02:54:06","26:1–26:31","","4","10","","ACM Trans. Interact. Intell. Syst.","Bridging the Gap Between Ethics and Practice","","","","","","","","","","","","December 2020","","","","C:\Users\Asus\Zotero\storage\IPV26HLT\Shneiderman - 2020 - Bridging the Gap Between Ethics and Practice Guid.pdf","","","Artificial Intelligence; design; Human-centered AI; Human-Computer Interaction; independent oversight; management strategies; reliable; safe; software engineering practices; trustworthy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47ZE7K7U","journalArticle","2019","Cousot, Patrick; Giacobazzi, Roberto; Ranzato, Francesco","A²I: abstract² interpretation","Proceedings of the ACM on Programming Languages","","","10.1145/3290355","https://doi.org/10.1145/3290355","The fundamental idea of Abstract2 Interpretation (A2I), also called meta-abstract interpretation, is to apply abstract interpretation to abstract interpretation-based static program analyses. A2I is generally meant to use abstract interpretation to analyse properties of program analysers. A2I can be either offline or online. Offline A2I is performed either before the program analysis, such as variable packing used by the Astrée program analyser, or after the program analysis, such as in alarm diagnosis. Online A2I is performed during the program analysis, such as Venet’s cofibred domains or Halbwachs et al.’s and Singh et al.’s variable partitioning techniques for fast polyhedra/numerical abstract domains. We formalize offline and online meta-abstract interpretation and illustrate this notion with the design of widenings and the decomposition of relational abstract domains to speed-up program analyses. This shows how novel static analyses can be extracted as meta-abstract interpretations to design efficient and precise program analysis algorithms.","2019-01-02","2021-06-14 02:55:00","2021-06-14 12:16:21","2021-06-14 02:54:07","42:1–42:31","","POPL","3","","Proc. ACM Program. Lang.","A&#xb2;I","","","","","","","","","","","","January 2019","","","","C:\Users\Asus\Zotero\storage\PQPYAVUD\Cousot et al. - 2019 - A&#xb2;I abstract&#xb2; interpretation.pdf","","","Abstract interpretation; meta-abstract interpretation; program analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2YHYKM9M","journalArticle","2018","Zhao, Yize; Kang, Jian; Long, Qi","Bayesian Multiresolution Variable Selection for Ultra-High Dimensional Neuroimaging Data","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","1545-5963","10.1109/TCBB.2015.2440244","https://doi.org/10.1109/TCBB.2015.2440244","Ultra-high dimensional variable selection has become increasingly important in analysis of neuroimaging data. For example, in the Autism Brain Imaging Data Exchange ABIDE study, neuroscientists are interested in identifying important biomarkers for early detection of the autism spectrum disorder ASD using high resolution brain images that include hundreds of thousands voxels. However, most existing methods are not feasible for solving this problem due to their extensive computational costs. In this work, we propose a novel multiresolution variable selection procedure under a Bayesian probit regression framework. It recursively uses posterior samples for coarser-scale variable selection to guide the posterior inference on finer-scale variable selection, leading to very efficient Markov chain Monte Carlo MCMC algorithms. The proposed algorithms are computationally feasible for ultra-high dimensional data. Also, our model incorporates two levels of structural information into variable selection using Ising priors: the spatial dependence between voxels and the functional connectivity between anatomical brain regions. Applied to the resting state functional magnetic resonance imaging R-fMRI data in the ABIDE study, our methods identify voxel-level imaging biomarkers highly predictive of the ASD, which are biologically meaningful and interpretable. Extensive simulations also show that our methods achieve better performance in variable selection compared to existing methods.","2018-03-01","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:08","537–550","","2","15","","IEEE/ACM Trans. Comput. Biol. Bioinformatics","","","","","","","","","","","","","March 2018","","","","C:\Users\Asus\Zotero\storage\DE4GPXCQ\Zhao et al. - 2018 - Bayesian Multiresolution Variable Selection for Ul.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQFV6VX8","journalArticle","2021","Hurley, Nathan C.; Spatz, Erica S.; Krumholz, Harlan M.; Jafari, Roozbeh; Mortazavi, Bobak J.","A Survey of Challenges and Opportunities in Sensing and Analytics for Risk Factors of Cardiovascular Disorders","ACM Transactions on Computing for Healthcare","","2691-1957","10.1145/3417958","https://doi.org/10.1145/3417958","Cardiovascular disorders cause nearly one in three deaths in the United States. Short- and long-term care for these disorders is often determined in short-term settings. However, these decisions are made with minimal longitudinal and long-term data. To overcome this bias towards data from acute care settings, improved longitudinal monitoring for cardiovascular patients is needed. Longitudinal monitoring provides a more comprehensive picture of patient health, allowing for informed decision making. This work surveys sensing and machine learning in the field of remote health monitoring for cardiovascular disorders. We highlight three needs in the design of new smart health technologies: (1) need for sensing technologies that track longitudinal trends of the cardiovascular disorder despite infrequent, noisy, or missing data measurements; (2) need for new analytic techniques designed in a longitudinal, continual fashion to aid in the development of new risk prediction techniques and in tracking disease progression; and (3) need for personalized and interpretable machine learning techniques, allowing for advancements in clinical decision making. We highlight these needs based upon the current state of the art in smart health technologies and analytics. We then discuss opportunities in addressing these needs for development of smart health technologies for the field of cardiovascular disorders and care.","2021-12-30","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:10","9:1–9:42","","1","2","","ACM Trans. Comput. Healthcare","","","","","","","","","","","","","January 2021","","","","","","","smart health; Cardiovascular disease; cardiovascular risk factors; longitudinal monitoring; patient analytics; sensors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PJVM6EKS","journalArticle","2021","Izotova, Elena; Kiesling, Jason; Martin, Fred","Students' consistency in computational modeling and their academic success","Journal of Computing Sciences in Colleges","","1937-4771","","","In this study, an assessment was designed to measure consistency in how subjects interpreted the effect of programming statements. The assessment consisted of 24 multiple-choice items which tested student interpretation of assignment and equality operators. Answers were analyzed to determine each subject's ""Consistency Score,"" which represents their consistency in this interpretation. The assessment was administered to computer science undergraduates at a public research university in the Northeast USA. The respondent results (n=128) were compared to the students' self-reported department GPA with the goal of determining whether consistency is correlated with student success. We found a positive correlation between a student's Consistency Score and their department GPA, with strong significance. This suggests the use of this instrument as a diagnostic for supporting students. This paper presents the design of the assessment, how the Consistency Score is calculated, and the study results.","2021-04-01","2021-06-14 02:55:00","2021-06-14 02:55:00","","63–72","","8","36","","J. Comput. Sci. Coll.","","","","","","","","","","","","","April 2021","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BMSQ4PQB","journalArticle","2020","Hind, Jade; Lisboa, Paulo; Hussain, Abir J.; Al-Jumeily, Dhiya","A Novel Approach to Detecting Epistasis using Random Sampling Regularisation","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","1545-5963","10.1109/TCBB.2019.2948330","https://doi.org/10.1109/TCBB.2019.2948330","Epistasis is a progressive approach that complements the `common disease, common variant' hypothesis that highlights the potential for connected networks of genetic variants collaborating to produce a phenotypic expression. Epistasis is commonly performed as a pairwise or limitless-arity capacity that considers variant networks as either variant vs variant or as high order interactions. This type of analysis extends the number of tests that were previously performed in a standard approach such as Genome-Wide Association Study (GWAS), in which False Discovery Rate (FDR) is already an issue, therefore by multiplying the number of tests up to a factorial rate also increases the issue of FDR. Further to this, epistasis introduces its own limitations of computational complexity and intensity that are generated based on the analysis performed; to consider the most intense approach, a multivariate analysis introduces a time complexity of O(n!). Proposed in this paper is a novel methodology for the detection of epistasis using interpretable methods and best practice to outline interactions through filtering processes. Using a process of Random Sampling Regularisation which randomly splits and produces sample sets to conduct a voting system to regularise the significance and reliability of biological markers, SNPs. Preliminary results are promising, outlining a concise detection of interactions. Results for the detection of epistasis, in the classification of breast cancer patients, indicated eight outlined risk candidate interactions from five variants and a singular candidate variant with high protective association.","2020-09-01","2021-06-14 02:55:00","2021-06-14 06:15:25","2021-06-14 02:54:11","1535–1545","","5","17","","IEEE/ACM Trans. Comput. Biol. Bioinformatics","","","","","","","","","","","","","Sept.-Oct. 2020","","","","C:\Users\Asus\Zotero\storage\SRLJ2AGF\Hind et al. - 2020 - A Novel Approach to Detecting Epistasis using Rand.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RA2EE768","journalArticle","2019","Ordookhanians, Allen; Li, Xin; Nakandala, Supun; Kumar, Arun","Demonstration of Krypton: optimized CNN inference for occlusion-based deep CNN explanations","Proceedings of the VLDB Endowment","","2150-8097","10.14778/3352063.3352093","https://doi.org/10.14778/3352063.3352093","In this demonstration, we present Krypton, a system for accelerating occlusion-based deep convolution neural network (CNN) explanation workloads. Driven by the success of CNNs in image understanding tasks, there is growing adoption of CNNs in various domains, including high stakes applications such as radiology. However, users of such applications often seek an ""explanation"" for why a CNN predicted a certain label. One of the most widely used approaches for explaining CNN predictions is the occlusion-based explanation (OBE) method. This approach is computationally expensive due to the large number of re-inference requests produced. Krypton reduces the runtime of OBE by up to 35x by enabling incremental and approximate inference optimizations that are inspired by classical database query optimization techniques. We allow the audience to interactively diagnose CNN predictions from several use cases, including radiology and natural images. A short video of our demonstration can be found here: https://youtu.be/1OWddbd4n6Y","2019-08-01","2021-06-14 02:55:00","2021-06-14 06:26:19","2021-06-14 02:54:12","1894–1897","","12","12","","Proc. VLDB Endow.","Demonstration of Krypton","","","","","","","","","","","","August 2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3VR5W3UK","journalArticle","2020","Tripathi, Suvidha; Singh, Satish Kumar","Cell Nuclei Classification in Histopathological Images using Hybrid OL ConvNet","ACM Transactions on Multimedia Computing, Communications, and Applications","","1551-6857","10.1145/3345318","https://doi.org/10.1145/3345318","Computer-aided histopathological image analysis for cancer detection is a major research challenge in the medical domain. Automatic detection and classification of nuclei for cancer diagnosis impose a lot of challenges in developing state-of-the-art algorithms due to the heterogeneity of cell nuclei and dataset variability. Recently, a multitude of classification algorithms have used complex deep learning models for their dataset. However, most of these methods are rigid, and their architectural arrangement suffers from inflexibility and non-interpretability. In this research article, we have proposed a hybrid and flexible deep learning architecture OLConvNet that integrates the interpretability of traditional object-level features and generalization of deep learning features by using a shallower Convolutional Neural Network (CNN) named as CNN3L. CNN3L reduces the training time by training fewer parameters and hence eliminating space constraints imposed by deeper algorithms. We used F1-score and multiclass Area Under the Curve (AUC) performance parameters to compare the results. To further strengthen the viability of our architectural approach, we tested our proposed methodology with state-of-the-art deep learning architectures AlexNet, VGG16, VGG19, ResNet50, InceptionV3, and DenseNet121 as backbone networks. After a comprehensive analysis of classification results from all four architectures, we observed that our proposed model works well and performs better than contemporary complex algorithms.","2020-03-12","2021-06-14 02:55:00","2021-06-17 04:31:47","2021-06-14 02:54:14","32:1–32:22","","1s","16","","ACM Trans. Multimedia Comput. Commun. Appl.","","","","","","","","","","","","","April 2020","","","","","","","Deep learning; transfer learning; cell nuclei classification; class balancing; convolutional neural networks; histopathological images; hybrid networks; multi layer perceptron; object-level features","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3B7D986C","journalArticle","2020","Jin, Zhuochen; Cui, Shuyuan; Guo, Shunan; Gotz, David; Sun, Jimeng; Cao, Nan","CarePre: An Intelligent Clinical Decision Assistance System","ACM Transactions on Computing for Healthcare","","2691-1957","10.1145/3344258","https://doi.org/10.1145/3344258","Clinical decision support systems are widely used to assist with medical decision making. However, clinical decision support systems typically require manually curated rules and other data that are difficult to maintain and keep up to date. Recent systems leverage advanced deep learning techniques and electronic health records to provide a more timely and precise result. Many of these techniques have been developed with a common focus on predicting upcoming medical events. However, although the prediction results from these approaches are promising, their value is limited by their lack of interpretability. To address this challenge, we introduce CarePre, an intelligent clinical decision assistance system. The system extends a state-of-the-art deep learning model to predict upcoming diagnosis events for a focal patient based on his or her historical medical records. The system includes an interactive framework together with intuitive visualizations designed to support diagnosis, treatment outcome analysis, and the interpretation of the analysis results. We demonstrate the effectiveness and usefulness of the CarePre system by reporting results from a quantities evaluation of the prediction algorithm, two case studies, and interviews with senior physicians and pulmonologists.","2020-03-02","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:15","6:1–6:20","","1","1","","ACM Trans. Comput. Healthcare","CarePre","","","","","","","","","","","","February 2020","","","","C:\Users\Asus\Zotero\storage\7VFGXK3E\Jin et al. - 2020 - CarePre An Intelligent Clinical Decision Assistan.pdf","","","neural networks; Personal health records; reasoning about belief and knowledge; user interface design; visual analytics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8PMJCKI9","journalArticle","2020","Bailey, Shawn; Zhang, Yue; Ramesh, Arti; Golbeck, Jennifer; Getoor, Lise","A Structured and Linguistic Approach to Understanding Recovery and Relapse in AA","ACM Transactions on the Web","","1559-1131","10.1145/3423208","https://doi.org/10.1145/3423208","Alcoholism, also known as Alcohol Use Disorder (AUD), is a serious problem affecting millions of people worldwide. Recovery from AUD is known to be challenging and often leads to relapse at various points after enrolling in a rehabilitation program such as Alcoholics Anonymous (AA). In this work, we present a structured and linguistic approach using hinge-loss Markov random fields (HL-MRFs) to understand recovery and relapse from AUD using social media data. We evaluate our models on AA-attending users extracted from: (i) the Twitter social network and predict recovery at two different points—90 days and 1 year after the user joins AA, respectively, and (ii) the Reddit AA recovery forums and predict whether the participating user is currently sober. The two datasets present two facets of the same underlying problem of understanding recovery and relapse in AUD users. We flesh out different characteristics in both these datasets: (i) In the Twitter dataset, we focus on the social aspect of the users and the relationship with recovery and relapse, and (ii) in the Reddit dataset, we focus on modeling the linguistic topics and dependency structure to understand users’ recovery journey. We design a unified modeling framework using HL-MRFs that takes the different characteristics of both these platforms into account. Our experiments reveal that our structured and linguistic approach is helpful in predicting recovery in users in both these datasets. We perform extensive quantitative analysis of different groups of features and dependencies among them in both datasets. The interpretable and intuitive nature of our models and analysis is helpful in making meaningful predictions and can potentially be helpful in identifying and preventing relapse early.","2020-11-05","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:17","5:1–5:35","","1","15","","ACM Trans. Web","","","","","","","","","","","","","January 2021","","","","","","","alcoholics anonymous; modeling recovery from alcoholism; probabilistic graphical models; Social media analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39V3YBTV","journalArticle","2017","Zhang, Yuankai; O'Neill, Adam; Sherr, Micah; Zhou, Wenchao","Privacy-preserving network provenance","Proceedings of the VLDB Endowment","","2150-8097","10.14778/3137628.3137661","https://doi.org/10.14778/3137628.3137661","Network accountability, forensic analysis, and failure diagnosis are becoming increasingly important for network management and security. Network provenance significantly aids network administrators in these tasks by explaining system behavior and revealing the dependencies between system states. Although resourceful, network provenance can sometimes be too rich, revealing potentially sensitive information that was involved in system execution. In this paper, we propose a cryptographic approach to preserve the confidentiality of provenance (sub)graphs while allowing users to query and access the parts of the graph for which they are authorized. Our proposed solution is a novel application of searchable symmetric encryption (SSE) and more generally structured encryption (SE). Our SE-enabled provenance system allows a node to enforce access control policies over its provenance data even after the data has been shipped to remote nodes (e.g., for optimization purposes). We present a prototype of our design and demonstrate its practicality, scalability, and efficiency for both provenance maintenance and querying.","2017-08-01","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:18","1550–1561","","11","10","","Proc. VLDB Endow.","","","","","","","","","","","","","August 2017","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AD6JLLUC","journalArticle","2017","Zhang, Danfeng; Myers, Andrew C.; Vytiniotis, Dimitrios; Peyton-Jones, Simon","SHErrLoc: A Static Holistic Error Locator","ACM Transactions on Programming Languages and Systems","","0164-0925","10.1145/3121137","https://doi.org/10.1145/3121137","We introduce a general way to locate programmer mistakes that are detected by static analyses. The program analysis is expressed in a general constraint language that is powerful enough to model type checking, information flow analysis, dataflow analysis, and points-to analysis. Mistakes in program analysis result in unsatisfiable constraints. Given an unsatisfiable system of constraints, both satisfiable and unsatisfiable constraints are analyzed to identify the program expressions most likely to be the cause of unsatisfiability. The likelihood of different error explanations is evaluated under the assumption that the programmer’s code is mostly correct, so the simplest explanations are chosen, following Bayesian principles. For analyses that rely on programmer-stated assumptions, the diagnosis also identifies assumptions likely to have been omitted. The new error diagnosis approach has been implemented as a tool called SHErrLoc, which is applied to three very different program analyses, such as type inference for a highly expressive type system implemented by the Glasgow Haskell Compiler—including type classes, Generalized Algebraic Data Types (GADTs), and type families. The effectiveness of the approach is evaluated using previously collected programs containing errors. The results show that when compared to existing compilers and other tools, SHErrLoc consistently identifies the location of programmer errors significantly more accurately, without any language-specific heuristics.","2017-08-17","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:19","18:1–18:47","","4","39","","ACM Trans. Program. Lang. Syst.","SHErrLoc","","","","","","","","","","","","September 2017","","","","C:\Users\Asus\Zotero\storage\HKKRSMLN\Zhang et al. - 2017 - SHErrLoc A Static Holistic Error Locator.pdf","","","Error diagnosis; Haskell; information flow; Jif; OCaml; static program analysis; type inference","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"74QDLCST","journalArticle","2018","Can, Gülcan; Odobez, Jean-Marc; Gatica-Perez, Daniel","How to Tell Ancient Signs Apart? Recognizing and Visualizing Maya Glyphs with CNNs","Journal on Computing and Cultural Heritage","","1556-4673","10.1145/3230670","https://doi.org/10.1145/3230670","Thanks to the digital preservation of cultural heritage materials, multimedia tools (e.g., based on automatic visual processing) considerably ease the work of scholars in the humanities and help them to perform quantitative analysis of their data. In this context, this article assesses three different Convolutional Neural Network (CNN) architectures along with three learning approaches to train them for hieroglyph classification, which is a very challenging task due to the limited availability of segmented ancient Maya glyphs. More precisely, the first approach, the baseline, relies on pretrained networks as feature extractor. The second one investigates a transfer learning method by fine-tuning a pretrained network for our glyph classification task. The third approach considers directly training networks from scratch with our glyph data. The merits of three different network architectures are compared: a generic sequential model (i.e., LeNet), a sketch-specific sequential network (i.e., Sketch-a-Net), and the recent Residual Networks. The sketch-specific model trained from scratch outperforms other models and training strategies. Even for a challenging 150-class classification task, this model achieves 70.3% average accuracy and proves itself promising in case of a small amount of cultural heritage shape data. Furthermore, we visualize the discriminative parts of glyphs with the recent Grad-CAM method, and demonstrate that the discriminative parts learned by the model agree, in general, with the expert annotation of the glyph specificity (diagnostic features). Finally, as a step toward systematic evaluation of these visualizations, we conduct a perceptual crowdsourcing study. Specifically, we analyze the interpretability of the representations from Sketch-a-Net and ResNet-50. Overall, our article takes two important steps toward providing tools to scholars in the digital humanities: increased performance for automation and improved interpretability of algorithms.","2018-12-05","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:20","20:1–20:25","","4","11","","J. Comput. Cult. Herit.","How to Tell Ancient Signs Apart?","","","","","","","","","","","","December 2018","","","","C:\Users\Asus\Zotero\storage\K6ZV62TY\Can et al. - 2018 - How to Tell Ancient Signs Apart Recognizing and V.pdf","","","transfer learning; convolutional neural networks; crowdsourcing; Maya glyphs; shape recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I4EMJ336","journalArticle","2016","Morrison-Smith, Sarah; Hofmann, Megan; Li, Yang; Ruiz, Jaime","Using Audio Cues to Support Motion Gesture Interaction on Mobile Devices","ACM Transactions on Applied Perception","","1544-3558","10.1145/2897516","https://doi.org/10.1145/2897516","Motion gestures are an underutilized input modality for mobile interaction despite numerous potential advantages. Negulescu et al. found that the lack of feedback on attempted motion gestures made it difficult for participants to diagnose and correct errors, resulting in poor recognition performance and user frustration. In this article, we describe and evaluate a training and feedback technique, Glissando, which uses audio characteristics to provide feedback on the system’s interpretation of user input. This technique enables feedback by verbally confirming correct gestures and notifying users of errors in addition to providing continuous feedback by manipulating the pitch of distinct musical notes mapped to each of three dimensional axes in order to provide both spatial and temporal information.","2016-05-28","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:21","16:1–16:19","","3","13","","ACM Trans. Appl. Percept.","","","","","","","","","","","","","May 2016","","","","C:\Users\Asus\Zotero\storage\US2NAP38\Morrison-Smith et al. - 2016 - Using Audio Cues to Support Motion Gesture Interac.pdf","","","audio feedback; mobile interaction; Motion gestures","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NGVAYT2","journalArticle","2017","Ma, Haiwei; Smith, C. Estelle; He, Lu; Narayanan, Saumik; Giaquinto, Robert A.; Evans, Roni; Hanson, Linda; Yarosh, Svetlana","Write for Life: Persisting in Online Health Communities through Expressive Writing and Social Support","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3134708","https://doi.org/10.1145/3134708","Expressive writing has been shown to improve physical, mental, and social health outcomes for patients struggling with difficult diagnoses. In many online health communities, writing comprises a substantial portion of the user experience, yet little work has explored how writing itself affects user engagement. This paper explores user engagement on CaringBridge, a prominent online community for writing about personal health journeys. We build a survival analysis model, defining a new set of variables that operationalize expressive writing, and comparing these effects to those of social support, which are well-known to benefit user engagement. Furthermore, we use machine learning methods to estimate that approximately one third of community members who self-identify with a cancer condition cease engagement due to literal death. Finally, we provide quantitative evidence that: (1) receiving support, expressive writing, and giving support, in decreasing magnitude of relative impact, are associated with user engagement on CaringBridge, and (2) that considering deceased sites separately in our analysis significantly shifts our interpretations of user behavior.","2017-12-06","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:22","73:1–73:24","","CSCW","1","","Proc. ACM Hum.-Comput. Interact.","Write for Life","","","","","","","","","","","","November 2017","","","","C:\Users\Asus\Zotero\storage\AHENIVSK\Ma et al. - 2017 - Write for Life Persisting in Online Health Commun.pdf","","","CaringBridge; death; expressive writing; online health community; social support; user engagement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JU74L9FR","journalArticle","2015","Liu, Jin-Xing; Xu, Yong; Zheng, Chun-Hou; Kong, Heng; Lai, Zhi-Hui","RPCA-based tumor classification using gene expression data","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","1545-5963","10.1109/TCBB.2014.2383375","https://doi.org/10.1109/TCBB.2014.2383375","Microarray techniques have been used to delineate cancer groups or to identify candidate genes for cancer prognosis. As such problems can be viewed as classification ones, various classification methods have been applied to analyze or interpret gene expression data. In this paper, we propose a novel method based on robust principal component analysis (RPCA) to classify tumor samples of gene expression data. Firstly, RPCA is utilized to highlight the characteristic genes associated with a special biological process. Then, RPCA and RPCA+LDA (robust principal component analysis and linear discriminant analysis) are used to identify the features. Finally, support vector machine (SVM) is applied to classify the tumor samples of gene expression data based on the identified features. Experiments on seven data sets demonstrate that our methods are effective and feasible for tumor classification.","2015-07-01","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:23","964–970","","4","12","","IEEE/ACM Trans. Comput. Biol. Bioinformatics","","","","","","","","","","","","","July/August 2015","","","","","","","feature selection; classification; data mining; principal component analysis; sparse method","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5DCXE44N","journalArticle","2019","Zeng, Zexian; Deng, Yu; Li, Xiaoyu; Naumann, Tristan; Luo, Yuan","Natural Language Processing for EHR-Based Computational Phenotyping","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","1545-5963","10.1109/TCBB.2018.2849968","https://doi.org/10.1109/TCBB.2018.2849968","This article reviews recent advances in applying natural language processing NLP to Electronic Health Records EHRs for computational phenotyping. NLP-based computational phenotyping has numerous applications including diagnosis categorization, novel phenotype discovery, clinical trial screening, pharmacogenomics, drug-drug interaction DDI, and adverse drug event ADE detection, as well as genome-wide and phenome-wide association studies. Significant progress has been made in algorithm development and resource construction for computational phenotyping. Among the surveyed methods, well-designed keyword search and rule-based systems often achieve good performance. However, the construction of keyword and rule lists requires significant manual effort, which is difficult to scale. Supervised machine learning models have been favored because they are capable of acquiring both classification patterns and structures from data. Recently, deep learning and unsupervised learning have received growing attention, with the former favored for its performance and the latter for its ability to find novel phenotypes. Integrating heterogeneous data sources have become increasingly important and have shown promise in improving model performance. Often, better performance is achieved by combining multiple modalities of information. Despite these many advances, challenges and opportunities remain for NLP-based computational phenotyping, including better model interpretability and generalizability, and proper characterization of feature relations in clinical narratives.","2019-01-01","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:26","139–153","","1","16","","IEEE/ACM Trans. Comput. Biol. Bioinformatics","","","","","","","","","","","","","January 2019","","","","C:\Users\Asus\Zotero\storage\3IYWSBZD\Zeng et al. - 2019 - Natural Language Processing for EHR-Based Computat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3V4JEMSW","journalArticle","2020","Paul, Sushmita; Madhumita","Computational Method for Identification of miRNA-mRNA Regulatory Modules in Cervical Cancer","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","1545-5963","10.1109/TCBB.2019.2910851","https://doi.org/10.1109/TCBB.2019.2910851","Cervical cancer is a leading severe malignancy throughout the world. Molecular processes and biomarkers leading to tumor progression in cervical cancer are either unknown or only partially understood. An increasing number of studies have shown that microRNAs play an important role in tumorigenesis so understanding the regulatory mechanism of miRNAs in gene-regulatory network will help elucidate the complex biological processes that occur during malignancy. Functional genomics data provides opportunities to study the aberrant microRNA-messenger RNA (miRNA-mRNA) interaction. Identification of miRNA-mRNA regulatory modules will aid deciphering aberrant transcriptional regulatory network in cervical cancer but is computationally challenging. In this regard, an algorithm, termed as relevant and functionally consistent miRNA-mRNA modules (RFCM<sup>3</sup>), is proposed. It integrates miRNA and mRNA expression data of cervical cancer for identification of potential miRNA-mRNA modules. It selects set of miRNA-mRNA modules by maximizing relation of mRNAs with miRNA and functional similarity between selected mRNAs. Later, using the knowledge of the miRNA-miRNA synergistic network different modules are fused and finally a set of modules are generated containing several miRNAs as well as mRNAs. This type of module explains the underlying biological pathways containing multiple miRNAs and mRNAs. The effectiveness of the proposed approach over other existing methods has been demonstrated on a miRNA and mRNA expression data of cervical cancer with respect to enrichment analyses and other standard metrices. The prognostic value of the genes in a module with respect to cervical cancer is also demonstrated. The approach was found to generate more robust, integrated, and functionally enriched miRNA-mRNA modules in cervical cancer.","2020-09-01","2021-06-14 02:55:00","2021-06-14 06:04:03","2021-06-14 02:54:26","1729–1740","","5","17","","IEEE/ACM Trans. Comput. Biol. Bioinformatics","&lt;italic&gt;RFCM&lt;sup&gt;3&lt;/sup&gt;&lt;/italic&gt;","","","","","","","","","","","","Sept.-Oct. 2020","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3X7JXI3J","journalArticle","2019","Wang, Ji; Bao, Weidong; Zheng, Lei; Zhu, Xiaomin; Yu, Philip S.","A A An Attention-augmented Deep Architecture for Hard Drive Status Monitoring in Large-scale Storage Systems","ACM Transactions on Storage","","1553-3077","10.1145/3340290","https://doi.org/10.1145/3340290","Data centers equipped with large-scale storage systems are critical infrastructures in the era of big data. The enormous amount of hard drives in storage systems magnify the failure probability, which may cause tremendous loss for both data service users and providers. Despite a set of reactive fault-tolerant measures such as RAID, it is still a tough issue to enhance the reliability of large-scale storage systems. Proactive prediction is an effective method to avoid possible hard-drive failures in advance. A series of models based on the SMART statistics have been proposed to predict impending hard-drive failures. Nonetheless, there remain some serious yet unsolved challenges like the lack of explainability of prediction results. To address these issues, we carefully analyze a dataset collected from a real-world large-scale storage system and then design an attention-augmented deep architecture for hard-drive health status assessment and failure prediction. The deep architecture, composed of a feature integration layer, a temporal dependency extraction layer, an attention layer, and a classification layer, cannot only monitor the status of hard drives but also assist in failure cause diagnoses. The experiments based on real-world datasets show that the proposed deep architecture is able to assess the hard-drive status and predict the impending failures accurately. In addition, the experimental results demonstrate that the attention-augmented deep architecture can reveal the degradation progression of hard drives automatically and assist administrators in tracing the cause of hard drive failures.","2019-08-13","2021-06-14 02:55:00","2021-06-15 08:17:41","2021-06-14 02:54:28","21:1–21:26","","3","15","","ACM Trans. Storage","","","","","","","","","","","","","August 2019","","","","","","","attention mechanism; deep neural network; Hard drive failure; recurrent neural network; SMART","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"83HGXLS4","journalArticle","2015","Pinto, Marcos; Hasam, Mamun; Wei, Hsinrong","Safe AI: creating a health care expert system","Journal of Computing Sciences in Colleges","","1937-4771","","","This paper describes the process of creating a health care expert system (ES) step-by-step. An expert system, a branch of Artificial Intelligence (AI), will be designed using rule-based method to indicate the most possible diagnosis for upper respiratory infections (URI) such as common cold, flu (influenza), croup, and sinus infection. The expert system presents the user with a list of possible diagnoses and their corresponding explanations depending on certain user's information: age, gender, procedence region, and medical history. It is a hands-on example that can be incorporated into college courses on Web services and/or mobile applications.","2015-12-01","2021-06-14 02:55:00","2021-06-14 02:55:00","","58–64","","2","31","","J. Comput. Sci. Coll.","Safe AI","","","","","","","","","","","","December 2015","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YP75TU5Z","journalArticle","2015","Yoon, Dong Young; Mozafari, Barzan; Brown, Douglas P.","DBSeer: pain-free database administration through workload intelligence","Proceedings of the VLDB Endowment","","2150-8097","10.14778/2824032.2824130","https://doi.org/10.14778/2824032.2824130","The pressing need for achieving and maintaining high performance in database systems has made database administration one of the most stressful jobs in information technology. On the other hand, the increasing complexity of database systems has made qualified database administrators (DBAs) a scarce resource. DBAs are now responsible for an array of demanding tasks; they need to (i) provision and tune their database according to their application requirements, (ii) constantly monitor their database for any performance failures or slowdowns, (iii) diagnose the root cause of the performance problem in an accurate and timely fashion, and (iv) take prompt actions that can restore acceptable database performance. However, much of the research in the past years has focused on improving the raw performance of the database systems, rather than improving their manageability. Besides sophisticated consoles for monitoring performance and a few auto-tuning wizards, DBAs are not provided with any help other than their own many years of experience. Typically, their only resort is trial-and-error, which is a tedious, ad-hoc and often sub-optimal solution. In this demonstration, we present DBSeer, a workload intelligence framework that exploits advanced machine learning and causality techniques to aid DBAs in their various responsibilities. DBSeer analyzes large volumes of statistics and telemetry data collected from various log files to provide the DBA with a suite of rich functionalities including performance prediction, performance diagnosis, bottleneck explanation, workload insight, optimal admission control, and what-if analysis. In this demo, we showcase various features of DBSeer by predicting and analyzing the performance of a live database system. Will also reproduce a number of realistic performance problems in the system, and allow the audience to use DBSeer to quickly diagnose and resolve their root cause.","2015-08-01","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:31","2036–2039","","12","8","","Proc. VLDB Endow.","DBSeer","","","","","","","","","","","","August 2015","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E6JWI9AQ","journalArticle","2020","Qiao, Yan; Jiao, Jun; Cui, Xinhong; Rao, Yuan","A A Robust Loss Inference in the Presence of Noisy Measurements and Hidden Fault Diagnosis","IEEE/ACM Transactions on Networking","","1063-6692","10.1109/TNET.2019.2948818","https://doi.org/10.1109/TNET.2019.2948818","This paper addresses the problem of inferring link loss rates based on network performance tomography in noisy network systems. Since network tomography emerged, all existing tomography-based methods are limited to the fulfillment of a basic condition: both network topologies and end-to-end routes must be absolutely accurate, which in most cases is impractical, especially for large-scale heterogeneous networks. To overcome the impracticability of tomography-based methods, we propose a robust tomography-based loss inference method capable of accurately inferring all link loss rates even when the given knowledge about the system is unreliable. Rather than computing the link loss rates directly from end-to-end loss rates, it calculates an upper bound for each link loss rate. It then infers all the link loss rates that most closely conform to the measurement results within their upper bounds. For a scenario where noisy measurements are caused by link (or router port) failures, we propose a hidden fault diagnosis approach that utilizes the inferred link loss rates to pinpoint the insidious faults that are hardly detected. It first determines the possible fake routes based on inferred link loss rates. Subsequently, it finds the maximum probable faults that can best explain the fake routes. Through intensive experiments, the results strongly confirm the promising performance of our proposed approaches.","2020-02-01","2021-06-14 02:55:00","2021-06-15 08:17:51","2021-06-14 02:54:33","43–56","","1","28","","IEEE/ACM Trans. Netw.","","","","","","","","","","","","","Feb. 2020","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TNQSH7GC","journalArticle","2017","Xie, Wei; Li, Cheng; Zhang, Pu","A Factor-Based Bayesian Framework for Risk Analysis in Stochastic Simulations","ACM Transactions on Modeling and Computer Simulation","","1049-3301","10.1145/3154387","https://doi.org/10.1145/3154387","Simulation is commonly used to study the random behaviors of large-scale stochastic systems with correlated inputs. Since the input correlation is often induced by latent common factors in many situations, to facilitate system diagnostics and risk management, we introduce a factor-based Bayesian framework that can improve both computational and statistical efficiency and provide insights for system risk analysis. Specifically, we develop a flexible Gaussian copula-based multivariate input model that can capture important properties in the real-world data. A nonparametric Bayesian approach is used to model marginal distributions, and it can capture the properties, including multi-modality and skewness. We explore the factor structure of the underlying generative processes for the dependence. Both input and simulation estimation uncertainty are characterized by the posterior distributions. In addition, we interpret the latent factors and estimate their effects on the system performance, which could be used to support diagnostics and decision making for large-scale stochastic systems. Our approach is supported by both asymptotic theory and empirical study.","2017-12-20","2021-06-14 02:55:00","2021-06-14 06:15:13","2021-06-14 02:54:35","27:1–27:31","","4","27","","ACM Trans. Model. Comput. Simul.","","","","","","","","","","","","","December 2017","","","","","","","factor model; gaussian copula; Multivariate input model; nonparametric model; risk analysis; simulation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XTCP2QWH","journalArticle","2019","Ng, Ada; Kornfield, Rachel; Schueller, Stephen M.; Zalta, Alyson K.; Brennan, Michael; Reddy, Madhu","Provider Perspectives on Integrating Sensor-Captured Patient-Generated Data in Mental Health Care","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3359217","https://doi.org/10.1145/3359217","The increasing ubiquity of health sensing technology holds promise to enable patients and health care providers to make more informed decisions based on continuously-captured data. The use of sensor-captured patient-generated data (sPGD) has been gaining greater prominence in the assessment of physical health, but we have little understanding of the role that sPGD can play in mental health. To better understand the use of sPGD in mental health, we interviewed care providers in an intensive treatment program (ITP) for veterans with post-traumatic stress disorder. In this program, patients were given Fitbits for their own voluntary use. Providers identified a number of potential benefits from patients' Fitbit use, such as patient empowerment and opportunities to reinforce therapeutic progress through collaborative data review and interpretation. However, despite the promise of sensor data as offering an ""objective"" view into patients' health behavior and symptoms, the relationships between sPGD and therapeutic progress are often ambiguous. Given substantial subjectivity involved in interpreting data from commercial wearables in the context of mental health treatment, providers emphasized potential risks to their patients and were uncertain how to adjust their practice to effectively guide collaborative use of the FitBit and its sPGD. We discuss the implications of these findings for designing systems to leverage sPGD in mental health care.?","2019-11-07","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:36","115:1–115:25","","CSCW","3","","Proc. ACM Hum.-Comput. Interact.","","","","","","","","","","","","","November 2019","","","","C:\Users\Asus\Zotero\storage\C7E8ZCG6\Ng et al. - 2019 - Provider Perspectives on Integrating Sensor-Captur.pdf","","","patient-generated data; sensors; mental health; post-traumatic stress disorder; wearables","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J2E2BKTA","journalArticle","2020","Xia, Yan; Zhu, Haiyi; Lu, Tun; Zhang, Peng; Gu, Ning","Exploring Antecedents and Consequences of Toxicity in Online Discussions: A Case Study on Reddit","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3415179","https://doi.org/10.1145/3415179","Toxicity in online discussions has been an intriguing phenomenon and an important problem. In this paper, we seek to better understand toxicity dynamics in online discussions via a case study on Reddit that explores the antecedents and consequences of toxicity in text. We inspected two dimensions of toxicity: language toxicity, i.e. how toxic the text itself is; and toxicity elicitation, i.e. how much toxicity it elicits in its response. Through regression analyses on Reddit comments, we found that both author propensity and toxicity in discussion context were strong positive antecedents of language toxicity; meanwhile, language toxicity significantly increased the volume and user evaluation of the discussion in some sub-communities, while toxicity elicitation showed mixed effects. We then discuss how our results help understand and regulate toxicity in online discussions by interpreting the complicated triggers and outcomes of toxicity.","2020-10-14","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:37","108:1–108:23","","CSCW2","4","","Proc. ACM Hum.-Comput. Interact.","Exploring Antecedents and Consequences of Toxicity in Online Discussions","","","","","","","","","","","","October 2020","","","","","","","online discussions; quantitative analysis; reddit; toxicity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GTAHK8MF","journalArticle","2019","Paul, Amit; Sil, Jaya","Identification of Differentially Expressed Genes to Establish New Biomarker for Cancer Prediction","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","1545-5963","10.1109/TCBB.2018.2837095","https://doi.org/10.1109/TCBB.2018.2837095","The goal of the human genome project is to integrate genetic information into different clinical therapies. To achieve this goal, different computational algorithms are devised for identifying the biomarker genes, cause of complex diseases. However, most of the methods developed so far using DNA microarray data lack in interpreting biological findings and are less accurate in disease prediction. In the paper, we propose two parameters <inline-formula><tex-math notation=""LaTeX"">$risk\_factor$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mo>_</mml:mo><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""sil-ieq1-2837095.gif""/></alternatives></inline-formula> and <inline-formula><tex-math notation=""LaTeX"">$confusion\_factor$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""sil-ieq2-2837095.gif""/></alternatives></inline-formula> to identify the biologically significant genes for cancer development. First, we evaluate <inline-formula><tex-math notation=""LaTeX"">$risk\_factor$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mo>_</mml:mo><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""sil-ieq3-2837095.gif""/></alternatives></inline-formula> of each gene and the genes with nonzero <inline-formula><tex-math notation=""LaTeX"">$risk\_factor$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mo>_</mml:mo><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""sil-ieq4-2837095.gif""/></alternatives></inline-formula> result misclassification of data, therefore removed. Next, we calculate <inline-formula><tex-math notation=""LaTeX"">$confusion\_factor$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""sil-ieq5-2837095.gif""/></alternatives></inline-formula> of the remaining genes which determines confusion of a gene in prediction due to closeness of the samples in the cancer and normal classes. We apply nondominated sorting genetic algorithm (NSGA-II) to select the maximally uncorrelated differentially expressed genes in the cancer class with minimum <inline-formula><tex-math notation=""LaTeX"">$confusion\_factor$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""sil-ieq6-2837095.gif""/></alternatives></inline-formula>. The proposed Gene Selection Explore (GSE) algorithm is compared to well established feature selection algorithms using 10 microarray data with respect to sensitivity, specificity, and accuracy. The identified genes appear in KEGG pathway and have several biological importance.","2019-11-01","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:39","1970–1985","","6","16","","IEEE/ACM Trans. Comput. Biol. Bioinformatics","","","","","","","","","","","","","Nov.-Dec. 2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ILB67D9V","journalArticle","2016","Tan, Jianchao; Lien, Jyh-Ming; Gingold, Yotam","Decomposing Images into Layers via RGB-Space Geometry","ACM Transactions on Graphics","","0730-0301","10.1145/2988229","https://doi.org/10.1145/2988229","In digital image editing software, layers organize images. However, layers are often not explicitly represented in the final image, and may never have existed for a scanned physical painting or a photograph. We propose a technique to decompose an image into layers. In our decomposition, each layer represents a single-color coat of paint applied with varying opacity. Our decomposition is based on the image’s RGB-space geometry. In RGB-space, the linear nature of the standard Porter-Duff [1984] “over” pixel compositing operation implies a geometric structure. The vertices of the convex hull of image pixels in RGB-space correspond to a palette of paint colors. These colors may be “hidden” and inaccessible to algorithms based on clustering visible colors. For our layer decomposition, users choose the palette size (degree of simplification to perform on the convex hull), as well as a layer order for the paint colors (vertices). We then solve a constrained optimization problem to find translucent, spatially coherent opacity for each layer, such that the composition of the layers reproduces the original image. We demonstrate the utility of the resulting decompositions for recoloring (global and local) and object insertion. Our layers can be interpreted as generalized barycentric coordinates; we compare to these and other recoloring approaches.","2016-11-15","2021-06-14 02:55:00","2021-06-14 09:36:16","2021-06-14 02:54:40","7:1–7:14","","1","36","","ACM Trans. Graph.","","","","","","","","","","","","","February 2017","","","","","","","colors; Images; layers; Photoshop; RGB","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCTUEUX2","journalArticle","2019","Ch'ng, Eugene","Art by Computing Machinery: Is Machine Art Acceptable in the Artworld?","ACM Transactions on Multimedia Computing, Communications, and Applications","","1551-6857","10.1145/3326338","https://doi.org/10.1145/3326338","When does a machine-created work becomes art? What is art? Can machine artworks fit in to the historical and present discourse? Do machine artworks demonstrate creativity, or are they a type of new media from which artists extend their creativity with? Will solely machine-created artworks be acceptable by our artworlds? This article probes these questions by first identifying the frameworks for defining and explaining art and evaluating its suitability for explaining machine artworks. It then explores how artworks have a necessary relationship with their human artists and the wider context of history, institutions, styles, and approaches and with audiences and artworlds. The article then questions whether machines have such a relational context and whether machines will ever live up to our standard of what constitutes an artwork as defined by us or whether machines are good only for assisting creativity. The question of intellectual property, rights, and ownership are also discussed for human--machine artworks and purely machine-produced works of art. The article critically assesses the viability of machines as artists as the central question in the historical discourse, extended through art and the artworld and evaluates machine-produced work from such a basis.","2019-07-03","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:42","59:1–59:17","","2s","15","","ACM Trans. Multimedia Comput. Commun. Appl.","Art by Computing Machinery","","","","","","","","","","","","August 2019","","","","","","","art theory; artworlds; Machine art; machine artist; machine artworks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9C4VRM5Y","journalArticle","2017","Athanasiadis, Emmanouil; Bourdakou, Marilena; Spyrou, George","D-Map: Random Walking on Gene Network Inference Maps Towards differential Avenue Discovery","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","1545-5963","10.1109/TCBB.2016.2535267","https://doi.org/10.1109/TCBB.2016.2535267","Differential rewiring of cellular interaction networks between disease and healthy state is of great importance. Through a systems level approach, malfunctioned mechanisms that are absent in the normal cases, may enlighten the key-players in terms of genes and their interaction chains related to disease. We have developed D-Map, a publicly available user-friendly web application, capable of generating and manipulating advanced differential networks by combining state-of-the-art inference reconstruction methods with random walk simulations. The inputs are expression profiles obtained from the Gene Expression Omnibus and a gene list under investigation. Differential networks may be visualized and interpreted through the use of D-Map interface, where display of the disease, the normal and the common state can be performed, interactively. A case study scenario concerning Alzheimer's disease, as well as breast, lung, and bladder cancer was conducted in order to demonstrate the usefulness of the proposed methodology to different disease types. Findings were consistent with the current bibliography, and the provided interaction lists may be further explored towards novel biological insights of the investigated diseases. The DMap web-application is available at: http://bioserver-3.bioacademy.gr/Bioserver/DMap/index.php.","2017-03-01","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:44","484–490","","2","14","","IEEE/ACM Trans. Comput. Biol. Bioinformatics","D-Map","","","","","","","","","","","","March 2017","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FUHBBLSK","journalArticle","2020","Thambawita, Vajira; Jha, Debesh; Hammer, Hugo Lewi; Johansen, Håvard D.; Johansen, Dag; Halvorsen, Pål; Riegler, Michael A.","An Extensive Study on Cross-Dataset Bias and Evaluation Metrics Interpretation for Machine Learning Applied to Gastrointestinal Tract Abnormality Classification","ACM Transactions on Computing for Healthcare","","2691-1957","10.1145/3386295","https://doi.org/10.1145/3386295","Precise and efficient automated identification of gastrointestinal (GI) tract diseases can help doctors treat more patients and improve the rate of disease detection and identification. Currently, automatic analysis of diseases in the GI tract is a hot topic in both computer science and medical-related journals. Nevertheless, the evaluation of such an automatic analysis is often incomplete or simply wrong. Algorithms are often only tested on small and biased datasets, and cross-dataset evaluations are rarely performed. A clear understanding of evaluation metrics and machine learning models with cross datasets is crucial to bring research in the field to a new quality level. Toward this goal, we present comprehensive evaluations of five distinct machine learning models using global features and deep neural networks that can classify 16 different key types of GI tract conditions, including pathological findings, anatomical landmarks, polyp removal conditions, and normal findings from images captured by common GI tract examination instruments. In our evaluation, we introduce performance hexagons using six performance metrics, such as recall, precision, specificity, accuracy, F1-score, and the Matthews correlation coefficient to demonstrate how to determine the real capabilities of models rather than evaluating them shallowly. Furthermore, we perform cross-dataset evaluations using different datasets for training and testing. With these cross-dataset evaluations, we demonstrate the challenge of actually building a generalizable model that could be used across different hospitals. Our experiments clearly show that more sophisticated performance metrics and evaluation methods need to be applied to get reliable models rather than depending on evaluations of the splits of the same dataset—that is, the performance metrics should always be interpreted together rather than relying on a single metric.","2020-06-22","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:46","17:1–17:29","","3","1","","ACM Trans. Comput. Healthcare","","","","","","","","","","","","","July 2020","","","","C:\Users\Asus\Zotero\storage\UZWTIXXP\Thambawita et al. - 2020 - An Extensive Study on Cross-Dataset Bias and Evalu.pdf","","","deep learning; computer-aided diagnosis; cross-dataset evaluations; CVC-12K; CVC-356; CVC-612; gastrointestinal tract diseases; global features; Kvasir; Medical; multi-class classification; Nerthus; polyp classification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UKTME26U","journalArticle","2017","Raj, Shriti; Newman, Mark W.; Lee, Joyce M.; Ackerman, Mark S.","Understanding Individual and Collaborative Problem-Solving with Patient-Generated Data: Challenges and Opportunities","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3134723","https://doi.org/10.1145/3134723","Making effective use of patient-generated data (PGD) is challenging for both patients and providers. Designing systems to support collaborative and individual use of PGD is a topic of importance in CSCW, considering the limitations of informatics tools. To inform better system design, we conducted a study including focus groups, observations and interviews with patients and providers to understand how PGD is interpreted and used. We found that while PGD is useful for identifying and solving disease-related problems, the following differences in patient-provider perceptions challenge its effective use - different perceptions about what is a problem, selecting what kinds of problems to focus on, and using different data representations. Drawing on these insights, we reflect on two specific conceptualizations of disease management behavior (sensemaking and problem-solving) as they relate to data specific activities of patients and providers and provide design suggestions for tools to support collaborative and individual use of PGD.","2017-12-06","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:47","88:1–88:18","","CSCW","1","","Proc. ACM Hum.-Comput. Interact.","Understanding Individual and Collaborative Problem-Solving with Patient-Generated Data","","","","","","","","","","","","November 2017","","","","","","","diabetes; chronic disease management; patient-generated data; reflection; interpreting data; patient-provider collaboration; personal informatics; problem-solving","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8MGKXI3F","journalArticle","2019","Schodde, Thorsten; Hoffmann, Laura; Stange, Sonja; Kopp, Stefan","Adapt, Explain, Engage - A Study on How Social Robots Can Scaffold Second-language Learning of Children","ACM Transactions on Human-Robot Interaction","","","10.1145/3366422","https://doi.org/10.1145/3366422","Social robots are increasingly applied to support children’s learning, but how a robot can foster (or may hinder) learning is still not fully clear. One technique used by teachers is scaffolding, temporarily assisting learners to achieve new skills or levels of understanding they would not reach on their own. We ask if and how a social robot can be utilized to scaffold second-language learning of children at kindergarten age (4--7 years). Specifically, we explore an adapt-and-explain scaffolding strategy in which a robot acts as a peer-like tutor who dynamically adapts its behavior or the learning tasks to the cognitive and affective state of the child, and provides verbal explanations of these adaptations. An evaluation study with 40 children shows that children benefit from the learning adaptation and that the explanations have a positive effect especially for slower learners. Further, in 76% of all cases the robot managed to “re-engage” children who started to disengage from the learning interaction, helping them to achieve an overall higher learning gain. These findings demonstrate that a social robot equipped with suitable scaffolding mechanisms can increase engagement and learning, especially when being adaptive to the individual behavior and states of a child learner.","2019-12-11","2021-06-14 02:55:00","2021-06-14 12:15:47","2021-06-14 02:54:48","6:1–6:27","","1","9","","J. Hum.-Robot Interact.","","","","","","","","","","","","","December 2019","","","","C:\Users\Asus\Zotero\storage\5A9BXWT7\Schodde et al. - 2019 - Adapt, Explain, Engage&#x2014;A Study on How Socia.pdf","","","Adaptive robot tutoring; engagement; open learner model; scaffolding; transparency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7NEUUA7K","journalArticle","2021","Matsuda, Akira; Okuzono, Toru; Nakamura, Hiromi; Kuzuoka, Hideaki; Rekimoto, Jun","A Surgical Scene Replay System for Learning Gastroenterological Endoscopic Surgery Skill by Multiple Synchronized-Video and Gaze Representation","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3461726","https://doi.org/10.1145/3461726","Gastroenterological endoscopic surgery needs complex surgical skills such as a sensation of body movement and manipulation of the endoscope that is hard to be explained verbally. Prior research reported that endoscopic surgery is one of the most challenging surgery to teach. Thus, surgeons need long-term practice to master the skills. To support learning such skills, we developed a surgical scene replay system. First, we surveyed 12 surgeons to reveal the reason for the difficulty and elicited three requirements for our system: (1) provide multiple videos that include an endoscope, a fluoroscopy, and hand manipulation for the endoscope to observe the surgery from multiple aspects; (2) visualize an experts' gaze position to understand experts' intention of hand manipulation, and (3) enlarge the size of the gazed video to inform learners where they should pay attention to. Our user study with the system indicates that participants could understand the experts' intentions and tacit knowledge easier than the existing video materials.","2021-05-27","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:50","204:1–204:22","","EICS","5","","Proc. ACM Hum.-Comput. Interact.","","","","","","","","","","","","","June 2021","","","","","","","gastroenterological endoscopic surgery; gaze; multiple videos; skill learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SNS2KS6J","journalArticle","2019","McClaine, Collin","Effect of security breaches on stock price","Journal of Computing Sciences in Colleges","","1937-4771","","","Modern times have begun to reveal the true cost data breaches have brought to companies. In my project, we are testing to see if there is an effect from security breaches on stock prices. Security breaches will be sampled from the years 2009 to 2018. We will be looking to see whether there is an increase in stock price, no change in the stock price, or a decrease in stock price, after the security breach occurred. This will be done using a linear regression through the statistical software STATA. Changes in stock price will be modeled with a Time Series Model over a large sample of companies that had significant security breaches. The breaches that occurred will be large scale security breaches of publicly traded companies and stock price of these companies will be analyzed in three different ways. These three ways include: before the breach had happened, when the breach occurred, and finally after the security breach occurred. Data will be drawn from the security breach dataset to ensure sufficient observations to minimize bias, give the model adequate explanatory power, and satisfactory explanation of the variation in stock price. The model will have several variables and these variables will be mainly financials of the companies that can affect stock price. We would also like to include a dummy variable, indicated by a 1(when there is a security breach present on that date or 0(when there is no security breach present). This may be able to help answer the question of whether the market can push companies to protect against security breaches.","2019-10-01","2021-06-14 02:55:00","2021-06-14 02:55:00","","213","","3","35","","J. Comput. Sci. Coll.","","","","","","","","","","","","","October 2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SK4EFFCL","journalArticle","2018","Duarte-Sanchez, Jorge E.; Velasco-Medina, Jaime; Moreno, Pedro A.","Hardware Accelerator for the Multifractal Analysis of DNA Sequences","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","1545-5963","10.1109/TCBB.2017.2731339","https://doi.org/10.1109/TCBB.2017.2731339","The multifractal analysis has allowed to quantify the genetic variability and non-linear stability along the human genome sequence. It has some implications in explaining several genetic diseases given by some chromosome abnormalities, among other genetic particularities. The multifractal analysis of a genome is carried out by dividing the complete DNA sequence in smaller fragments and calculating the generalized dimension spectrum of each fragment using the chaos game representation and the box-counting method. This is a time consuming process because it involves the processing of large data sets using floating-point representation. In order to reduce the computation time, we designed an application-specific processor, here called multifractal processor, which is based on our proposed hardware-oriented algorithm for calculating efficiently the generalized dimension spectrum of DNA sequences. The multifractal processor was implemented on a low-cost SoC-FPGA and was verified by processing a complete human genome. The execution time and numeric results of the Multifractal processor were compared with the results obtained from the software implementation executed in a 20-core workstation, achieving a speed up of 2.6x and an average error of 0.0003 percent.","2018-09-01","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:52","1611–1624","","5","15","","IEEE/ACM Trans. Comput. Biol. Bioinformatics","","","","","","","","","","","","","September 2018","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YQGCFRW8","journalArticle","2017","Bourke, Timothy; Carcenac, Francois; Colaço, Jean-Louis; Pagano, Bruno; Pasteur, Cédric; Pouzet, Marc","A Synchronous Look at the Simulink Standard Library","ACM Transactions on Embedded Computing Systems","","1539-9087","10.1145/3126516","https://doi.org/10.1145/3126516","Hybrid systems modelers like Simulink come with a rich collection of discrete-time and continuous-time blocks. Most blocks are not defined in terms of more elementary ones—and some cannot be—but are instead written in imperative code and explained informally in a reference manual. This raises the question of defining a minimal set of orthogonal programming constructs such that most blocks can be programmed directly and thereby given a specification that is mathematically precise, and whose compiled version performs comparably to handwritten code. In this paper, we show that a fairly large set of blocks of a standard library like the one provided by Simulink can be programmed in a precise, purely functional language using stream equations, hierarchical automata, Ordinary Differential Equations (ODEs), and deterministic synchronous parallel composition. Some blocks cannot be expressed in our setting as they mix discrete-time and continuous-time signals in unprincipled ways that are statically forbidden by the type checker. The experiment is conducted in Zélus, a synchronous language that conservatively extends Lustre with ODEs to program systems that mix discrete-time and continuous-time signals.","2017-09-27","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:55","176:1–176:24","","5s","16","","ACM Trans. Embed. Comput. Syst.","","","","","","","","","","","","","October 2017","","","","C:\Users\Asus\Zotero\storage\NUZ3TWTV\Bourke et al. - 2017 - A Synchronous Look at the Simulink Standard Librar.pdf","","","Block diagrams; Hybrid systems; Synchronous languages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CL4JCZT5","journalArticle","2016","Kadleček, Petr; Ichim, Alexandru-Eugen; Liu, Tiantian; Křivánek, Jaroslav; Kavan, Ladislav","Reconstructing personalized anatomical models for physics-based body animation","ACM Transactions on Graphics","","0730-0301","10.1145/2980179.2982438","https://doi.org/10.1145/2980179.2982438","We present a method to create personalized anatomical models ready for physics-based animation, using only a set of 3D surface scans. We start by building a template anatomical model of an average male which supports deformations due to both 1) subject-specific variations: shapes and sizes of bones, muscles, and adipose tissues and 2) skeletal poses. Next, we capture a set of 3D scans of an actor in various poses. Our key contribution is formulating and solving a large-scale optimization problem where we compute both subject-specific and pose-dependent parameters such that our resulting anatomical model explains the captured 3D scans as closely as possible. Compared to data-driven body modeling techniques that focus only on the surface, our approach has the advantage of creating physics-based models, which provide realistic 3D geometry of the bones and muscles, and naturally supports effects such as inertia, gravity, and collisions according to Newtonian dynamics.","2016-11-11","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:54:57","213:1–213:13","","6","35","","ACM Trans. Graph.","","","","","","","","","","","","","November 2016","","","","C:\Users\Asus\Zotero\storage\XMPRLQEB\Kadleček et al. - 2016 - Reconstructing personalized anatomical models for .pdf","","","3D avatar creation; anatomical models; body animation; rigging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VVLM34KU","journalArticle","2019","Wang, Xiaokang; Yang, Laurence T.; Wang, Yihao; Liu, Xingang; Zhang, Qingxia; Deen, M. Jamal","A Distributed Tensor-Train Decomposition Method for Cyber-Physical-Social Services","ACM Transactions on Cyber-Physical Systems","","2378-962X","10.1145/3323926","https://doi.org/10.1145/3323926","Cyber-Physical-Social Systems (CPSS) integrating the cyber, physical, and social worlds is a key technology to provide proactive and personalized services for humans. In this paper, we studied CPSS by taking human-interaction-aware big data (HIBD) as the starting point. However, the HIBD collected from all aspects of our daily lives are of high-order and large-scale, which bring ever-increasing challenges for their cleaning, integration, processing, and interpretation. Therefore, new strategies for representing and processing of HIBD become increasingly important in the provision of CPSS services. As an emerging technique, tensor is proving to be a suitable and promising representation and processing tool of HIBD. In particular, tensor networks, as a significant tensor decomposition technique, bring advantages of computing, storage, and applications of HIBD. Furthermore, Tensor-Train (TT), a type of tensor network, is particularly well suited for representing and processing high-order data by decomposing a high-order tensor into a series of low-order tensors. However, at present, there is still need for an efficient Tensor-Train decomposition method for massive data. Therefore, for larger-scale HIBD, a highly-efficient computational method of Tensor-Train is required. In this paper, a distributed Tensor-Train (DTT) decomposition method is proposed to process the high-order and large-scale HIBD. The high performance of the proposed DTT such as the execution time is demonstrated with a case study on a typical form of CPSS data, Computed Tomography (CT) image data.","2019-10-04","2021-06-14 02:55:00","2021-06-14 02:55:00","2021-06-14 02:55:00","35:1–35:15","","4","3","","ACM Trans. Cyber-Phys. Syst.","","","","","","","","","","","","","October 2019","","","","","","","big data; CPSS; distributed computing; services; tensor; tensor networks; tensor-train","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""