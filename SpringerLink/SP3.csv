"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"PBAM3TBA","journalArticle","2021","Govindarajan, Satyavratan; Swaminathan, Ramakrishnan","Differentiation of COVID-19 conditions in planar chest radiographs using optimized convolutional neural networks","Applied Intelligence","","1573-7497","10.1007/s10489-020-01941-8","https://doi.org/10.1007/s10489-020-01941-8","In this study, an attempt has been made to differentiate Novel Coronavirus-2019 (COVID-19) conditions from healthy subjects in Chest radiographs using a simplified end-to-end Convolutional Neural Network (CNN) model and occlusion sensitivity maps. Early detection and faster automated screening of the COVID-19 patients is essential. For this, the images are considered from publicly available datasets. Significant biomarkers representing critical image features are extracted from CNN by experimentally investigating on cross-validation methods and hyperparameter settings. The performance of the network is evaluated using standard metrics. Perturbation based occlusion sensitivity maps are employed on the features obtained from the classification model to visualise the localization of abnormal areas. Results demonstrate that the simplified CNN model with optimised parameters is able to extract significant features with a sensitivity of 97.35% and F-measure of 96.71% to detect COVID-19 images. The algorithm achieves an Area Under the Curve-Receiver Operating Characteristic score of 99.4% with Matthews correlation coefficient of 0.93. High value of Diagnostic odds ratio is also obtained. Occlusion sensitivity maps provide precise localization of abnormal regions by identifying COVID-19 conditions. As early detection through chest radiographic images are useful for automated screening of the disease, this method appears to be clinically relevant in providing a visual diagnostic solution using a simplified and efficient model.","2021-05-01","2021-06-07 21:07:51","2021-06-07 21:07:51","2021-06-07 21:07:50","2764-2775","","5","51","","Appl Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\5UEEMELY\Govindarajan and Swaminathan - 2021 - Differentiation of COVID-19 conditions in planar c.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RUDA5JFV","journalArticle","2021","Punn, Narinder Singh; Agarwal, Sonali","Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray images using fine-tuned deep neural networks","Applied Intelligence","","1573-7497","10.1007/s10489-020-01900-3","https://doi.org/10.1007/s10489-020-01900-3","The novel coronavirus 2019 (COVID-19) is a respiratory syndrome that resembles pneumonia. The current diagnostic procedure of COVID-19 follows reverse-transcriptase polymerase chain reaction (RT-PCR) based approach which however is less sensitive to identify the virus at the initial stage. Hence, a more robust and alternate diagnosis technique is desirable. Recently, with the release of publicly available datasets of corona positive patients comprising of computed tomography (CT) and chest X-ray (CXR) imaging; scientists, researchers and healthcare experts are contributing for faster and automated diagnosis of COVID-19 by identifying pulmonary infections using deep learning approaches to achieve better cure and treatment. These datasets have limited samples concerned with the positive COVID-19 cases, which raise the challenge for unbiased learning. Following from this context, this article presents the random oversampling and weighted class loss function approach for unbiased fine-tuned learning (transfer learning) in various state-of-the-art deep learning approaches such as baseline ResNet, Inception-v3, Inception ResNet-v2, DenseNet169, and NASNetLarge to perform binary classification (as normal and COVID-19 cases) and also multi-class classification (as COVID-19, pneumonia, and normal case) of posteroanterior CXR images. Accuracy, precision, recall, loss, and area under the curve (AUC) are utilized to evaluate the performance of the models. Considering the experimental results, the performance of each model is scenario dependent; however, NASNetLarge displayed better scores in contrast to other architectures, which is further compared with other recently proposed approaches. This article also added the visual explanation to illustrate the basis of model classification and perception of COVID-19 in CXR images.","2021-05-01","2021-06-07 21:07:51","2021-06-07 21:07:51","2021-06-07 21:07:51","2689-2702","","5","51","","Appl Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\F46NMHKN\Punn and Agarwal - 2021 - Automated diagnosis of COVID-19 with limited poste.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CRH2BNBD","journalArticle","2020","Shanmugam, Ramalingam","Restricted Prevalence Rates of COVID-19’s Infectivity, Hospitalization, Recovery, Mortality in the USA and Their Implications","Journal of Healthcare Informatics Research","","2509-498X","10.1007/s41666-020-00078-0","https://doi.org/10.1007/s41666-020-00078-0","This article constructs and demonstrates an alternate probabilistic approach (using incidence rate restricted model), compared with the deterministic mathematical models such as SIR, to capture the impact of healthcare efforts on the prevalence rate of the COVID-19’s infectivity, hospitalization, recovery, and mortality in the eastern, central, mountain, and pacific time zone states in the USA. We add additional new properties for the incidence rate restricted Poisson probability distribution. With new properties, our method becomes feasible to comprehend not only the patterns of the prevalence rate of the COVID-19’s infectivity, hospitalization, recovery, and mortality but also to quantitatively assess the effectiveness of social distancing, healthcare management’s efforts to hospitalize the patients, the patient’s immunity to recover, and lastly the unfortunate mortality itself. To make regional comparisons (as the people’s movement is far more frequent within than outside the regional zone on daily basis), we group the COVID-19 data in terms of eastern, central, mountain, and pacific zone states. Several non-intuitive findings in the data results are noticed. They include the existence of imbalance, different vulnerability, and risk reduction in these four regions. For example, the impact of healthcare efforts is high in the recovery category in the pacific states. The impact is less in the hospitalization category in the mountain states. The least impact is seen in the infectivity category in the eastern zone states. A few thoughts on future research work are cited. It requires collecting rich data on COVID-19 and extracting valuable information for better public health policies.","2020-10-09","2021-06-07 21:07:51","2021-06-08 18:32:50","2021-06-07 21:07:51","133-150","","2","5","","J Healthc Inform Res","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\JWBTJNRJ\Shanmugam - 2021 - Restricted Prevalence Rates of COVID-19’s Infectiv.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WGZJBQJM","journalArticle","2021","Miron, Marius; Tolan, Songül; Gómez, Emilia; Castillo, Carlos","Evaluating causes of algorithmic bias in juvenile criminal recidivism","Artificial Intelligence and Law","","1572-8382","10.1007/s10506-020-09268-y","https://doi.org/10.1007/s10506-020-09268-y","In this paper we investigate risk prediction of criminal re-offense among juvenile defendants using general-purpose machine learning (ML) algorithms. We show that in our dataset, containing hundreds of cases, ML models achieve better predictive power than a structured professional risk assessment tool, the Structured Assessment of Violence Risk in Youth (SAVRY), at the expense of not satisfying relevant group fairness metrics that SAVRY does satisfy. We explore in more detail two possible causes of this algorithmic bias that are related to biases in the data with respect to two protected groups, foreigners and women. In particular, we look at (1) the differences in the prevalence of re-offense between protected groups and (2) the influence of protected group or correlated features in the prediction. Our experiments show that both can lead to disparity between groups on the considered group fairness metrics. We observe that methods to mitigate the influence of either cause do not guarantee fair outcomes. An analysis of feature importance using LIME, a machine learning interpretability method, shows that some mitigation methods can shift the set of features that ML techniques rely on away from demographics and criminal history which are highly correlated with sensitive features.","2021-06-01","2021-06-07 21:07:51","2021-06-07 21:07:51","2021-06-07 21:07:51","111-147","","2","29","","Artif Intell Law","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\Z28XTFCL\Miron et al. - 2021 - Evaluating causes of algorithmic bias in juvenile .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y2YXALKF","journalArticle","2021","Hira, Swati; Bai, Anita; Hira, Sanchit","An automatic approach based on CNN architecture to detect Covid-19 disease from chest X-ray images","Applied Intelligence","","1573-7497","10.1007/s10489-020-02010-w","https://doi.org/10.1007/s10489-020-02010-w","Novel coronavirus (COVID-19) is started from Wuhan (City in China), and is rapidly spreading among people living in other countries. Today, around 215 countries are affected by COVID-19 disease. WHO announced approximately number of cases 11,274,600 worldwide. Due to rapidly rising cases daily in the hospitals, there are a limited number of resources available to control COVID-19 disease. Therefore, it is essential to develop an accurate diagnosis of COVID-19 disease. Early diagnosis of COVID-19 patients is important for preventing the disease from spreading to others. In this paper, we proposed a deep learning based approach that can differentiate COVID- 19 disease patients from viral pneumonia, bacterial pneumonia, and healthy (normal) cases. In this approach, deep transfer learning is adopted. We used binary and multi-class dataset which is categorized in four types for experimentation: (i) Collection of 728 X-ray images including 224 images with confirmed COVID-19 disease and 504 normal condition images (ii) Collection of 1428 X-ray images including 224 images with confirmed COVID-19 disease, 700 images with confirmed common bacterial pneumonia, and 504 normal condition images. (iii) Collections of 1442 X- ray images including 224 images with confirmed COVID-19 disease, 714 images with confirmed bacterial and viral pneumonia, and 504 images of normal conditions (iv) Collections of 5232 X- ray images including 2358 images with confirmed bacterial and 1345 with viral pneumonia, and 1346 images of normal conditions. In this paper, we have used nine convolutional neural network based architecture (AlexNet, GoogleNet, ResNet-50, Se-ResNet-50, DenseNet121, Inception V4, Inception ResNet V2, ResNeXt-50, and Se-ResNeXt-50). Experimental results indicate that the pre trained model Se-ResNeXt-50 achieves the highest classification accuracy of 99.32% for binary class and 97.55% for multi-class among all pre-trained models.","2021-05-01","2021-06-07 21:07:51","2021-06-07 21:07:51","2021-06-07 21:07:51","2864-2889","","5","51","","Appl Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\42DJJ6IM\Hira et al. - 2021 - An automatic approach based on CNN architecture to.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KV9U5UJF","journalArticle","2021","Voukelatou, Vasiliki; Gabrielli, Lorenzo; Miliou, Ioanna; Cresci, Stefano; Sharma, Rajesh; Tesconi, Maurizio; Pappalardo, Luca","Measuring objective and subjective well-being: dimensions and data sources","International Journal of Data Science and Analytics","","2364-4168","10.1007/s41060-020-00224-2","https://doi.org/10.1007/s41060-020-00224-2","Well-being is an important value for people’s lives, and it could be considered as an index of societal progress. Researchers have suggested two main approaches for the overall measurement of well-being, the objective and the subjective well-being. Both approaches, as well as their relevant dimensions, have been traditionally captured with surveys. During the last decades, new data sources have been suggested as an alternative or complement to traditional data. This paper aims to present the theoretical background of well-being, by distinguishing between objective and subjective approaches, their relevant dimensions, the new data sources used for their measurement and relevant studies. We also intend to shed light on still barely unexplored dimensions and data sources that could potentially contribute as a key for public policing and social development.","2021-05-01","2021-06-07 21:07:51","2021-06-07 21:07:51","2021-06-07 21:07:51","279-309","","4","11","","Int J Data Sci Anal","Measuring objective and subjective well-being","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\DZCTVGSE\Voukelatou et al. - 2021 - Measuring objective and subjective well-being dim.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E45J5M2W","journalArticle","2021","Zhou, Tao; Fan, Deng-Ping; Cheng, Ming-Ming; Shen, Jianbing; Shao, Ling","RGB-D salient object detection: A survey","Computational Visual Media","","2096-0662","10.1007/s41095-020-0199-z","https://doi.org/10.1007/s41095-020-0199-z","Salient object detection, which simulates human visual perception in locating the most significant object(s) in a scene, has been widely applied to various computer vision tasks. Now, the advent of depth sensors means that depth maps can easily be captured; this additional spatial information can boost the performance of salient object detection. Although various RGB-D based salient object detection models with promising performance have been proposed over the past several years, an in-depth understanding of these models and the challenges in this field remains lacking. In this paper, we provide a comprehensive survey of RGB-D based salient object detection models from various perspectives, and review related benchmark datasets in detail. Further, as light fields can also provide depth maps, we review salient object detection models and popular benchmark datasets from this domain too. Moreover, to investigate the ability of existing models to detect salient objects, we have carried out a comprehensive attribute-based evaluation of several representative RGB-D based salient object detection models. Finally, we discuss several challenges and open directions of RGB-D based salient object detection for future research. All collected models, benchmark datasets, datasets constructed for attribute-based evaluation, and related code are publicly available at https://github.com/taozh2017/RGBD-SODsurvey.","2021-03-01","2021-06-07 21:08:19","2021-06-07 21:08:19","2021-06-07 21:08:18","37-69","","1","7","","Comp. Visual Media","RGB-D salient object detection","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\ILYHDZMQ\Zhou et al. - 2021 - RGB-D salient object detection A survey.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S329AFDN","journalArticle","2021","Yang, Shan; Zheng, Xiangwei; Ji, Cun; Chen, Xuanchi","Multi-layer Representation Learning and Its Application to Electronic Health Records","Neural Processing Letters","","1573-773X","10.1007/s11063-021-10449-2","https://doi.org/10.1007/s11063-021-10449-2","Electronic Health Records (EHRs) are digital records associated with hospitalization, diagnosis, medications and so on. Secondary use of EHRs can promote the clinical informatics applications and the development of healthcare undertaking. EHRs have the unique characteristic where the patient visits are temporally ordered but the diagnosis codes within a visit are randomly ordered. The hierarchical structure requires a multi-layer network to explore the different relational information of EHRs. In this paper, we propose a Multi-Layer Representation Learning method (MLRL), which is capable of learning effective patient representation by hierarchically exploring the valuable information in both diagnosis codes and patient visits. Firstly, MLRL utilizes the multi-head attention mechanism to explore the potential connections in diagnosis codes, and a linear transformation is implemented to further map the code vectors to non-negative real-valued representations. The initial visit vectors are then obtained by summarizing all the code representations. Secondly, the proposed method combines Bidirectional Long Short-Term Memory with self-attention mechanism to learn the weighted visit vectors which are aggregated to form the patient representation. Finally, to evaluate the performance of MLRL, we apply it to patient’s mortality prediction on real EHRs and the experimental results demonstrate that MLRL has a significant improvement in prediction performance. MLRL achieves around 0.915 in Area Under Curve which is superior to the results obtained by baseline methods. Furthermore, compared with raw data and other data representations, the learned representation with MLRL shows its outstanding results and availability on multiple different classifiers.","2021-04-01","2021-06-07 21:08:19","2021-06-07 21:08:19","2021-06-07 21:08:18","1417-1433","","2","53","","Neural Process Lett","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\UGFMFPFT\Yang et al. - 2021 - Multi-layer Representation Learning and Its Applic.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RIZ6AFMN","journalArticle","2021","Yuan, Jun; Chen, Changjian; Yang, Weikai; Liu, Mengchen; Xia, Jiazhi; Liu, Shixia","A survey of visual analytics techniques for machine learning","Computational Visual Media","","2096-0662","10.1007/s41095-020-0191-7","https://doi.org/10.1007/s41095-020-0191-7","Visual analytics for machine learning has recently evolved as one of the most exciting areas in the field of visualization. To better identify which research topics are promising and to learn how to apply relevant techniques in visual analytics, we systematically review 259 papers published in the last ten years together with representative works before 2010. We build a taxonomy, which includes three first-level categories: techniques before model building, techniques during modeling building, and techniques after model building. Each category is further characterized by representative analysis tasks, and each task is exemplified by a set of recent influential works. We also discuss and highlight research challenges and promising potential future research opportunities useful for visual analytics researchers.","2021-03-01","2021-06-07 21:08:19","2021-06-07 21:08:19","2021-06-07 21:08:18","3-36","","1","7","","Comp. Visual Media","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\U2MYDYIM\Yuan et al. - 2021 - A survey of visual analytics techniques for machin.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KSAJYJ74","journalArticle","2021","Qian, Zhaozhi; Alaa, Ahmed M.; van der Schaar, Mihaela","CPAS: the UK’s national machine learning-based hospital capacity planning system for COVID-19","Machine Learning","","1573-0565","10.1007/s10994-020-05921-4","https://doi.org/10.1007/s10994-020-05921-4","The coronavirus disease 2019 (COVID-19) global pandemic poses the threat of overwhelming healthcare systems with unprecedented demands for intensive care resources. Managing these demands cannot be effectively conducted without a nationwide collective effort that relies on data to forecast hospital demands on the national, regional, hospital and individual levels. To this end, we developed the COVID-19 Capacity Planning and Analysis System (CPAS)—a machine learning-based system for hospital resource planning that we have successfully deployed at individual hospitals and across regions in the UK in coordination with NHS Digital. In this paper, we discuss the main challenges of deploying a machine learning-based decision support system at national scale, and explain how CPAS addresses these challenges by (1) defining the appropriate learning problem, (2) combining bottom-up and top-down analytical approaches, (3) using state-of-the-art machine learning algorithms, (4) integrating heterogeneous data sources, and (5) presenting the result with an interactive and transparent interface. CPAS is one of the first machine learning-based systems to be deployed in hospitals on a national scale to address the COVID-19 pandemic—we conclude the paper with a summary of the lessons learned from this experience.","2021-01-01","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:43","15-35","","1","110","","Mach Learn","CPAS","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\K2I97PT3\Qian et al. - 2021 - CPAS the UK’s national machine learning-based hos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IYGBMJ57","journalArticle","2021","van der Schaar, Mihaela; Alaa, Ahmed M.; Floto, Andres; Gimson, Alexander; Scholtes, Stefan; Wood, Angela; McKinney, Eoin; Jarrett, Daniel; Lio, Pietro; Ercole, Ari","How artificial intelligence and machine learning can help healthcare systems respond to COVID-19","Machine Learning","","1573-0565","10.1007/s10994-020-05928-x","https://doi.org/10.1007/s10994-020-05928-x","The COVID-19 global pandemic is a threat not only to the health of millions of individuals, but also to the stability of infrastructure and economies around the world.The disease will inevitably place an overwhelming burden on healthcare systems that cannot be effectively dealt with by existing facilities or responses based on conventional approaches. We believe that a rigorous clinical and societal response can only be mounted by using intelligence derived from a variety of data sources to better utilize scarce healthcare resources, provide personalized patient management plans, inform policy, and expedite clinical trials.In this paper, we introduce five of the most important challenges in responding to COVID-19 and show how each of them can be addressed by recent developments in machine learning (ML) and artificial intelligence (AI).We argue that the integration of these techniques into local, national, and international healthcare systems will save lives, and propose specific methods by which implementation can happen swiftly and efficiently. We offer to extend these resources and knowledge to assist policymakers seeking to implement these techniques.","2021-01-01","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:43","1-14","","1","110","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\VUGXAU9W\van der Schaar et al. - 2021 - How artificial intelligence and machine learning c.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FNVLJM5S","journalArticle","2021","Benjamins, Richard","A A A choices framework for the responsible use of AI","AI and Ethics","","2730-5961","10.1007/s43681-020-00012-5","https://doi.org/10.1007/s43681-020-00012-5","Popular press and media often make us believe that artificial intelligence technology is ethical or unethical by itself. In this paper, we will argue that organizations that develop or apply AI have certain choices they can make that will lead to a more or less responsible use of AI. By approaching those choices in a methodological way, organizations can make better decisions toward the ethical use of this powerful technology.","2021-02-01","2021-06-07 21:08:45","2021-06-15 08:59:00","2021-06-07 21:08:43","49-53","","1","1","","AI Ethics","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\DIKUMV2M\Benjamins - 2021 - A choices framework for the responsible use of AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TDW78IAG","journalArticle","2021","Eitel-Porter, Ray","Beyond the promise: implementing ethical AI","AI and Ethics","","2730-5961","10.1007/s43681-020-00011-6","https://doi.org/10.1007/s43681-020-00011-6","Artificial Intelligence (AI) applications can and do have unintended negative consequences for businesses if not implemented with care. Specifically, faulty or biased AI applications risk compliance and governance breaches and damage to the corporate brand. These issues commonly arise from a number of pitfalls associated with AI development, which include rushed development, a lack of technical understanding, and improper quality assurance, among other factors. To mitigate these risks, a growing number of organisations are working on ethical AI principles and frameworks. However, ethical AI principles alone are not sufficient for ensuring responsible AI use in enterprises. Businesses also require strong, mandated governance controls including tools for managing processes and creating associated audit trails to enforce their principles. Businesses that implement strong governance frameworks, overseen by an ethics board and strengthened with appropriate training, will reduce the risks associated with AI. When applied to AI modelling, the governance will also make it easier for businesses to bring their AI deployments to scale.","2021-02-01","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:43","73-80","","1","1","","AI Ethics","Beyond the promise","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\C99VRLAZ\Eitel-Porter - 2021 - Beyond the promise implementing ethical AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AHUZWNCU","journalArticle","2021","Bany Muhammad, Mohammed; Yeasin, Mohammed","A A Eigen-CAM: Visual Explanations for Deep Convolutional Neural Networks","SN Computer Science","","2661-8907","10.1007/s42979-021-00449-3","https://doi.org/10.1007/s42979-021-00449-3","The adoption of deep convolutional neural networks (CNN) is growing exponentially in wide varieties of applications due to exceptional performance that equals to or is better than classical machine learning as well as a human. However, such models are difficult to interpret, susceptible to overfit, and hard to decode failure. An increasing body of literature, such as class activation map (CAM), focused on understanding what representations or features a model learned from the data. This paper presents novel Eigen-CAM to enhance explanations of CNN predictions by visualizing principal components of learned representations from convolutional layers. The Eigen-CAM is intuitive, easy to use, computationally efficient, and does not require correct classification by the model. Eigen-CAM can work with all CNN models without the need to modify layers or retrain models. For the task of generating a visual explanation of CNN predictions, compared to state-of-the-art methods, Eigen-CAM is more consistent, class discriminative, and robust against classification errors made by dense layers. Empirical analyses and comparison with the best state-of-the-art methods show up to 12% improvement in weakly-supervised object localization, an average of 13% improvement in weakly-supervised segmentation, and at least 15% improvement in generic object proposal.","2021-01-20","2021-06-07 21:08:45","2021-06-15 09:06:42","2021-06-07 21:08:43","47","","1","2","","SN COMPUT. SCI.","Eigen-CAM","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\EYYJDFRX\Bany Muhammad and Yeasin - 2021 - Eigen-CAM Visual Explanations for Deep Convolutio.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AEZ6772A","journalArticle","2021","Martin, Kyle; Liret, Anne; Wiratunga, Nirmalie; Owusu, Gilbert; Kern, Mathias","Evaluating Explainability Methods Intended for Multiple Stakeholders","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-020-00702-6","https://doi.org/10.1007/s13218-020-00702-6","Explanation mechanisms for intelligent systems are typically designed to respond to specific user needs, yet in practice these systems tend to have a wide variety of users. This can present a challenge to organisations looking to satisfy the explanation needs of different groups using an individual system. In this paper we present an explainability framework formed of a catalogue of explanation methods, and designed to integrate with a range of projects within a telecommunications organisation. Explainability methods are split into low-level explanations and high-level explanations for increasing levels of contextual support in their explanations. We motivate this framework using the specific case-study of explaining the conclusions of field network engineering experts to non-technical planning staff and evaluate our results using feedback from two distinct user groups; domain-expert telecommunication engineers and non-expert desk agent staff. We also present and investigate two metrics designed to model the quality of explanations - Meet-In-The-Middle (MITM) and Trust-Your-Neighbours (TYN). Our analysis of these metrics offers new insights into the use of similarity knowledge for the evaluation of explanations.","2021-02-07","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:43","","","","","","Künstl Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\G388FBMM\Martin et al. - 2021 - Evaluating Explainability Methods Intended for Mul.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IPD32GNS","journalArticle","2021","Wang, Junshu; Zhang, Guoming; Wang, Wei; Zhang, Ka; Sheng, Yehua","Cloud-based intelligent self-diagnosis and department recommendation service using Chinese medical BERT","Journal of Cloud Computing","","2192-113X","10.1186/s13677-020-00218-2","https://doi.org/10.1186/s13677-020-00218-2","With the rapid development of hospital informatization and Internet medical service in recent years, most hospitals have launched online hospital appointment registration systems to remove patient queues and improve the efficiency of medical services. However, most of the patients lack professional medical knowledge and have no idea of how to choose department when registering. To instruct the patients to seek medical care and register effectively, we proposed CIDRS, an intelligent self-diagnosis and department recommendation framework based on Chinese medical Bidirectional Encoder Representations from Transformers (BERT) in the cloud computing environment. We also established a Chinese BERT model (CHMBERT) trained on a large-scale Chinese medical text corpus. This model was used to optimize self-diagnosis and department recommendation tasks. To solve the limited computing power of terminals, we deployed the proposed framework in a cloud computing environment based on container and micro-service technologies. Real-world medical datasets from hospitals were used in the experiments, and results showed that the proposed model was superior to the traditional deep learning models and other pre-trained language models in terms of performance.","2021-01-15","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:44","4","","1","10","","J Cloud Comp","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\39AKA4ME\Wang et al. - 2021 - Cloud-based intelligent self-diagnosis and departm.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"584AWWC3","journalArticle","2021","","A A News","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-021-00711-z","https://doi.org/10.1007/s13218-021-00711-z","","2021-02-11","2021-06-07 21:08:45","2021-06-17 14:04:15","2021-06-07 21:08:44","","","","","","Künstl Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\CKW68VSQ\2021 - News.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RBRLGYL9","journalArticle","2021","Roberts, Huw; Cowls, Josh; Morley, Jessica; Taddeo, Mariarosaria; Wang, Vincent; Floridi, Luciano","The Chinese approach to artificial intelligence: an analysis of policy, ethics, and regulation","AI & SOCIETY","","1435-5655","10.1007/s00146-020-00992-2","https://doi.org/10.1007/s00146-020-00992-2","In July 2017, China’s State Council released the country’s strategy for developing artificial intelligence (AI), entitled ‘New Generation Artificial Intelligence Development Plan’ (新一代人工智能发展规划). This strategy outlined China’s aims to become the world leader in AI by 2030, to monetise AI into a trillion-yuan (ca. 150 billion dollars) industry, and to emerge as the driving force in defining ethical norms and standards for AI. Several reports have analysed specific aspects of China’s AI policies or have assessed the country’s technical capabilities. Instead, in this article, we focus on the socio-political background and policy debates that are shaping China’s AI strategy. In particular, we analyse the main strategic areas in which China is investing in AI and the concurrent ethical debates that are delimiting its use. By focusing on the policy backdrop, we seek to provide a more comprehensive and critical understanding of China’s AI policy by bringing together debates and analyses of a wide array of policy documents.","2021-03-01","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:44","59-77","","1","36","","AI & Soc","The Chinese approach to artificial intelligence","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\2KJK3ADM\Roberts et al. - 2021 - The Chinese approach to artificial intelligence a.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WD6JQFFX","journalArticle","2021","Gomez, Rocio; Sridharan, Mohan; Riley, Heather","What do you really want to do? Towards a Theory of Intentions for Human-Robot Collaboration","Annals of Mathematics and Artificial Intelligence","","1573-7470","10.1007/s10472-019-09672-4","https://doi.org/10.1007/s10472-019-09672-4","The architecture described in this paper encodes a theory of intentions based on the key principles of non-procrastination, persistence, and automatically limiting reasoning to relevant knowledge and observations. The architecture reasons with transition diagrams of any given domain at two different resolutions, with the fine-resolution description defined as a refinement of, and hence tightly-coupled to, a coarse-resolution description. For any given goal, nonmonotonic logical reasoning with the coarse-resolution description computes an activity, i.e., a plan, comprising a sequence of abstract actions to be executed to achieve the goal. Each abstract action is implemented as a sequence of concrete actions by automatically zooming to and reasoning with the part of the fine-resolution transition diagram relevant to the current coarse-resolution transition and the goal. Each concrete action in this sequence is executed using probabilistic models of the uncertainty in sensing and actuation, and the corresponding fine-resolution outcomes are used to infer coarse-resolution observations that are added to the coarse-resolution history. The architecture’s capabilities are evaluated in the context of a simulated robot assisting humans in an office domain, on a physical robot (Baxter) manipulating tabletop objects, and on a wheeled robot (Turtlebot) moving objects to particular places or people. The experimental results indicate improvements in reliability and computational efficiency compared with an architecture that does not include the theory of intentions, and an architecture that does not include zooming for fine-resolution reasoning.","2021-02-01","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:44","179-208","","1","89","","Ann Math Artif Intell","What do you really want to do?","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\KCDP8838\Gomez et al. - 2021 - What do you really want to do Towards a Theory of.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LI4WXDJ6","journalArticle","2020","Tausch, Alina; Kluge, Annette","The best task allocation process is to decide on one’s own: effects of the allocation agent in human–robot interaction on perceived work characteristics and satisfaction","Cognition, Technology & Work","","1435-5566","10.1007/s10111-020-00656-7","https://doi.org/10.1007/s10111-020-00656-7","New technologies are ever evolving and have the power to change human work for the better or the worse depending on the implementation. For human–robot interaction (HRI), it is decisive how humans and robots will share tasks and who will be in charge for decisions on task allocation. The aim of this online experiment was to examine the influence of different decision agents on the perception of a task allocation process in HRI. We assume that inclusion of the worker in the allocation will create more perceived work resources and will lead to more satisfaction with the allocation and the work results than a decision made by another agent. To test these hypotheses, we used a fictional production scenario where tasks were allocated to the participant and a robot. The allocation decision was either made by the robot, by an organizational unit, or by the participants themselves. We then looked for differences between those conditions. Our sample consisted of 151 people. In multiple ANOVAs, we could show that satisfaction with the allocation process, the solution, and with the result of the work process was higher in the condition where participants themselves were given agency in the allocation process compared to the other two. Those participants also experienced more task identity and autonomy. This has implications for the design of allocation processes: The inclusion of workers in task allocation can play a crucial role in leveraging the acceptance of HRI and in designing humane work systems in Industry 4.0.","2020-12-21","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:44","","","","","","Cogn Tech Work","The best task allocation process is to decide on one’s own","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\VX4Q2WPU\Tausch and Kluge - 2020 - The best task allocation process is to decide on o.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G8MCHDQH","journalArticle","2021","Puindi, António Casimiro","Structural Models: A Computational Look for Signal Extraction and Forecasting Seasonal Time Series","SN Computer Science","","2661-8907","10.1007/s42979-021-00474-2","https://doi.org/10.1007/s42979-021-00474-2","Data analysis requires doing statistical programming which can be done at a simpler or more complex level. This article presents the main computational aspects for signal extraction, estimation and forecasting seasonal time series. From a structural model with covariates and different errors affecting the observations and the states, intelligent computational procedures are designed. First, the intelligent computational algorithms to obtain the main matrices of the system are derived; second, a general intelligent computational procedure and an algorithm that performs the Kalman filter and model estimation are also derived. Furthermore, the intelligent computational procedures and the structural model are evaluated using real seasonal time series, and the results demonstrate that the proposed method and model are very attractive and promising for model estimation, signal extraction and forecasting tasks.","2021-02-13","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:44","96","","2","2","","SN COMPUT. SCI.","Structural Models","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\XNSB6WXI\Puindi - 2021 - Structural Models A Computational Look for Signal.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6DE6F9QL","journalArticle","2021","Kazim, Emre; Denny, Danielle Mendes Thame; Koshiyama, Adriano","AI auditing and impact assessment: according to the UK information commissioner’s office","AI and Ethics","","2730-5961","10.1007/s43681-021-00039-2","https://doi.org/10.1007/s43681-021-00039-2","As the use of data and artificial intelligence systems becomes crucial to core services and business, it increasingly demands a multi-stakeholder and complex governance approach. The Information Commissioner's Office’s ‘Guidance on the AI auditing framework: Draft guidance for consultation’ is a move forward in AI governance. The aim of this initiative is toward producing guidance that encompasses both technical (e.g. system impact assessments) and non-engineering (e.g. human oversight) components to governance and represents a significant milestone in the movement towards standardising AI governance. This paper will summarise and critically evaluate the ICO effort and try to anticipate future debates and present some general recommendations.","2021-02-07","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:44","","","","","","AI Ethics","AI auditing and impact assessment","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\LQD7CUSB\Kazim et al. - 2021 - AI auditing and impact assessment according to th.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HKEQDIWS","journalArticle","2021","Singh, Rajeev Kumar; Pandey, Rohan; Babu, Rishie Nandhan","COVIDScreen: explainable deep learning framework for differential diagnosis of COVID-19 using chest X-rays","Neural Computing and Applications","","1433-3058","10.1007/s00521-020-05636-6","https://doi.org/10.1007/s00521-020-05636-6","COVID-19 has emerged as a global crisis with unprecedented socio-economic challenges, jeopardizing our lives and livelihoods for years to come. The unavailability of vaccines for COVID-19 has rendered rapid testing of the population instrumental in order to contain the exponential rise in cases of infection. Shortage of RT-PCR test kits and delays in obtaining test results calls for alternative methods of rapid and reliable diagnosis. In this article, we propose a novel deep learning-based solution using chest X-rays which can help in rapid triaging of COVID-19 patients. The proposed solution uses image enhancement, image segmentation, and employs a modified stacked ensemble model consisting of four CNN base-learners along with Naive Bayes as meta-learner to classify chest X-rays into three classes viz. COVID-19, pneumonia, and normal. An effective pruning strategy as introduced in the proposed framework results in increased model performance, generalizability, and decreased model complexity. We incorporate explainability in our article by using Grad-CAM visualization in order to establish trust in the medical AI system. Furthermore, we evaluate multiple state-of-the-art GAN architectures and their ability to generate realistic synthetic samples of COVID-19 chest X-rays to deal with limited numbers of training samples. The proposed solution significantly outperforms existing methods, with 98.67% accuracy, 0.98 Kappa score, and F-1 scores of 100, 98, and 98 for COVID-19, normal, and pneumonia classes, respectively, on standard datasets. The proposed solution can be used as one element of patient evaluation along with gold-standard clinical and laboratory testing.","2021-01-08","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:44","","","","","","Neural Comput & Applic","COVIDScreen","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\WG8QAVUR\Singh et al. - 2021 - COVIDScreen explainable deep learning framework f.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZVFJE4K","journalArticle","2021","Ruiz, Alejandro Pasos; Flynn, Michael; Large, James; Middlehurst, Matthew; Bagnall, Anthony","The great multivariate time series classification bake off: a review and experimental evaluation of recent algorithmic advances","Data Mining and Knowledge Discovery","","1573-756X","10.1007/s10618-020-00727-3","https://doi.org/10.1007/s10618-020-00727-3","Time Series Classification (TSC) involves building predictive models for a discrete target variable from ordered, real valued, attributes. Over recent years, a new set of TSC algorithms have been developed which have made significant improvement over the previous state of the art. The main focus has been on univariate TSC, i.e. the problem where each case has a single series and a class label. In reality, it is more common to encounter multivariate TSC (MTSC) problems where the time series for a single case has multiple dimensions. Despite this, much less consideration has been given to MTSC than the univariate case. The UCR archive has provided a valuable resource for univariate TSC, and the lack of a standard set of test problems may explain why there has been less focus on MTSC. The UEA archive of 30 MTSC problems released in 2018 has made comparison of algorithms easier. We review recently proposed bespoke MTSC algorithms based on deep learning, shapelets and bag of words approaches. If an algorithm cannot naturally handle multivariate data, the simplest approach to adapt a univariate classifier to MTSC is to ensemble it over the multivariate dimensions. We compare the bespoke algorithms to these dimension independent approaches on the 26 of the 30 MTSC archive problems where the data are all of equal length. We demonstrate that four classifiers are significantly more accurate than the benchmark dynamic time warping algorithm and that one of these recently proposed classifiers, ROCKET, achieves significant improvement on the archive datasets in at least an order of magnitude less time than the other three.","2021-03-01","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:45","401-449","","2","35","","Data Min Knowl Disc","The great multivariate time series classification bake off","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\89CPFU35\Ruiz et al. - 2021 - The great multivariate time series classification .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZQPYVETW","journalArticle","2021","Tsamados, Andreas; Aggarwal, Nikita; Cowls, Josh; Morley, Jessica; Roberts, Huw; Taddeo, Mariarosaria; Floridi, Luciano","The ethics of algorithms: key problems and solutions","AI & SOCIETY","","1435-5655","10.1007/s00146-021-01154-8","https://doi.org/10.1007/s00146-021-01154-8","Research on the ethics of algorithms has grown substantially over the past decade. Alongside the exponential development and application of machine learning algorithms, new ethical problems and solutions relating to their ubiquitous use in society have been proposed. This article builds on a review of the ethics of algorithms published in 2016 (Mittelstadt et al. Big Data Soc 3(2), 2016). The goals are to contribute to the debate on the identification and analysis of the ethical implications of algorithms, to provide an updated analysis of epistemic and normative concerns, and to offer actionable guidance for the governance of the design, development and deployment of algorithms.","2021-02-20","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:45","","","","","","AI & Soc","The ethics of algorithms","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\4NMK4D3R\Tsamados et al. - 2021 - The ethics of algorithms key problems and solutio.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G87TDWAY","journalArticle","2021","Peeters, Marieke M. M.; van Diggelen, Jurriaan; van den Bosch, Karel; Bronkhorst, Adelbert; Neerincx, Mark A.; Schraagen, Jan Maarten; Raaijmakers, Stephan","Hybrid collective intelligence in a human–AI society","AI & SOCIETY","","1435-5655","10.1007/s00146-020-01005-y","https://doi.org/10.1007/s00146-020-01005-y","Within current debates about the future impact of Artificial Intelligence (AI) on human society, roughly three different perspectives can be recognised: (1) the technology-centric perspective, claiming that AI will soon outperform humankind in all areas, and that the primary threat for humankind is superintelligence; (2) the human-centric perspective, claiming that humans will always remain superior to AI when it comes to social and societal aspects, and that the main threat of AI is that humankind’s social nature is overlooked in technological designs; and (3) the collective intelligence-centric perspective, claiming that true intelligence lies in the collective of intelligent agents, both human and artificial, and that the main threat for humankind is that technological designs create problems at the collective, systemic level that are hard to oversee and control. The current paper offers the following contributions: (a) a clear description for each of the three perspectives, along with their history and background; (b) an analysis and interpretation of current applications of AI in human society according to each of the three perspectives, thereby disentangling miscommunication in the debate concerning threats of AI; and (c) a new integrated and comprehensive research design framework that addresses all aspects of the above three perspectives, and includes principles that support developers to reflect and anticipate upon potential effects of AI in society.","2021-03-01","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:45","217-238","","1","36","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\ZJY346EJ\Peeters et al. - 2021 - Hybrid collective intelligence in a human–AI socie.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N4VQR3Z6","journalArticle","2021","Nikpour, Hoda; Aamodt, Agnar","Inference and reasoning in a Bayesian knowledge-intensive CBR system","Progress in Artificial Intelligence","","2192-6360","10.1007/s13748-020-00223-1","https://doi.org/10.1007/s13748-020-00223-1","This paper presents the inference and reasoning methods in a Bayesian supported knowledge-intensive case-based reasoning (CBR) system called BNCreek. The inference and reasoning process in this system is a combination of three methods. The semantic network inference methods and the CBR method are employed to handle the difficulties of inferencing and reasoning in uncertain domains. The Bayesian network inference methods are employed to make the process more accurate. An experiment from oil well drilling as a complex and uncertain application domain is conducted. The system is evaluated against expert estimations and compared with seven other corresponding systems. The normalized discounted cumulative gain (NDCG) as a rank-based metric, the weighted error (WE), and root-square error (RSE) as the statistical metrics are employed to evaluate different aspects of the system capabilities. The results show the efficiency of the developed inference and reasoning methods.","2021-03-01","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:45","49-63","","1","10","","Prog Artif Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\HZQXCQPR\Nikpour and Aamodt - 2021 - Inference and reasoning in a Bayesian knowledge-in.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PF9IUKSM","journalArticle","2021","Gallagher, Ryan J.; Frank, Morgan R.; Mitchell, Lewis; Schwartz, Aaron J.; Reagan, Andrew J.; Danforth, Christopher M.; Dodds, Peter Sheridan","Generalized word shift graphs: a method for visualizing and explaining pairwise comparisons between texts","EPJ Data Science","","2193-1127","10.1140/epjds/s13688-021-00260-3","https://doi.org/10.1140/epjds/s13688-021-00260-3","A common task in computational text analyses is to quantify how two corpora differ according to a measurement like word frequency, sentiment, or information content. However, collapsing the texts’ rich stories into a single number is often conceptually perilous, and it is difficult to confidently interpret interesting or unexpected textual patterns without looming concerns about data artifacts or measurement validity. To better capture fine-grained differences between texts, we introduce generalized word shift graphs, visualizations which yield a meaningful and interpretable summary of how individual words contribute to the variation between two texts for any measure that can be formulated as a weighted average. We show that this framework naturally encompasses many of the most commonly used approaches for comparing texts, including relative frequencies, dictionary scores, and entropy-based measures like the Kullback–Leibler and Jensen–Shannon divergences. Through a diverse set of case studies ranging from presidential speeches to tweets posted in urban green spaces, we demonstrate how generalized word shift graphs can be flexibly applied across domains for diagnostic investigation, hypothesis generation, and substantive interpretation. By providing a detailed lens into textual shifts between corpora, generalized word shift graphs help computational social scientists, digital humanists, and other text analysis practitioners fashion more robust scientific narratives.","2021-01-19","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:45","4","","1","10","","EPJ Data Sci.","Generalized word shift graphs","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\NI6G8ZJN\Gallagher et al. - 2021 - Generalized word shift graphs a method for visual.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y27GGX48","journalArticle","2021","Zolfaghari, Samaneh; Khodabandehloo, Elham; Riboni, Daniele","TraMiner: Vision-Based Analysis of Locomotion Traces for Cognitive Assessment in Smart-Homes","Cognitive Computation","","1866-9964","10.1007/s12559-020-09816-3","https://doi.org/10.1007/s12559-020-09816-3","The rapid increase in the senior population is posing serious challenges to national healthcare systems. Hence, innovative tools are needed to early detect health issues, including cognitive decline. Several clinical studies show that it is possible to identify cognitive impairment based on the locomotion patterns of the elderly. In this work, we investigate the use of sensor data and deep learning to recognize those patterns in instrumented smart-homes. In order to get rid of the noise introduced by indoor constraints and activity execution, we introduce novel visual feature extraction methods for locomotion data. Our solution relies on locomotion trace segmentation, image-based extraction of salient features from locomotion segments, and vision-based deep learning. We carried out extensive experiments with a large dataset acquired in a smart-home test bed from 153 seniors, including people with cognitive diseases. Results show that our system can accurately recognize the cognitive status of the senior, reaching a macro-$$F_1$$score of 0.873 for the three categories that we target: cognitive health, mild cognitive impairment, and dementia. Moreover, an experimental comparison shows that our system outperforms state-of-the-art methods.","2021-02-02","2021-06-07 21:08:45","2021-06-07 21:08:45","2021-06-07 21:08:45","","","","","","Cogn Comput","TraMiner","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\MBZRM5TG\Zolfaghari et al. - 2021 - TraMiner Vision-Based Analysis of Locomotion Trac.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQ5836ZH","journalArticle","2020","Rehs, Andreas","A structural topic model approach to scientific reorientation of economics and chemistry after German reunification","Scientometrics","","1588-2861","10.1007/s11192-020-03640-0","https://doi.org/10.1007/s11192-020-03640-0","The detection of differences or similarities in large numbers of scientific publications is an open problem in scientometric research. In this paper we therefore develop and apply a machine learning approach based on structural topic modelling in combination with cosine similarity and a linear regression framework in order to identify differences in dissertation titles written at East and West German universities before and after German reunification. German reunification and its surrounding time period is used because it provides a structure with both minor and major differences in research topics that could be detected by our approach. Our dataset is based on dissertation titles in economics and business administration and chemistry from 1980 to 2010. We use university affiliation and year of the dissertation to train a structural topic model and then test the model on a set of unseen dissertation titles. Subsequently, we compare the resulting topic distribution of each title to every other title with cosine similarity. The cosine similarities and the regional and temporal origin of the dissertation titles they come from are then used in a linear regression approach. Our results on research topics in economics and business administration suggest substantial differences between East and West Germany before the reunification and a rapid conformation thereafter. In chemistry we observe minor differences between East and West before the reunification and a slightly increased similarity thereafter.","2020-11-01","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:10","1229-1251","","2","125","","Scientometrics","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\MXR6SSFD\Rehs - 2020 - A structural topic model approach to scientific re.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DTXMSKRR","journalArticle","2020","Karim, Md. Rezaul; Rahman, Ashiqur; Jares, João Bosco; Decker, Stefan; Beyan, Oya","A snapshot neural ensemble method for cancer-type prediction based on copy number variations","Neural Computing and Applications","","1433-3058","10.1007/s00521-019-04616-9","https://doi.org/10.1007/s00521-019-04616-9","An accurate diagnosis and prognosis for cancer are specific to patients with particular cancer types and molecular traits, which needs to address carefully. The discovery of important biomarkers is becoming an important step toward understanding the molecular mechanisms of carcinogenesis in which genomics data and clinical outcomes need to be analyzed before making any clinical decision. Copy number variations (CNVs) are found to be associated with the risk of individual cancers and hence can be used to reveal genetic predispositions before cancer develops. In this paper, we collect the CNVs data about 8000 cancer patients covering 14 different cancer types from The Cancer Genome Atlas. Then, two different sparse representations of CNVs based on 578 oncogenes and 20,308 protein-coding genes, including genomic deletions and duplication across the samples, are prepared. Then, we train Conv-LSTM and convolutional autoencoder (CAE) networks using both representations and create snapshot models. While the Conv-LSTM can capture locally and globally important features, CAE can utilize unsupervised pretraining to initialize the weights in the subsequent convolutional layers against the sparsity. Model averaging ensemble (MAE) is then applied to combine the snapshot models in order to make a single prediction. Finally, we identify most significant CNVs biomarkers using guided-gradient class activation map plus (GradCAM++) and rank top genes for different cancer types. Results covering several experiments show fairly high prediction accuracies for the majority of cancer types. In particular, using protein-coding genes, Conv-LSTM and CAE networks can predict cancer types correctly at least 72.96% and 76.77% of the cases, respectively. Contrarily, using oncogenes gives moderately higher accuracies of 74.25% and 78.32%, whereas the snapshot model based on MAE shows overall 2.5% of accuracy improvement.","2020-10-01","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:10","15281-15299","","19","32","","Neural Comput & Applic","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\Y5JKWV96\Karim et al. - 2020 - A snapshot neural ensemble method for cancer-type .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"49IKFMEE","journalArticle","2020","Manevitz, Larry M.; Frid, Alex","Cognition and Neurocomputation","Annals of Mathematics and Artificial Intelligence","","1573-7470","10.1007/s10472-020-09713-3","https://doi.org/10.1007/s10472-020-09713-3","","2020-12-01","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:11","1119-1123","","11","88","","Ann Math Artif Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\BJGP5GA3\Manevitz and Frid - 2020 - Cognition and Neurocomputation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E6GWSZMN","journalArticle","2020","Kiener, Maximilian","Artificial intelligence in medicine and the disclosure of risks","AI & SOCIETY","","1435-5655","10.1007/s00146-020-01085-w","https://doi.org/10.1007/s00146-020-01085-w","This paper focuses on the use of ‘black box’ AI in medicine and asks whether the physician needs to disclose to patients that even the best AI comes with the risks of cyberattacks, systematic bias, and a particular type of mismatch between AI’s implicit assumptions and an individual patient’s background situation. Pace current clinical practice, I argue that, under certain circumstances, these risks do need to be disclosed. Otherwise, the physician either vitiates a patient’s informed consent or violates a more general obligation to warn him about potentially harmful consequences. To support this view, I argue, first, that the already widely accepted conditions in the evaluation of risks, i.e. the ‘nature’ and ‘likelihood’ of risks, speak in favour of disclosure and, second, that principled objections against the disclosure of these risks do not withstand scrutiny. Moreover, I also explain that these risks are exacerbated by pandemics like the COVID-19 crisis, which further emphasises their significance.","2020-10-22","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:11","","","","","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\CN8LX37C\Kiener - 2020 - Artificial intelligence in medicine and the disclo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZ4W5NGP","journalArticle","2020","Illankoon, Prasanna; Tretten, Phillip","Collaborating AI and human experts in the maintenance domain","AI & SOCIETY","","1435-5655","10.1007/s00146-020-01076-x","https://doi.org/10.1007/s00146-020-01076-x","Maintenance decision errors can result in very costly problems. The 4th industrial revolution has given new opportunities for the development of and use of intelligent decision support systems. With these technological advancements, key concerns focus on gaining a better understanding of the linkage between the technicians’ knowledge and the intelligent decision support systems. The research reported in this study has two primary objectives. (1) To propose a theoretical model that links technicians’ knowledge and intelligent decision support systems, and (2) to present a use case how to apply the theoretical model. The foundation of the new model builds upon two main streams of study in the decision support literature: “distribution” of knowledge among different agents, and “collaboration” of knowledge for reaching a shared goal. This study resulted in the identification of two main gaps: firstly, there must be a greater focus upon the technicians’ knowledge; secondly, technicians need assistance to maintain their focus on the big picture. We used the cognitive fit theory, and the theory of distributed situation awareness to propose the new theoretical model called “distributed collaborative awareness model.” The model considers both explicit and implicit knowledge and accommodates the dynamic challenges involved in operational level maintenance. As an application of this model, we identify and recommend some technological developments required in augmented reality based maintenance decision support.","2020-10-01","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:11","","","","","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\IIEF52DT\Illankoon and Tretten - 2020 - Collaborating AI and human experts in the maintena.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQAYZJVA","journalArticle","2020","","A A News","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-020-00695-2","https://doi.org/10.1007/s13218-020-00695-2","","2020-12-01","2021-06-07 21:09:12","2021-06-17 14:04:10","2021-06-07 21:09:11","585-588","","4","34","","Künstl Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\48ESEIMF\2020 - News.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FZP6SSEI","journalArticle","2020","Miyata, Yosuke; Ishita, Emi; Yang, Fang; Yamamoto, Michimasa; Iwase, Azusa; Kurata, Keiko","Knowledge structure transition in library and information science: topic modeling and visualization","Scientometrics","","1588-2861","10.1007/s11192-020-03657-5","https://doi.org/10.1007/s11192-020-03657-5","The purpose of this research is to identify topics in library and information science (LIS) using latent Dirichlet allocation (LDA) and to visualize the knowledge structure of the field as consisting of specific topics and its transition from 2000–2002 to 2015–2017. The full text of 1648 research articles from five peer-reviewed representative LIS journals in these two periods was analyzed by using LDA. A total of 30 topics in each period were labeled based on the frequency of terms and the contents of the articles. These topics were plotted on a two-dimensional map using LDAvis and categorized based on their location and characteristics in the plots. Although research areas in some forms were persistent with which discovered in previous studies, they were crucial to the transition of the knowledge structure in LIS and had the following three features: (1) The Internet became the premise of research in LIS in 2015–2017. (2) Theoretical approach or empirical work can be considered as a factor in the transition of the knowledge structure in some categories. (3) The topic diversity of the five core LIS journals decreased from the 2000–2002 to 2015–2017.","2020-10-01","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:11","665-687","","1","125","","Scientometrics","Knowledge structure transition in library and information science","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\AGQ7GHXE\Miyata et al. - 2020 - Knowledge structure transition in library and info.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DES2G486","journalArticle","2020","Noor, Manan Binth Taj; Zenia, Nusrat Zerin; Kaiser, M. Shamim; Mamun, Shamim Al; Mahmud, Mufti","Application of deep learning in detecting neurological disorders from magnetic resonance images: a survey on the detection of Alzheimer’s disease, Parkinson’s disease and schizophrenia","Brain Informatics","","2198-4026","10.1186/s40708-020-00112-2","https://doi.org/10.1186/s40708-020-00112-2","Neuroimaging, in particular magnetic resonance imaging (MRI), has been playing an important role in understanding brain functionalities and its disorders during the last couple of decades. These cutting-edge MRI scans, supported by high-performance computational tools and novel ML techniques, have opened up possibilities to unprecedentedly identify neurological disorders. However, similarities in disease phenotypes make it very difficult to detect such disorders accurately from the acquired neuroimaging data. This article critically examines and compares performances of the existing deep learning (DL)-based methods to detect neurological disorders—focusing on Alzheimer’s disease, Parkinson’s disease and schizophrenia—from MRI data acquired using different modalities including functional and structural MRI. The comparative performance analysis of various DL architectures across different disorders and imaging modalities suggests that the Convolutional Neural Network outperforms other methods in detecting neurological disorders. Towards the end, a number of current research challenges are indicated and some possible future research directions are provided.","2020-10-09","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:11","11","","1","7","","Brain Inf.","Application of deep learning in detecting neurological disorders from magnetic resonance images","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\UUUUWGJQ\Noor et al. - 2020 - Application of deep learning in detecting neurolog.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E8M6JEBN","journalArticle","2020","Masud, Mehedi; Eldin Rashed, Amr E.; Hossain, M. Shamim","Convolutional neural network-based models for diagnosis of breast cancer","Neural Computing and Applications","","1433-3058","10.1007/s00521-020-05394-5","https://doi.org/10.1007/s00521-020-05394-5","Breast cancer is the most prevailing cancer in the world and each year affecting millions of women. It is also the cause of largest number of deaths in women dying in cancers. During the last few years, researchers are proposing different convolutional neural network models in order to facilitate diagnostic process of breast cancer. Convolutional neural networks are showing promising results to classify cancers using image datasets. There is still a lack of standard models which can claim the best model because of unavailability of large datasets that can be used for models’ training and validation. Hence, researchers are now focusing on leveraging the transfer learning approach using pre-trained models as feature extractors that are trained over millions of different images. With this motivation, this paper considers eight different fine-tuned pre-trained models to observe how these models classify breast cancers applying on ultrasound images. We also propose a shallow custom convolutional neural network that outperforms the pre-trained models with respect to different performance metrics. The proposed model shows 100% accuracy and achieves 1.0 AUC score, whereas the best pre-trained model shows 92% accuracy and 0.972 AUC score. In order to avoid biasness, the model is trained using the fivefold cross validation technique. Moreover, the model is faster in training than the pre-trained models and requires a small number of trainable parameters. The Grad-CAM heat map visualization technique also shows how perfectly the proposed model extracts important features to classify breast cancers.","2020-10-09","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:11","","","","","","Neural Comput & Applic","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\XIG5GBGB\Masud et al. - 2020 - Convolutional neural network-based models for diag.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DQQ2LWCQ","journalArticle","2020","Tachmazidis, Ilias; Chen, Tianhua; Adamou, Marios; Antoniou, Grigoris","A hybrid AI approach for supporting clinical diagnosis of attention deficit hyperactivity disorder (ADHD) in adults","Health Information Science and Systems","","2047-2501","10.1007/s13755-020-00123-7","https://doi.org/10.1007/s13755-020-00123-7","Attention deficit hyperactivity disorder (ADHD) is a neurodevelopmental disorder that includes symptoms such as inattentiveness, hyperactivity and impulsiveness. It is considered as an important public health issue and prevalence of, as well as demand for diagnosis, has increased as awareness of the disease grew over the past years. Supply of specialist medical experts has not kept pace with the increasing demand for assessment, both due to financial pressures on health systems and the difficulty to train new experts, resulting in growing waiting lists. Patients are not being treated quickly enough causing problems in other areas of health systems (e.g. increased GP visits, increased risk of self-harm and accidents) and more broadly (e.g. time off work, relationship problems). Advances in AI make it possible to support the clinical diagnosis of ADHD based on the analysis of relevant data. This paper reports on findings related to the mental health services of a specialist Trust within the UK’s National Health Service (NHS). The analysis studied data of adult patients who underwent diagnosis over the past few years, and developed a hybrid approach, consisting of two different models: a machine learning model obtained by training on data of past cases; and a knowledge model capturing the expertise of medical experts through knowledge engineering. The resulting algorithm has an accuracy of 95% on data currently available, and is currently being tested in a clinical environment.","2020-11-20","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:11","1","","1","9","","Health Inf Sci Syst","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\RY497Z84\Tachmazidis et al. - 2020 - A hybrid AI approach for supporting clinical diagn.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQKVQR6F","journalArticle","2020","Ma, Zhengjing; Mei, Gang; Piccialli, Francesco","Machine learning for landslides prevention: a survey","Neural Computing and Applications","","1433-3058","10.1007/s00521-020-05529-8","https://doi.org/10.1007/s00521-020-05529-8","Landslides are one of the most critical categories of natural disasters worldwide and induce severely destructive outcomes to human life and the overall economic system. To reduce its negative effects, landslides prevention has become an urgent task, which includes investigating landslide-related information and predicting potential landslides. Machine learning is a state-of-the-art analytics tool that has been widely used in landslides prevention. This paper presents a comprehensive survey of relevant research on machine learning applied in landslides prevention, mainly focusing on (1) landslides detection based on images, (2) landslides susceptibility assessment, and (3) the development of landslide warning systems. Moreover, this paper discusses the current challenges and potential opportunities in the application of machine learning algorithms for landslides prevention.","2020-11-22","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:12","","","","","","Neural Comput & Applic","Machine learning for landslides prevention","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\5NIHZ3VB\Ma et al. - 2020 - Machine learning for landslides prevention a surv.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D8IKIJQ2","journalArticle","2020","Hacker, Philipp; Krestel, Ralf; Grundmann, Stefan; Naumann, Felix","Explainable AI under contract and tort law: legal incentives and technical challenges","Artificial Intelligence and Law","","1572-8382","10.1007/s10506-020-09260-6","https://doi.org/10.1007/s10506-020-09260-6","This paper shows that the law, in subtle ways, may set hitherto unrecognized incentives for the adoption of explainable machine learning applications. In doing so, we make two novel contributions. First, on the legal side, we show that to avoid liability, professional actors, such as doctors and managers, may soon be legally compelled to use explainable ML models. We argue that the importance of explainability reaches far beyond data protection law, and crucially influences questions of contractual and tort liability for the use of ML models. To this effect, we conduct two legal case studies, in medical and corporate merger applications of ML. As a second contribution, we discuss the (legally required) trade-off between accuracy and explainability and demonstrate the effect in a technical case study in the context of spam classification.","2020-12-01","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:12","415-439","","4","28","","Artif Intell Law","Explainable AI under contract and tort law","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\L4CXVHPH\Hacker et al. - 2020 - Explainable AI under contract and tort law legal .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZDFTS6BQ","journalArticle","2020","Shukla, Prashant; Verma, Abhishek; Abhishek; Verma, Shekhar; Kumar, Manish","Interpreting SVM for medical images using Quadtree","Multimedia Tools and Applications","","1573-7721","10.1007/s11042-020-09431-2","https://doi.org/10.1007/s11042-020-09431-2","In this paper, we propose a quadtree based approach to capture the spatial information of medical images for explaining nonlinear SVM prediction. In medical image classification, interpretability becomes important to understand why the adopted model works. Explaining an SVM prediction is difficult due to implicit mapping done in kernel classification is uninformative about the position of data points in the feature space and the nature of the separating hyperplane in the original space. The proposed method finds ROIs which contain the discriminative regions behind the prediction. Localization of the discriminative region in small boxes can help in interpreting the prediction by SVM. Quadtree decomposition is applied recursively before applying SVMs on sub images and model identified ROIs are highlighted. Pictorial results of experiments on various medical image datasets prove the effectiveness of this approach. We validate the correctness of our method by applying occlusion methods.","2020-10-01","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:12","29353-29373","","39","79","","Multimed Tools Appl","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\ZNDCIZP3\Shukla et al. - 2020 - Interpreting SVM for medical images using Quadtree.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"57LAVSTZ","journalArticle","2020","Tran, Thi Ngoc Trang; Felfernig, Alexander; Trattner, Christoph; Holzinger, Andreas","Recommender systems in the healthcare domain: state-of-the-art and research issues","Journal of Intelligent Information Systems","","1573-7675","10.1007/s10844-020-00633-6","https://doi.org/10.1007/s10844-020-00633-6","Nowadays, a vast amount of clinical data scattered across different sites on the Internet hinders users from finding helpful information for their well-being improvement. Besides, the overload of medical information (e.g., on drugs, medical tests, and treatment suggestions) have brought many difficulties to medical professionals in making patient-oriented decisions. These issues raise the need to apply recommender systems in the healthcare domain to help both, end-users and medical professionals, make more efficient and accurate health-related decisions. In this article, we provide a systematic overview of existing research on healthcare recommender systems. Different from existing related overview papers, our article provides insights into recommendation scenarios and recommendation approaches. Examples thereof are food recommendation, drug recommendation, health status prediction, healthcare service recommendation, and healthcare professional recommendation. Additionally, we develop working examples to give a deep understanding of recommendation algorithms. Finally, we discuss challenges concerning the development of healthcare recommender systems in the future.","2020-12-17","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:12","","","","","","J Intell Inf Syst","Recommender systems in the healthcare domain","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\MKYTQHEM\Tran et al. - 2020 - Recommender systems in the healthcare domain stat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GRGPUSEY","journalArticle","2020","Loyola-González, Octavio; Medina-Pérez, Miguel Angel; Choo, Kim-Kwang Raymond","A Review of Supervised Classification based on Contrast Patterns: Applications, Trends, and Challenges","Journal of Grid Computing","","1572-9184","10.1007/s10723-020-09526-y","https://doi.org/10.1007/s10723-020-09526-y","Supervised classification based on Contrast Patterns (CP) is a trending topic in the pattern recognition literature, partly because it contains an important family of both understandable and accurate classifiers. In this paper, we survey 105 articles and provide an in-depth review of CP-based supervised classification and its applications. Based on our review, we present a taxonomy of the existing application domains of CP-based supervised classification, and a scientometric study. We also discuss potential future research opportunities.","2020-12-01","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:12","797-845","","4","18","","J Grid Computing","A Review of Supervised Classification based on Contrast Patterns","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\AVZ27F5F\Loyola-González et al. - 2020 - A Review of Supervised Classification based on Con.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YIIA43E6","journalArticle","2020","Chazette, Larissa; Schneider, Kurt","Explainability as a non-functional requirement: challenges and recommendations","Requirements Engineering","","1432-010X","10.1007/s00766-020-00333-1","https://doi.org/10.1007/s00766-020-00333-1","Software systems are becoming increasingly complex. Their ubiquitous presence makes users more dependent on their correctness in many aspects of daily life. As a result, there is a growing need to make software systems and their decisions more comprehensible, with more transparency in software-based decision making. Transparency is therefore becoming increasingly important as a non-functional requirement. However, the abstract quality aspect of transparency needs to be better understood and related to mechanisms that can foster it. The integration of explanations into software has often been discussed as a solution to mitigate system opacity. Yet, an important first step is to understand user requirements in terms of explainable software behavior: Are users really interested in software transparency and are explanations considered an appropriate way to achieve it? We conducted a survey with 107 end users to assess their opinion on the current level of transparency in software systems and what they consider to be the main advantages and disadvantages of embedded explanations. We assess the relationship between explanations and transparency and analyze its potential impact on software quality. As explainability has become an important issue, researchers and professionals have been discussing how to deal with it in practice. While there are differences of opinion on the need for built-in explanations, understanding this concept and its impact on software is a key step for requirements engineering. Based on our research results and on the study of existing literature, we offer recommendations for the elicitation and analysis of explainability and discuss strategies for the practice.","2020-12-01","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:12","493-514","","4","25","","Requirements Eng","Explainability as a non-functional requirement","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\V3J6DB68\Chazette and Schneider - 2020 - Explainability as a non-functional requirement ch.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"45UTTX8J","journalArticle","2020","Hatwell, Julian; Gaber, Mohamed Medhat; Azad, R. Muhammad Atif","A A CHIRPS: Explaining random forest classification","Artificial Intelligence Review","","1573-7462","10.1007/s10462-020-09833-6","https://doi.org/10.1007/s10462-020-09833-6","Modern machine learning methods typically produce “black box” models that are opaque to interpretation. Yet, their demand has been increasing in the Human-in-the-Loop processes, that is, those processes that require a human agent to verify, approve or reason about the automated decisions before they can be applied. To facilitate this interpretation, we propose Collection of High Importance Random Path Snippets (CHIRPS); a novel algorithm for explaining random forest classification per data instance. CHIRPS extracts a decision path from each tree in the forest that contributes to the majority classification, and then uses frequent pattern mining to identify the most commonly occurring split conditions. Then a simple, conjunctive form rule is constructed where the antecedent terms are derived from the attributes that had the most influence on the classification. This rule is returned alongside estimates of the rule’s precision and coverage on the training data along with counter-factual details. An experimental study involving nine data sets shows that classification rules returned by CHIRPS have a precision at least as high as the state of the art when evaluated on unseen data (0.91–0.99) and offer a much greater coverage (0.04–0.54). Furthermore, CHIRPS uniquely controls against under- and over-fitting solutions by maximising novel objective functions that are better suited to the local (per instance) explanation setting.","2020-12-01","2021-06-07 21:09:12","2021-06-15 09:05:13","2021-06-07 21:09:12","5747-5788","","8","53","","Artif Intell Rev","CHIRPS","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\ZHI38G2Q\Hatwell et al. - 2020 - CHIRPS Explaining random forest classification.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PWYCQHJF","journalArticle","2020","Hekmati, Rasoul; Azencott, Robert; Zhang, Wei; Chu, Zili D.; Paldino, Michael J.","Localization of epileptic seizure focus by computerized analysis of fMRI recordings","Brain Informatics","","2198-4026","10.1186/s40708-020-00114-0","https://doi.org/10.1186/s40708-020-00114-0","By computerized analysis of cortical activity recorded via fMRI for pediatric epilepsy patients, we implement algorithmic localization of epileptic seizure focus within one of eight cortical lobes. Our innovative machine learning techniques involve intensive analysis of large matrices of mutual information coefficients between pairs of anatomically identified cortical regions. Drastic selection of pairs of regions with biologically significant inter-connectivity provides efficient inputs for our multi-layer perceptron (MLP) classifier. By imposing rigorous parameter parsimony to avoid overfitting, we construct a small-size MLP with very good percentages of successful classification.","2020-10-31","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:12","13","","1","7","","Brain Inf.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\X6HRJ4XB\Hekmati et al. - 2020 - Localization of epileptic seizure focus by compute.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2PZJEIUJ","journalArticle","2020","Bibri, Simon Elias; Krogstie, John","Environmentally data-driven smart sustainable cities: applied innovative solutions for energy efficiency, pollution reduction, and urban metabolism","Energy Informatics","","2520-8942","10.1186/s42162-020-00130-8","https://doi.org/10.1186/s42162-020-00130-8","The IoT and big data technologies have become essential to the functioning of both smart cities and sustainable cities, and thus, urban operational functioning and planning are becoming highly responsive to a form of data-driven urbanism. This offers the prospect of building models of smart sustainable cities functioning in real time from routinely sensed data. This in turn allows to monitor, understand, analyze, and plan such cities to improve their energy efficiency and environmental health in real time thanks to new urban intelligence functions as an advanced form of decision support. However, prior studies tend to deal largely with data-driven technologies and solutions in the realm of smart cities, mostly in relation to economic and social aspects, leaving important questions involving the underlying substantive and synergistic effects on environmental sustainability barely explored to date. These issues also apply to sustainable cities, especially eco-cities. Therefore, this paper investigates the potential and role of data-driven smart solutions in improving and advancing environmental sustainability in the context of smart cities as well as sustainable cities, under what can be labeled “environmentally data-driven smart sustainable cities.” To illuminate this emerging urban phenomenon, a descriptive/illustrative case study is adopted as a qualitative research methodology§ to examine and compare Stockholm and Barcelona as the ecologically and technologically leading cities in Europe respectively. The results show that smart grids, smart meters, smart buildings, smart environmental monitoring, and smart urban metabolism are the main data-driven smart solutions applied for improving and advancing environmental sustainability in both eco-cities and smart cities. There is a clear synergy between such solutions in terms of their interaction or cooperation to produce combined effects greater than the sum of their separate effects—with respect to the environment. This involves energy efficiency improvement, environmental pollution reduction, renewable energy adoption, and real-time feedback on energy flows, with high temporal and spatial resolutions. Stockholm takes the lead over Barcelona as regards the best practices for environmental sustainability given its long history of environmental work, strong environmental policy, progressive environmental performance, high environmental standards, and ambitious goals. It also has, like Barcelona, a high level of the implementation of applied data-driven technology solutions in the areas of energy and environment. However, the two cities differ in the nature of such implementation. We conclude that city governments do not have a unified agenda as a form of strategic planning, and data-driven decisions are unique to each city, so are environmental challenges. Big data are the answer, but each city sets its own questions based on what characterize it in terms of visions, policies, strategies, pathways, and priorities.","2020-11-23","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:12","29","","1","3","","Energy Inform","Environmentally data-driven smart sustainable cities","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\HFCJYDTD\Bibri and Krogstie - 2020 - Environmentally data-driven smart sustainable citi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VAQC9L9T","journalArticle","2020","Ferenc, Rudolf; Tóth, Zoltán; Ladányi, Gergely; Siket, István; Gyimóthy, Tibor","A public unified bug dataset for java and its assessment regarding metrics and bug prediction","Software Quality Journal","","1573-1367","10.1007/s11219-020-09515-0","https://doi.org/10.1007/s11219-020-09515-0","Bug datasets have been created and used by many researchers to build and validate novel bug prediction models. In this work, our aim is to collect existing public source code metric-based bug datasets and unify their contents. Furthermore, we wish to assess the plethora of collected metrics and the capabilities of the unified bug dataset in bug prediction. We considered 5 public datasets and we downloaded the corresponding source code for each system in the datasets and performed source code analysis to obtain a common set of source code metrics. This way, we produced a unified bug dataset at class and file level as well. We investigated the diversion of metric definitions and values of the different bug datasets. Finally, we used a decision tree algorithm to show the capabilities of the dataset in bug prediction. We found that there are statistically significant differences in the values of the original and the newly calculated metrics; furthermore, notations and definitions can severely differ. We compared the bug prediction capabilities of the original and the extended metric suites (within-project learning). Afterwards, we merged all classes (and files) into one large dataset which consists of 47,618 elements (43,744 for files) and we evaluated the bug prediction model build on this large dataset as well. Finally, we also investigated cross-project capabilities of the bug prediction models and datasets. We made the unified dataset publicly available for everyone. By using a public unified dataset as an input for different bug prediction related investigations, researchers can make their studies reproducible, thus able to be validated and verified.","2020-12-01","2021-06-07 21:09:12","2021-06-07 21:09:12","2021-06-07 21:09:12","1447-1506","","4","28","","Software Qual J","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\5LVVBXG7\Ferenc et al. - 2020 - A public unified bug dataset for java and its asse.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y6BAXUTD","journalArticle","2020","Shafik, Wasswa; Matinkhah, S. Mojtaba; Ghasemzadeh, Mohammad","Theoretical Understanding of Deep Learning in UAV Biomedical Engineering Technologies Analysis","SN Computer Science","","2661-8907","10.1007/s42979-020-00323-8","https://doi.org/10.1007/s42979-020-00323-8","The unmanned aerial vehicles (UAVs) emerged into a promising research trend within the recurrent year where current and future networks are to use enhanced connectivity in these digital immigrations in different fields like medical, communication, search, and rescue operations among others. The current technologies are using fixed base stations to operate on-site and off-site in the fixed position with its associated problems like poor connectivity. This opens gates for the UAVs technology to be used as a mobile alternative to increase accessibility with a fifth-generation (5G) connectivity that focuses on increased availability and connectivity. There has been less usage of wireless technologies in the medical field. This paper first presents a study on deep learning to medical field application in general, and provides detailed steps that are involved in the multi-armed bandit approach in solving UAV biomedical engineering technologies devices and medical exploration to exploitation dilemma. The paper further presents a detailed description of the bandit network applicability to achieve close optimal medical engineered devices’ performance and efficiency. The simulated results depicted that a multi-armed bandit problem approach can be applied in optimizing the performance of any medical networked device issue compared to the Thompson sampling, Bayesian algorithm, and ε-greedy algorithm. The results obtained further illustrated the optimized utilization of biomedical engineering technologies systems achieving thus close optimal performance on the average period through deep learning of realistic medical situations.","2020-09-24","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:41","307","","6","1","","SN COMPUT. SCI.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\HE96RWFK\Shafik et al. - 2020 - Theoretical Understanding of Deep Learning in UAV .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5I5KUKDH","journalArticle","2020","Lehmann, Claude; Goren Huber, Lilach; Horisberger, Thomas; Scheiba, Georg; Sima, Ana Claudia; Stockinger, Kurt","Big Data architecture for intelligent maintenance: a focus on query processing and machine learning algorithms","Journal of Big Data","","2196-1115","10.1186/s40537-020-00340-7","https://doi.org/10.1186/s40537-020-00340-7","Exploiting available condition monitoring data of industrial machines for intelligent maintenance purposes has been attracting attention in various application fields. Machine learning algorithms for fault detection, diagnosis and prognosis are popular and easily accessible. However, our experience in working at the intersection of academia and industry showed that the major challenges of building an end-to-end system in a real-world industrial setting go beyond the design of machine learning algorithms. One of the major challenges is the design of an end-to-end data management solution that is able to efficiently store and process large amounts of heterogeneous data streams resulting from a variety of physical machines. In this paper we present the design of an end-to-end Big Data architecture that enables intelligent maintenance in a real-world industrial setting. In particular, we will discuss various physical design choices for optimizing high-dimensional queries, such as partitioning and Z-ordering, that serve as the basis for health analytics. Finally, we describe a concrete fault detection use case with two different health monitoring algorithms based on machine learning and classical statistics and discuss their advantages and disadvantages. The paper covers some of the most important aspects of the practical implementation of such an end-to-end solution and demonstrates the challenges and their mitigation for the specific application of laser cutting machines.","2020-08-12","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:42","61","","1","7","","J Big Data","Big Data architecture for intelligent maintenance","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\R9Q9D5GB\Lehmann et al. - 2020 - Big Data architecture for intelligent maintenance.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8R4GUEZ3","journalArticle","2020","Bikku, Thulasi","Multi-layered deep learning perceptron approach for health risk prediction","Journal of Big Data","","2196-1115","10.1186/s40537-020-00316-7","https://doi.org/10.1186/s40537-020-00316-7","In today's world, due to the increase of medical data there is an interest in data preprocessing, classification and prediction of disease risks. Machine learning and Artificial Intelligence indicates that the predictive analysis becomes part of the medical activities especially in the domain of medical death prevention. The proposed work is focused on supervised learning methods and their capability to find hidden patterns in the real historical medical data. The objective is to predict future risk with a certain probability using Multi-layer perceptron (MLP) method. In the proposed work, MLP based on data classification technique is used for accurate classification and risk analysis of medical data. The proposed method is compared with traditional classification methods and the results show that the proposed method is better than the traditional methods.","2020-07-23","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:42","50","","1","7","","J Big Data","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\VL55XWLL\Bikku - 2020 - Multi-layered deep learning perceptron approach fo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HZA8QLYH","journalArticle","2020","Kraaij, Wessel; Verberne, Suzan; Koldijk, Saskia; de Korte, Elsbeth; van Dantzig, Saskia; Sappelli, Maya; Shoaib, Muhammad; Bosems, Steven; Achterkamp, Reinoud; Bonomi, Alberto; Schavemaker, John; Hulsebosch, Bob; Wabeke, Thymen; Vollenbroek-Hutten, Miriam; Neerincx, Mark; Sinderen, Marten van","Personalized support for well-being at work: an overview of the SWELL project","User Modeling and User-Adapted Interaction","","1573-1391","10.1007/s11257-019-09238-3","https://doi.org/10.1007/s11257-019-09238-3","Recent advances in wearable sensor technology and smartphones enable simple and affordable collection of personal analytics. This paper reflects on the lessons learned in the SWELL project that addressed the design of user-centered ICT applications for self-management of vitality in the domain of knowledge workers. These workers often have a sedentary lifestyle and are susceptible to mental health effects due to a high workload. We present the sense–reason–act framework that is the basis of the SWELL approach and we provide an overview of the individual studies carried out in SWELL. In this paper, we revisit our work on reasoning: interpreting raw heterogeneous sensor data, and acting: providing personalized feedback to support behavioural change. We conclude that simple affordable sensors can be used to classify user behaviour and heath status in a physically non-intrusive way. The interpreted data can be used to inform personalized feedback strategies. Further longitudinal studies can now be initiated to assess the effectiveness of m-Health interventions using the SWELL methods.","2020-07-01","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:42","413-446","","3","30","","User Model User-Adap Inter","Personalized support for well-being at work","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\UCE3A265\Kraaij et al. - 2020 - Personalized support for well-being at work an ov.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CKGFNELQ","journalArticle","2020","Kovacs, Tibor; Ko, Andrea","Monitoring Pneumatic Actuators’ Behavior Using Real-World Data Set","SN Computer Science","","2661-8907","10.1007/s42979-020-00202-2","https://doi.org/10.1007/s42979-020-00202-2","Developing a big data signal processing method is to monitor the behavior of a common component: a pneumatic actuator. The method is aimed at supporting condition-based maintenance activities: monitoring signals over an extended period, and identifying, classifying different machine states that may indicate abnormal behavior. Furthermore, preparing a balanced data set for training supervised machine learning models that represent the component’s all identified conditions. Peak detection, garbage removal and down-sampling by interpolation were applied for signal preprocessing. Undersampling the over-represented signals, Ward’s hierarchical clustering with multivariate Euclidean distance calculation and Kohonen self-organizing map (KSOM) methods were used for identifying and grouping similar signal patterns. The study demonstrated that the behavior of equipment displaying complex signals could be monitored with the method described. Both hierarchical clustering and KSOM are suitable methods for identifying and clustering signals of different machine states that may be overlooked if screened by humans. Using the proposed methods, signals could be screened thoroughly and over a long period of time that is critical when failures or abnormal behavior is rare. Visual display of the identified clusters over time could help analyzing the deterioration of machine conditions. The clustered signals could be used to create a balanced set of training data for developing supervised machine learning models to automatically identify previously recognized machine conditions that indicate abnormal behavior.","2020-06-11","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:42","196","","4","1","","SN COMPUT. SCI.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\57Z476QB\Kovacs and Ko - 2020 - Monitoring Pneumatic Actuators’ Behavior Using Rea.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QGRV2FQV","journalArticle","2020","Teso, Stefano; Hinz, Oliver","A A EDITORIAL Challenges in Interactive Machine Learning","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-020-00662-x","https://doi.org/10.1007/s13218-020-00662-x","","2020-06-01","2021-06-07 21:09:43","2021-06-15 09:23:23","2021-06-07 21:09:42","127-130","","2","34","","Künstl Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\BR7DW4DK\Teso and Hinz - 2020 - Challenges in Interactive Machine Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36REZ73E","journalArticle","2020","","A A News","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-020-00663-w","https://doi.org/10.1007/s13218-020-00663-w","","2020-06-01","2021-06-07 21:09:43","2021-06-17 14:04:05","2021-06-07 21:09:42","279-283","","2","34","","Künstl Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\2TD4NY4J\2020 - News.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PIK57I3B","journalArticle","2020","Ghoshal, Sid; Roberts, Stephen","Thresholded ConvNet ensembles: neural networks for technical forecasting","Neural Computing and Applications","","1433-3058","10.1007/s00521-020-04877-9","https://doi.org/10.1007/s00521-020-04877-9","Much of modern practice in financial forecasting relies on technicals, an umbrella term for several heuristics applying visual pattern recognition to price charts. Despite its ubiquity in financial media, the reliability of its signals remains a contentious and highly subjective form of ‘domain knowledge’. We investigate the predictive value of patterns in financial time series, applying machine learning and signal processing techniques to 22 years of US equity data. By reframing technical analysis as a poorly specified, arbitrarily preset feature-extractive layer in a deep neural network, we show that better convolutional filters can be learned directly from the data, and provide visual representations of the features being identified. We find that an ensemble of shallow, thresholded convolutional neural networks optimised over different resolutions achieves state-of-the-art performance on this domain, outperforming technical methods while retaining some of their interpretability.","2020-09-01","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:42","15249-15262","","18","32","","Neural Comput & Applic","Thresholded ConvNet ensembles","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\VFVB7Y8L\Ghoshal and Roberts - 2020 - Thresholded ConvNet ensembles neural networks for.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BN55NC2S","journalArticle","2020","van der Putten, Joost; van der Sommen, Fons; de Groof, Jeroen; Struyvenberg, Maarten; Zinger, Svitlana; Curvers, Wouter; Schoon, Erik; Bergman, Jacques; de With, Peter H. N.","Modeling clinical assessor intervariability using deep hypersphere encoder–decoder networks","Neural Computing and Applications","","1433-3058","10.1007/s00521-019-04607-w","https://doi.org/10.1007/s00521-019-04607-w","In medical imaging, a proper gold-standard ground truth as, e.g., annotated segmentations by assessors or experts is lacking or only scarcely available and suffers from large intervariability in those segmentations. Most state-of-the-art segmentation models do not take inter-observer variability into account and are fully deterministic in nature. In this work, we propose hypersphere encoder–decoder networks in combination with dynamic leaky ReLUs, as a new method to explicitly incorporate inter-observer variability into a segmentation model. With this model, we can then generate multiple proposals based on the inter-observer agreement. As a result, the output segmentations of the proposed model can be tuned to typical margins inherent to the ambiguity in the data. For experimental validation, we provide a proof of concept on a toy data set as well as show improved segmentation results on two medical data sets. The proposed method has several advantages over current state-of-the-art segmentation models such as interpretability in the uncertainty of segmentation borders. Experiments with a medical localization problem show that it offers improved biopsy localizations, which are on average 12% closer to the optimal biopsy location.","2020-07-01","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:42","10705-10717","","14","32","","Neural Comput & Applic","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\7TJ6KCLF\van der Putten et al. - 2020 - Modeling clinical assessor intervariability using .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D7E5MEVW","journalArticle","2020","Cimiano, Philipp; Heyer, Gerhard; Kohlhase, Michael; Stein, Benno; Ziegler, Jürgen; Härder, Theo","A A EDITORIAL","Datenbank-Spektrum","","1610-1995","10.1007/s13222-020-00352-w","https://doi.org/10.1007/s13222-020-00352-w","","2020-07-01","2021-06-07 21:09:43","2021-06-15 09:25:35","2021-06-07 21:09:42","87-91","","2","20","","Datenbank Spektrum","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\YCVLV4E7\Cimiano et al. - 2020 - Editorial.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RQS7PR9F","journalArticle","2020","Batarseh, Feras A.; Ghassib, Iya; Chong, Deri (Sondor); Su, Po-Hsuan","Preventive healthcare policies in the US: solutions for disease management using Big Data Analytics","Journal of Big Data","","2196-1115","10.1186/s40537-020-00315-8","https://doi.org/10.1186/s40537-020-00315-8","Data-driven healthcare policy discussions are gaining traction after the Covid-19 outbreak and ahead of the 2020 US presidential elections. The US has a hybrid healthcare structure; it is a system that does not provide universal coverage, albeit few years ago enacted a mandate (Affordable Care Act-ACA) that provides coverage for the majority of Americans. The US has the highest health expenditure per capita of all western and developed countries; however, most Americans don’t tap into the benefits of preventive healthcare. It is estimated that only 8% of Americans undergo routine preventive screenings. On a national level, very few states (15 out of the 50) have above-average preventive healthcare metrics. In literature, many studies focus on the cure of diseases (research areas such as drug discovery and disease prediction); whilst a minority have examined data-driven preventive measures—a matter that Americans and policy makers ought to place at the forefront of national issues. In this work, we present solutions for preventive practices and policies through Machine Learning (ML) methods. ML is morally neutral, it depends on the data that train the models; in this work, we make the case that Big Data is an imperative paradigm for healthcare. We examine disparities in clinical data for US patients by developing correlation and imputation methods for data completeness. Non-conventional patterns are identified. The data lifecycle followed is methodical and deliberate; 1000+ clinical, demographical, and laboratory variables are collected from the Centers for Disease Control and Prevention (CDC). Multiple statistical models are deployed (Pearson correlations, Cramer’s V, MICE, and ANOVA). Other unsupervised ML models are also examined (K-modes and K-prototypes for clustering). Through the results presented in the paper, pointers to preventive chronic disease tests are presented, and the models are tested and evaluated.","2020-06-23","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:42","38","","1","7","","J Big Data","Preventive healthcare policies in the US","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\FGTY9LID\Batarseh et al. - 2020 - Preventive healthcare policies in the US solution.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MHTTFNJP","journalArticle","2020","Agarwal, Ritu; Wadhwa, Mani","Review of State-of-the-Art Design Techniques for Chatbots","SN Computer Science","","2661-8907","10.1007/s42979-020-00255-3","https://doi.org/10.1007/s42979-020-00255-3","Amazon’s Alexa, Apple’s Siri, Google Assistant and Microsoft’s Cortana, clearly illustrate the impressive research work and potentials to be explored in the field of conversational agents. Conversational agent, chatter-bot or chatbot is a program expected to converse with near-human intelligence. Chatbots are designed to be used either as task-oriented ones or simply open-ended dialogue generator. Many approaches have been proposed in this field which ranges from earlier versions of hard-coded response generator to the advanced development techniques in Artificial Intelligence. In a broader sense, these can be categorized as rule-based and neural network based. While rule-based relies on predefined templates and responses, a neural network based relies on deep learning models. Rule-based are preferable for simpler task-oriented conversations. Open-domain conversational modeling is a more challenging area and uses mostly neural network-based approaches. This paper begins with an introduction of chatbots, followed by in-depth discussion on various classical or rule-based and neural-network-based approaches. The evaluation metrics employed for chatbots are mentioned. The paper concludes with a table consisting of recent research done in the field. It covers all the latest and significant publications in the field, the evaluation metrics employed, the corpus which is used as well as the possible areas of enhancement that exist in the proposed techniques.","2020-07-29","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:42","246","","5","1","","SN COMPUT. SCI.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\829JPQGX\Agarwal and Wadhwa - 2020 - Review of State-of-the-Art Design Techniques for C.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4KRMT7X3","journalArticle","2020","Casale, G.; Artač, M.; van den Heuvel, W.-J.; van Hoorn, A.; Jakovits, P.; Leymann, F.; Long, M.; Papanikolaou, V.; Presenza, D.; Russo, A.; Srirama, S. N.; Tamburri, D. A.; Wurster, M.; Zhu, L.","RADON: rational decomposition and orchestration for serverless computing","SICS Software-Intensive Cyber-Physical Systems","","2524-8529","10.1007/s00450-019-00413-w","https://doi.org/10.1007/s00450-019-00413-w","Emerging serverless computing technologies, such as function as a service (FaaS), enable developers to virtualize the internal logic of an application, simplifying the management of cloud-native services and allowing cost savings through billing and scaling at the level of individual functions. Serverless computing is therefore rapidly shifting the attention of software vendors to the challenge of developing cloud applications deployable on FaaS platforms. In this vision paper, we present the research agenda of the RADON project (http://radon-h2020.eu), which aims to develop a model-driven DevOps framework for creating and managing applications based on serverless computing. RADON applications will consist of fine-grained and independent microservices that can efficiently and optimally exploit FaaS and container technologies. Our methodology strives to tackle complexity in designing such applications, including the solution of optimal decomposition, the reuse of serverless functions as well as the abstraction and actuation of event processing chains, while avoiding cloud vendor lock-in through models.","2020-08-01","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:42","77-87","","1","35","","SICS Softw.-Inensiv. Cyber-Phys. Syst.","RADON","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\XPFS6J44\Casale et al. - 2020 - RADON rational decomposition and orchestration fo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N4I35WIP","journalArticle","2020","Pannu, Husanbir Singh; Ahuja, Sahil; Dang, Nitin; Soni, Sahil; Malhi, Avleen Kaur","Deep learning based image classification for intestinal hemorrhage","Multimedia Tools and Applications","","1573-7721","10.1007/s11042-020-08905-7","https://doi.org/10.1007/s11042-020-08905-7","Convolutional neural networks (CNN) have become a popular choice for image segmentation and classification. Internal body images are obscure in nature with involvement of noise, luminance variation, rotation and blur. Thus optimal choice of features for machine learning model to classify bleeding is still an open problem. CNN is efficient for attribute selection and ensemble learning makes a generalized robust system. Capsule endoscopy is a new technology which enables a gastroenterologist to visualize the entire digestive tract including small bowel to diagnose bleeding, ulcer and polyp. This paper presents a supervised learning ensemble to detect the bleeding in the images of Wireless Capsule Endoscopy. It accurately finds out the best possible combination of attributes required to classify bleeding symptoms in endoscopy images. A careful setting for CNN layer options and optimizer for back propagation after reducing the color palette using minimum variance quantization has shown promising results. Results of testing on public and real dataset has been analyzed. Proposed ensemble is able to achieve 0.95 on the public endoscopy dataset and 0.93 accuracy on the real video dataset. A detailed data analysis has also been incorporated in the study including RGB pixel intensities, distributions of binary classes and various class ratios for training.","2020-08-01","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:43","21941-21966","","29","79","","Multimed Tools Appl","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\XZ92ARTA\Pannu et al. - 2020 - Deep learning based image classification for intes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5BKSUR6V","journalArticle","2020","Bibri, Simon Elias; Krogstie, John","The emerging data–driven Smart City and its innovative applied solutions for sustainability: the cases of London and Barcelona","Energy Informatics","","2520-8942","10.1186/s42162-020-00108-6","https://doi.org/10.1186/s42162-020-00108-6","The big data revolution is heralding an era where instrumentation, datafication, and computation are increasingly pervading the very fabric of cities. Big data technologies have become essential to the functioning of cities. Consequently, urban processes and practices are becoming highly responsive to a form of data-driven urbanism that is the key mode of production for smart cities. Such form is increasingly being directed towards tackling the challenges of sustainability in the light of the escalating urbanization trend. This paper investigates how the emerging data-driven smart city is being practiced and justified in terms of the development and implementation of its innovative applied solutions for sustainability. To illuminate this new urban phenomenon, a descriptive case study is adopted as a qualitative research methodology to examine and compare London and Barcelona as the leading data-driven smart cities in Europe. This study shows that these cities have a high level of the development of applied data-driven technologies, but they slightly differ in the level of the implementation of such technologies in different city systems and domains with respect to sustainability areas. They also moderately differ in the degree of their readiness as to the availability and development level of the competences and infrastructure needed to generate, transmit, process, and analyze large masses of data to extract useful knowledge for enhanced decision making and deep insights pertaining to urban operational functioning, management, and planning in relation to sustainability. London takes the lead as regards the ICT infrastructure and data sources, whereas Barcelona has the best practices in the data-oriented competences, notably horizontal information platforms, operations centers, dashboards, training programs and educational institutes, innovation labs, research centers, and strategic planning offices. This research enhances the scholarly community’s current understanding of the new phenomenon of the data-driven city with respect to the untapped synergic potential of the integration of smart urbanism and sustainable urbanism for advancing sustainability in the light of the emerging paradigm of big data computing. No previous work has, to the best of our knowledge, explored and highlighted the link between the data-driven smart solutions and the sustainable development strategies in the context of data-driven sustainable smart cities as a new paradigm of urbanism.","2020-06-26","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:43","5","","1","3","","Energy Inform","The emerging data–driven Smart City and its innovative applied solutions for sustainability","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\CYEJBGLY\Bibri and Krogstie - 2020 - The emerging data–driven Smart City and its innova.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBGVFHYR","journalArticle","2020","Ulyanov, Dmitry; Vedaldi, Andrea; Lempitsky, Victor","Deep Image Prior","International Journal of Computer Vision","","1573-1405","10.1007/s11263-020-01303-4","https://doi.org/10.1007/s11263-020-01303-4","Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity (Code and supplementary material are available at https://dmitryulyanov.github.io/deep_image_prior).","2020-07-01","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:43","1867-1888","","7","128","","Int J Comput Vis","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\GH5YZQQX\Ulyanov et al. - 2020 - Deep Image Prior.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NWYV2NGD","journalArticle","2020","Collazos-Huertas, D. F.; Álvarez-Meza, A. M.; Acosta-Medina, C. D.; Castaño-Duque, G. A.; Castellanos-Dominguez, G.","CNN-based framework using spatial dropping for enhanced interpretation of neural activity in motor imagery classification","Brain Informatics","","2198-4026","10.1186/s40708-020-00110-4","https://doi.org/10.1186/s40708-020-00110-4","Interpretation of brain activity responses using motor imagery (MI) paradigms is vital for medical diagnosis and monitoring. Assessed by machine learning techniques, identification of imagined actions is hindered by substantial intra- and inter-subject variability. Here, we develop an architecture of Convolutional Neural Networks (CNN) with an enhanced interpretation of the spatial brain neural patterns that mainly contribute to the classification of MI tasks. Two methods of 2D-feature extraction from EEG data are contrasted: Power Spectral Density and Continuous Wavelet Transform. For preserving the spatial interpretation of extracting EEG patterns, we project the multi-channel data using a topographic interpolation. Besides, we include a spatial dropping algorithm to remove the learned weights that reflect the localities not engaged with the elicited brain response. We evaluate two labeled scenarios of MI tasks: bi-class and three-class. Obtained results in an MI database show that the thresholding strategy combined with Continuous Wavelet Transform improves the accuracy and enhances the interpretability of CNN architecture, showing that the highest contribution clusters over the sensorimotor cortex with a differentiated behavior of rhythms $$\mu $$and $$\beta $$.","2020-09-03","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:43","8","","1","7","","Brain Inf.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\NN27S3SY\Collazos-Huertas et al. - 2020 - CNN-based framework using spatial dropping for enh.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UK8LEVCF","journalArticle","2020","Rodríguez-Salas, Dalia; Mürschberger, Nina; Ravikumar, Nishant; Seuret, Mathias; Maier, Andreas","Mapping Ensembles of Trees to Sparse, Interpretable Multilayer Perceptron Networks","SN Computer Science","","2661-8907","10.1007/s42979-020-00268-y","https://doi.org/10.1007/s42979-020-00268-y","Tree-based classifiers provide easy-to-understand outputs. Artificial neural networks (ANN) commonly outperform tree-based classifiers; nevertheless, understanding their outputs requires specialized knowledge in most cases. The highly redundant architecture of ANN is typically designed through an expensive trial-and-error scheme. We aim at (1) investigating whether using ensembles of decision trees to design the architecture of low-redundant, sparse ANN provides better-performing networks, and (2) evaluating whether such trees can be used to provide human-understandable explanations for their outputs. Information about the hierarchy of the features, and how good they are at separating subsets of samples among the classes, is gathered from each branch in an ensemble of trees. This information is used to design the architecture of a sparse multilayer perceptron network. Networks built using our method are called ForestNet. Tree branches corresponding to highly activated neurons are used to provide explanations of the networks’ outputs. ForestNets are able to handle low- and high-dimensional data, as we show on an evaluation using four datasets. Our networks consistently outperformed their respective ensemble of trees and had similar performance to their fully connected counterparts with a significant reduction of connections. Furthermore, our interpretation method seems to provide support for the ForestNet outputs. While ForestNet’s architectures do not allow them yet to capture well the intrinsic variability of visual data, they exhibit very promising results by reducing more than 98% of connections for such visual tasks. Structure similarities between ForestNets and their respective tree ensemble provide means to interpret their outputs.","2020-08-06","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:43","252","","5","1","","SN COMPUT. SCI.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\EK5WGNAQ\Rodríguez-Salas et al. - 2020 - Mapping Ensembles of Trees to Sparse, Interpretabl.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZFQMQ8I5","journalArticle","2020","Chowdhury, Nihad K.; Rahman, Md. Muhtadir; Kabir, Muhammad Ashad","PDCOVIDNet: a parallel-dilated convolutional neural network architecture for detecting COVID-19 from chest X-ray images","Health Information Science and Systems","","2047-2501","10.1007/s13755-020-00119-3","https://doi.org/10.1007/s13755-020-00119-3","The COVID-19 pandemic continues to severely undermine the prosperity of the global health system. To combat this pandemic, effective screening techniques for infected patients are indispensable. There is no doubt that the use of chest X-ray images for radiological assessment is one of the essential screening techniques. Some of the early studies revealed that the patient’s chest X-ray images showed abnormalities, which is natural for patients infected with COVID-19. In this paper, we proposed a parallel-dilated convolutional neural network (CNN) based COVID-19 detection system from chest X-ray images, named as Parallel-Dilated COVIDNet (PDCOVIDNet). First, the publicly available chest X-ray collection fully preloaded and enhanced, and then classified by the proposed method. Differing convolution dilation rate in a parallel form demonstrates the proof-of-principle for using PDCOVIDNet to extract radiological features for COVID-19 detection. Accordingly, we have assisted our method with two visualization methods, which are specifically designed to increase understanding of the key components associated with COVID-19 infection. Both visualization methods compute gradients for a given image category related to feature maps of the last convolutional layer to create a class-discriminative region. In our experiment, we used a total of 2905 chest X-ray images, comprising three cases (such as COVID-19, normal, and viral pneumonia), and empirical evaluations revealed that the proposed method extracted more significant features expeditiously related to suspected disease. The experimental results demonstrate that our proposed method significantly improves performance metrics: the accuracy, precision, recall and F1 scores reach $$96.58\%$$, $$96.58\%$$, $$96.59\%$$and $$96.58\%$$, respectively, which is comparable or enhanced compared with the state-of-the-art methods. We believe that our contribution can support resistance to COVID-19, and will adopt for COVID-19 screening in AI-based systems.","2020-09-21","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:43","27","","1","8","","Health Inf Sci Syst","PDCOVIDNet","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\5VRV8JNA\Chowdhury et al. - 2020 - PDCOVIDNet a parallel-dilated convolutional neura.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"673TAKTZ","journalArticle","2020","Khowaja, Sunder Ali; Yahya, Bernardo Nugroho; Lee, Seok-Lyong","CAPHAR: context-aware personalized human activity recognition using associative learning in smart environments","Human-centric Computing and Information Sciences","","2192-1962","10.1186/s13673-020-00240-y","https://doi.org/10.1186/s13673-020-00240-y","The existing action recognition systems mainly focus on generalized methods to categorize human actions. However, the generalized systems cannot attain the same level of recognition performance for new users mainly due to the high variance in terms of human behavior and the way of performing actions, i.e. activity handling. The use of personalized models based on similarity was introduced to overcome the activity handling problem, but the improvement was found to be limited as the similarity was based on physiognomies rather than the behavior. Moreover, human interaction with contextual information has not been studied extensively in the domain of action recognition. Such interactions can provide an edge for both recognizing high-level activities and improving the personalization effect. In this paper, we propose the context-aware personalized human activity recognition (CAPHAR) framework which computes the class association rules between low-level actions/sensor activations and the contextual information to recognize high-level activities. The personalization in CAPHAR leverages the individual behavior process using a similarity metric to reduce the effect of the activity handling problem. The experimental results on the “daily lifelog” dataset show that CAPHAR can achieve at most 23.73% better accuracy for new users in comparison to the existing classification methods.","2020-08-12","2021-06-07 21:09:43","2021-06-07 21:09:43","2021-06-07 21:09:43","35","","1","10","","Hum. Cent. Comput. Inf. Sci.","CAPHAR","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\ILXAUGQQ\Khowaja et al. - 2020 - CAPHAR context-aware personalized human activity .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZIU2X6UW","journalArticle","2020","Tsai, Shuo-Chang; Chen, Cheng-Huan; Shiao, Yi-Tzone; Ciou, Jin-Shuei; Wu, Trong-Neng","Precision education with statistical learning and deep learning: a case study in Taiwan","International Journal of Educational Technology in Higher Education","","2365-9440","10.1186/s41239-020-00186-2","https://doi.org/10.1186/s41239-020-00186-2","The low birth rate in Taiwan has led to a severe challenge for many universities to enroll a sufficient number of students. Consequently, a large number of students have been admitted to universities regardless of whether they have an aptitude for academic studies. Early diagnosis of students with a high dropout risk enables interventions to be provided early on, which can help these students to complete their studies, graduate, and enhance their future competitiveness in the workplace. Effective prelearning interventions are necessary, therefore students’ learning backgrounds should be thoroughly examined. This study investigated how big data and artificial intelligence can be used to help universities to more precisely understand student backgrounds, according to which corresponding interventions can be provided. For this study, 3552 students from a university in Taiwan were sampled. A statistical learning method and a machine learning method based on deep neural networks were used to predict their probability of dropping out. The results revealed that student academic performance (regarding the dynamics of class ranking percentage), student loan applications, the number of absences from school, and the number of alerted subjects successfully predicted whether or not students would drop out of university with an accuracy rate of 68% when the statistical learning method was employed, and 77% for the deep learning method, in the case of giving first priority to the high sensitivity in predicting dropouts. However, when the specificity metric was preferred, then the two approaches both reached more than 80% accuracy rates. These results may enable the university to provide interventions to students for assisting course selection and enhancing their competencies based on their aptitudes, potentially reducing the dropout rate and facilitating adaptive learning, thereby achieving a win-win situation for both the university and the students. This research offers a feasible direction for using artificial intelligence applications on the basis of a university’s institutional research database.","2020-04-08","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:09","12","","1","17","","Int J Educ Technol High Educ","Precision education with statistical learning and deep learning","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\LAU7435V\Tsai et al. - 2020 - Precision education with statistical learning and .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"22WPU8PM","journalArticle","2020","Cerquitelli, Tania; Chiusano, Silvia; Vargas-Solar, Genoveva","A A Editorial special issue on data analytics for engineering, science and society","Computing","","1436-5057","10.1007/s00607-020-00810-z","https://doi.org/10.1007/s00607-020-00810-z","","2020-05-01","2021-06-07 21:10:11","2021-06-15 09:26:07","2021-06-07 21:10:10","1093-1096","","5","102","","Computing","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\WUSZB6AM\Cerquitelli et al. - 2020 - Editorial special issue on data analytics for engi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CWX3TVT6","journalArticle","2020","Holzinger, Andreas; Carrington, André; Müller, Heimo","Measuring the Quality of Explanations: The System Causability Scale (SCS)","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-020-00636-z","https://doi.org/10.1007/s13218-020-00636-z","Recent success in Artificial Intelligence (AI) and Machine Learning (ML) allow problem solving automatically without any human intervention. Autonomous approaches can be very convenient. However, in certain domains, e.g., in the medical domain, it is necessary to enable a domain expert to understand, why an algorithm came up with a certain result. Consequently, the field of Explainable AI (xAI) rapidly gained interest worldwide in various domains, particularly in medicine. Explainable AI studies transparency and traceability of opaque AI/ML and there are already a huge variety of methods. For example with layer-wise relevance propagation relevant parts of inputs to, and representations in, a neural network which caused a result, can be highlighted. This is a first important step to ensure that end users, e.g., medical professionals, assume responsibility for decision making with AI/ML and of interest to professionals and regulators. Interactive ML adds the component of human expertise to AI/ML processes by enabling them to re-enact and retrace AI/ML results, e.g. let them check it for plausibility. This requires new human–AI interfaces for explainable AI. In order to build effective and efficient interactive human–AI interfaces we have to deal with the question of how to evaluate the quality of explanations given by an explainable AI system. In this paper we introduce our System Causability Scale to measure the quality of explanations. It is based on our notion of Causability (Holzinger et al. in Wiley Interdiscip Rev Data Min Knowl Discov 9(4), 2019) combined with concepts adapted from a widely-accepted usability scale.","2020-06-01","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:10","193-198","","2","34","","Künstl Intell","Measuring the Quality of Explanations","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\6JR2Z7Q4\Holzinger et al. - 2020 - Measuring the Quality of Explanations The System .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VMP25ZEG","journalArticle","2020","Hisano, Ryohei; Sornette, Didier; Mizuno, Takayuki","Prediction of ESG compliance using a heterogeneous information network","Journal of Big Data","","2196-1115","10.1186/s40537-020-00295-9","https://doi.org/10.1186/s40537-020-00295-9","Negative screening is one method to avoid interactions with inappropriate entities. For example, financial institutions keep investment exclusion lists of inappropriate firms that have environmental, social, and governance (ESG) problems. They create their investment exclusion lists by gathering information from various news sources to keep their portfolios profitable as well as green. International organizations also maintain smart sanctions lists that are used to prohibit trade with entities that are involved in illegal activities. In the present paper, we focus on the prediction of investment exclusion lists in the finance domain. We construct a vast heterogeneous information network that covers the necessary information surrounding each firm, which is assembled using seven professionally curated datasets and two open datasets, which results in approximately 50 million nodes and 400 million edges in total. Exploiting these vast datasets and motivated by how professional investigators and journalists undertake their daily investigations, we propose a model that can learn to predict firms that are more likely to be added to an investment exclusion list in the near future. Our approach is tested using the negative news investment exclusion list data of more than 35,000 firms worldwide from January 2012 to May 2018. Comparing with the state-of-the-art methods with and without using the network, we show that the predictive accuracy is substantially improved when using the vast information stored in the heterogeneous information network. This work suggests new ways to consolidate the diffuse information contained in big data to monitor dominant firms on a global scale for better risk management and more socially responsible investment.","2020-03-16","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:10","22","","1","7","","J Big Data","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\JU7BCTH8\Hisano et al. - 2020 - Prediction of ESG compliance using a heterogeneous.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HDWEEKTF","journalArticle","2020","Sonntag, Daniel","A A EDITORIAL AI in Medicine, Covid-19 and Springer Nature's Open Access Agreement","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-020-00661-y","https://doi.org/10.1007/s13218-020-00661-y","","2020-06-01","2021-06-07 21:10:11","2021-06-15 09:18:16","2021-06-07 21:10:10","123-125","","2","34","","Künstl Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\9HKRZ7MY\Sonntag - 2020 - AI in Medicine, Covid-19 and Springer Nature's Ope.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XDHJP9FL","journalArticle","2019","Timpf, Sabine","A A EDITORIAL Categorisations: AI","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-019-00629-7","https://doi.org/10.1007/s13218-019-00629-7","","2019-12-01","2021-06-07 21:10:11","2021-06-15 09:22:49","2021-06-07 21:10:10","313-314","","4","33","","Künstl Intell","Categorisations","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\BWDD4EE6\Timpf - 2019 - Categorisations AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I86NPNJL","journalArticle","2020","Teh, Hui Yie; Kempa-Liehr, Andreas W.; Wang, Kevin I-Kai","Sensor data quality: a systematic review","Journal of Big Data","","2196-1115","10.1186/s40537-020-0285-1","https://doi.org/10.1186/s40537-020-0285-1","Sensor data quality plays a vital role in Internet of Things (IoT) applications as they are rendered useless if the data quality is bad. This systematic review aims to provide an introduction and guide for researchers who are interested in quality-related issues of physical sensor data. The process and results of the systematic review are presented which aims to answer the following research questions: what are the different types of physical sensor data errors, how to quantify or detect those errors, how to correct them and what domains are the solutions in. Out of 6970 literatures obtained from three databases (ACM Digital Library, IEEE Xplore and ScienceDirect) using the search string refined via topic modelling, 57 publications were selected and examined. Results show that the different types of sensor data errors addressed by those papers are mostly missing data and faults e.g. outliers, bias and drift. The most common solutions for error detection are based on principal component analysis (PCA) and artificial neural network (ANN) which accounts for about 40% of all error detection papers found in the study. Similarly, for fault correction, PCA and ANN are among the most common, along with Bayesian Networks. Missing values on the other hand, are mostly imputed using Association Rule Mining. Other techniques include hybrid solutions that combine several data science methods to detect and correct the errors. Through this systematic review, it is found that the methods proposed to solve physical sensor data errors cannot be directly compared due to the non-uniform evaluation process and the high use of non-publicly available datasets. Bayesian data analysis done on the 57 selected publications also suggests that publications using publicly available datasets for method evaluation have higher citation rates.","2020-02-11","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:10","11","","1","7","","J Big Data","Sensor data quality","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\3JURD2GD\Teh et al. - 2020 - Sensor data quality a systematic review.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EMRGXE66","journalArticle","2020","Kuwajima, Hiroshi; Yasuoka, Hirotoshi; Nakae, Toshihiro","Engineering problems in machine learning systems","Machine Learning","","1573-0565","10.1007/s10994-020-05872-w","https://doi.org/10.1007/s10994-020-05872-w","Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems that employ machine learning and deep learning models, such as automated driving vehicles. In order to use machine learning in a safety-critical system, it is necessary to demonstrate the safety and security of the system through engineering processes. However, thus far, no such widely accepted engineering concepts or frameworks have been established for these systems. The key to using a machine learning model in a deductively engineered system is decomposing the data-driven training of machine learning models into requirement, design, and verification, particularly for machine learning models used in safety-critical systems. Simultaneously, open problems and relevant technical fields are not organized in a manner that enables researchers to select a theme and work on it. In this study, we identify, classify, and explore the open problems in engineering (safety-critical) machine learning systems—that is, in terms of requirement, design, and verification of machine learning models and systems—as well as discuss related works and research directions, using automated driving vehicles as an example. Our results show that machine learning models are characterized by a lack of requirements specification, lack of design specification, lack of interpretability, and lack of robustness. We also perform a gap analysis on a conventional system quality standard SQuaRE with the characteristics of machine learning models to study quality models for machine learning systems. We find that a lack of requirements specification and lack of robustness have the greatest impact on conventional quality models.","2020-05-01","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:10","1103-1126","","5","109","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\XQANCSV7\Kuwajima et al. - 2020 - Engineering problems in machine learning systems.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DDFPD9UM","journalArticle","2019","Sridharan, Mohan; Meadows, Ben","Towards a Theory of Explanations for Human–Robot Collaboration","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-019-00616-y","https://doi.org/10.1007/s13218-019-00616-y","This paper makes two contributions towards enabling a robot to provide explanatory descriptions of its decisions, the underlying knowledge and beliefs, and the experiences that informed these beliefs. First, we present a theory of explanations comprising (i) claims about representing, reasoning with, and learning domain knowledge to support the construction of explanations; (ii) three fundamental axes to characterize explanations; and (iii) a methodology for constructing these explanations. Second, we describe an architecture for robots that implements this theory and supports scalability to complex domains and explanations. We demonstrate the architecture’s capabilities in the context of a simulated robot (a) moving target objects to desired locations or people; or (b) following recipes to bake biscuits.","2019-12-01","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:10","331-342","","4","33","","Künstl Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\7RSWCCPD\Sridharan and Meadows - 2019 - Towards a Theory of Explanations for Human–Robot C.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G3WXE6Q7","journalArticle","2020","Schmid, Ute; Finzel, Bettina","Mutual Explanations for Cooperative Decision Making in Medicine","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-020-00633-2","https://doi.org/10.1007/s13218-020-00633-2","Exploiting mutual explanations for interactive learning is presented as part of an interdisciplinary research project on transparent machine learning for medical decision support. Focus of the project is to combine deep learning black box approaches with interpretable machine learning for classification of different types of medical images to combine the predictive accuracy of deep learning and the transparency and comprehensibility of interpretable models. Specifically, we present an extension of the Inductive Logic Programming system Aleph to allow for interactive learning. Medical experts can ask for verbal explanations. They can correct classification decisions and in addition can also correct the explanations. Thereby, expert knowledge can be taken into account in form of constraints for model adaption.","2020-06-01","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:10","227-233","","2","34","","Künstl Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\PDJ26T22\Schmid and Finzel - 2020 - Mutual Explanations for Cooperative Decision Makin.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IWNERA86","journalArticle","2020","Wrede, Britta","A A EDITORIAL Explaining AI: Are We Ready For It?","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-020-00639-w","https://doi.org/10.1007/s13218-020-00639-w","","2020-03-01","2021-06-07 21:10:11","2021-06-17 14:03:00","2021-06-07 21:10:11","1-3","","1","34","","Künstl Intell","Explaining AI","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\P78ACL8E\Wrede - 2020 - Explaining AI Are We Ready For It.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S9WF2YZA","journalArticle","2020","Bertsimas, Dimitris; Van Parys, Bart","Sparse hierarchical regression with polynomials","Machine Learning","","1573-0565","10.1007/s10994-020-05868-6","https://doi.org/10.1007/s10994-020-05868-6","We present a novel method for sparse polynomial regression. We are interested in that degree r polynomial which depends on at most k inputs, counting at most $$\ell$$ monomial terms, and minimizes the sum of the squares of its prediction errors. Such highly structured sparse regression was denoted by Bach (Advances in neural information processing systems, pp 105–112, 2009) as sparse hierarchical regression in the context of kernel learning. Hierarchical sparse specification aligns well with modern big data settings where many inputs are not relevant for prediction purposes and the functional complexity of the regressor needs to be controlled as to avoid overfitting. We propose an efficient two-step approach to this hierarchical sparse regression problem. First, we discard irrelevant inputs using an extremely fast input ranking heuristic. Secondly, we take advantage of modern cutting plane methods for integer optimization to solve the remaining reduced hierarchical $$(k, \ell )$$-sparse problem exactly. The ability of our method to identify all k relevant inputs and all $$\ell$$ monomial terms is shown empirically to experience a phase transition. Crucially, the same transition also presents itself in our ability to reject all irrelevant features and monomials as well. In the regime where our method is statistically powerful, its computational complexity is interestingly on par with Lasso based heuristics. Hierarchical sparsity can retain the flexibility of general nonparametric methods such as nearest neighbors or regression trees (CART), without sacrificing much statistical power. The presented work hence fills a void in terms of a lack of powerful disciplined nonlinear sparse regression methods in high-dimensional settings. Our method is shown empirically to scale to regression problems with $$n\approx 10{,}000$$ observations for input dimension $$p\approx 1000$$.","2020-05-01","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:11","973-997","","5","109","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\CYCKJKW7\Bertsimas and Van Parys - 2020 - Sparse hierarchical regression with polynomials.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"566FZQJX","journalArticle","2019","Lubba, Carl H.; Sethi, Sarab S.; Knaute, Philip; Schultz, Simon R.; Fulcher, Ben D.; Jones, Nick S.","catch22: CAnonical Time-series CHaracteristics","Data Mining and Knowledge Discovery","","1573-756X","10.1007/s10618-019-00647-x","https://doi.org/10.1007/s10618-019-00647-x","Capturing the dynamical properties of time series concisely as interpretable feature vectors can enable efficient clustering and classification for time-series applications across science and industry. Selecting an appropriate feature-based representation of time series for a given application can be achieved through systematic comparison across a comprehensive time-series feature library, such as those in the hctsa toolbox. However, this approach is computationally expensive and involves evaluating many similar features, limiting the widespread adoption of feature-based representations of time series for real-world applications. In this work, we introduce a method to infer small sets of time-series features that (i) exhibit strong classification performance across a given collection of time-series problems, and (ii) are minimally redundant. Applying our method to a set of 93 time-series classification datasets (containing over 147,000 time series) and using a filtered version of the hctsa feature library (4791 features), we introduce a set of 22 CAnonical Time-series CHaracteristics, catch22, tailored to the dynamics typically encountered in time-series data-mining tasks. This dimensionality reduction, from 4791 to 22, is associated with an approximately 1000-fold reduction in computation time and near linear scaling with time-series length, despite an average reduction in classification accuracy of just 7%. catch22 captures a diverse and interpretable signature of time series in terms of their properties, including linear and non-linear autocorrelation, successive differences, value distributions and outliers, and fluctuation scaling properties. We provide an efficient implementation of catch22, accessible from many programming environments, that facilitates feature-based time-series analysis for scientific, industrial, financial and medical applications using a common language of interpretable time-series properties.","2019-11-01","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:11","1821-1852","","6","33","","Data Min Knowl Disc","catch22","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\RAH42VK7\Lubba et al. - 2019 - catch22 CAnonical Time-series CHaracteristics.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z8C9XIPI","journalArticle","2019","Boman, Magnus; Ben Abdesslem, Fehmi; Forsell, Erik; Gillblad, Daniel; Görnerup, Olof; Isacsson, Nils; Sahlgren, Magnus; Kaldo, Viktor","Learning machines in Internet-delivered psychological treatment","Progress in Artificial Intelligence","","2192-6360","10.1007/s13748-019-00192-0","https://doi.org/10.1007/s13748-019-00192-0","A learning machine, in the form of a gating network that governs a finite number of different machine learning methods, is described at the conceptual level with examples of concrete prediction subtasks. A historical data set with data from over 5000 patients in Internet-based psychological treatment will be used to equip healthcare staff with decision support for questions pertaining to ongoing and future cases in clinical care for depression, social anxiety, and panic disorder. The organizational knowledge graph is used to inform the weight adjustment of the gating network and for routing subtasks to the different methods employed locally for prediction. The result is an operational model for assisting therapists in their clinical work, about to be subjected to validation in a clinical trial.","2019-12-01","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:11","475-485","","4","8","","Prog Artif Intell","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\E5R5JC82\Boman et al. - 2019 - Learning machines in Internet-delivered psychologi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JTT3PS7","journalArticle","2020","Westera, Wim; Prada, Rui; Mascarenhas, Samuel; Santos, Pedro A.; Dias, João; Guimarães, Manuel; Georgiadis, Konstantinos; Nyamsuren, Enkhbold; Bahreini, Kiavash; Yumak, Zerrin; Christyowidiasmoro, Chris; Dascalu, Mihai; Gutu-Robu, Gabriel; Ruseti, Stefan","Artificial intelligence moving serious gaming: Presenting reusable game AI components","Education and Information Technologies","","1573-7608","10.1007/s10639-019-09968-2","https://doi.org/10.1007/s10639-019-09968-2","This article provides a comprehensive overview of artificial intelligence (AI) for serious games. Reporting about the work of a European flagship project on serious game technologies, it presents a set of advanced game AI components that enable pedagogical affordances and that can be easily reused across a wide diversity of game engines and game platforms. Serious game AI functionalities include player modelling (real-time facial emotion recognition, automated difficulty adaptation, stealth assessment), natural language processing (sentiment analysis and essay scoring on free texts), and believable non-playing characters (emotional and socio-cultural, non-verbal bodily motion, and lip-synchronised speech), respectively. The reuse of these components enables game developers to develop high quality serious games at reduced costs and in shorter periods of time. All these components are open source software and can be freely downloaded from the newly launched portal at gamecomponents.eu. The components come with detailed installation manuals and tutorial videos. All components have been applied and validated in serious games that were tested with real end-users.","2020-01-01","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:11","351-380","","1","25","","Educ Inf Technol","Artificial intelligence moving serious gaming","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\BP8BRMDX\Westera et al. - 2020 - Artificial intelligence moving serious gaming Pre.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4X5DQLWA","journalArticle","2020","Ochodek, Miroslaw; Hebig, Regina; Meding, Wilhelm; Frost, Gert; Staron, Miroslaw","Recognizing lines of code violating company-specific coding guidelines using machine learning","Empirical Software Engineering","","1573-7616","10.1007/s10664-019-09769-8","https://doi.org/10.1007/s10664-019-09769-8","Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.","2020-01-01","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:11","220-265","","1","25","","Empir Software Eng","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\YFZMLTVF\Ochodek et al. - 2020 - Recognizing lines of code violating company-specif.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G3LHQHKX","journalArticle","2020","Humm, Bernhard; Bense, Hermann; Bock, Jürgen; Classen, Mario; Halvani, Oren; Herta, Christian; Hoppe, Thomas; Juwig, Oliver; Siegel, Melanie","Applying machine intelligence in practice","Informatik Spektrum","","1432-122X","10.1007/s00287-020-01259-2","https://doi.org/10.1007/s00287-020-01259-2","The relevance of Machine Intelligence, a.k.a. Artificial Intelligence (AI), is undisputed at the present time. This is not only due to AI successes in research but, more prominently, its use in day-to-day practice. In 2014, we started a series of annual workshops at the Leibniz Zentrum für Informatik, Schloss Dagstuhl, Germany, initially focussing on Corporate Semantic Web and later widening the scope to Applied Machine Intelligence. This article presents a number of AI applications from various application domains, including medicine, industrial manufacturing and the insurance sector. Best practices, current trends, possibilities and limitations of new AI approaches for developing AI applications are also presented. Focus is put on the areas of natural language processing, ontologies and machine learning. The article concludes with a summary and outlook.","2020-04-01","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:11","137-144","","2","43","","Informatik Spektrum","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\BDS72RBY\Humm et al. - 2020 - Applying machine intelligence in practice.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"66CJ5P76","journalArticle","2019","Robbins, Scott","A A A Misdirected Principle with a Catch: Explicability for AI","Minds and Machines","","1572-8641","10.1007/s11023-019-09509-3","https://doi.org/10.1007/s11023-019-09509-3","There is widespread agreement that there should be a principle requiring that artificial intelligence (AI) be ‘explicable’. Microsoft, Google, the World Economic Forum, the draft AI ethics guidelines for the EU commission, etc. all include a principle for AI that falls under the umbrella of ‘explicability’. Roughly, the principle states that “for AI to promote and not constrain human autonomy, our ‘decision about who should decide’ must be informed by knowledge of how AI would act instead of us” (Floridi et al. in Minds Mach 28(4):689–707, 2018). There is a strong intuition that if an algorithm decides, for example, whether to give someone a loan, then that algorithm should be explicable. I argue here, however, that such a principle is misdirected. The property of requiring explicability should attach to a particular action or decision rather than the entity making that decision. It is the context and the potential harm resulting from decisions that drive the moral need for explicability—not the process by which decisions are reached. Related to this is the fact that AI is used for many low-risk purposes for which it would be unnecessary to require that it be explicable. A principle requiring explicability would prevent us from reaping the benefits of AI used in these situations. Finally, the explanations given by explicable AI are only fruitful if we already know which considerations are acceptable for the decision at hand. If we already have these considerations, then there is no need to use contemporary AI algorithms because standard automation would be available. In other words, a principle of explicability for AI makes the use of AI redundant.","2019-12-01","2021-06-07 21:10:11","2021-06-15 08:59:17","2021-06-07 21:10:11","495-514","","4","29","","Minds & Machines","A Misdirected Principle with a Catch","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\VVMUJIED\Robbins - 2019 - A Misdirected Principle with a Catch Explicabilit.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9DMQTTED","journalArticle","2019","Adnan, Kiran; Akbar, Rehan","An analytical study of information extraction from unstructured and multidimensional big data","Journal of Big Data","","2196-1115","10.1186/s40537-019-0254-8","https://doi.org/10.1186/s40537-019-0254-8","Process of information extraction (IE) is used to extract useful information from unstructured or semi-structured data. Big data arise new challenges for IE techniques with the rapid growth of multifaceted also called as multidimensional unstructured data. Traditional IE systems are inefficient to deal with this huge deluge of unstructured big data. The volume and variety of big data demand to improve the computational capabilities of these IE systems. It is necessary to understand the competency and limitations of the existing IE techniques related to data pre-processing, data extraction and transformation, and representations for huge volumes of multidimensional unstructured data. Numerous studies have been conducted on IE, addressing the challenges and issues for different data types such as text, image, audio and video. Very limited consolidated research work have been conducted to investigate the task-dependent and task-independent limitations of IE covering all data types in a single study. This research work address this limitation and present a systematic literature review of state-of-the-art techniques for a variety of big data, consolidating all data types. Recent challenges of IE are also identified and summarized. Potential solutions are proposed giving future research directions in big data IE. The research is significant in terms of recent trends and challenges related to big data analytics. The outcome of the research and recommendations will help to improve the big data analytics by making it more productive.","2019-10-17","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:11","91","","1","6","","J Big Data","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\SVG94MYJ\Adnan and Akbar - 2019 - An analytical study of information extraction from.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"665XIQ74","journalArticle","2020","Miller, Julian Francis","Cartesian genetic programming: its status and future","Genetic Programming and Evolvable Machines","","1573-7632","10.1007/s10710-019-09360-6","https://doi.org/10.1007/s10710-019-09360-6","Cartesian genetic programming, a well-established method of genetic programming, is approximately 20 years old. It represents solutions to computational problems as graphs. Its genetic encoding includes explicitly redundant genes which are well-known to assist in effective evolutionary search. In this article, we review and compare many of the important aspects of the method and findings discussed since its inception. In the process, we make many suggestions for further work which could improve the efficiency of the CGP for solving computational problems.","2020-06-01","2021-06-07 21:10:11","2021-06-07 21:10:11","2021-06-07 21:10:11","129-168","","1","21","","Genet Program Evolvable Mach","Cartesian genetic programming","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\FH9UUCUE\Miller - 2020 - Cartesian genetic programming its status and futu.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6NTB5E63","journalArticle","2018","Hu, Xia; Štiglic, Gregor; Wang, Fei","A A EDITORIAL Special Issue on Data Mining in Health Informatics","Journal of Healthcare Informatics Research","","2509-498X","10.1007/s41666-018-0039-4","https://doi.org/10.1007/s41666-018-0039-4","","2018-12-01","2021-06-07 21:10:40","2021-06-17 14:28:16","2021-06-07 21:10:38","367-369","","4","2","","J Healthc Inform Res","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\BA6MFLBX\Hu et al. - 2018 - Special Issue on Data Mining in Health Informatics.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5DJJ2TEK","journalArticle","2019","Tadist, Khawla; Najah, Said; Nikolov, Nikola S.; Mrabti, Fatiha; Zahi, Azeddine","Feature selection methods and genomic big data: a systematic review","Journal of Big Data","","2196-1115","10.1186/s40537-019-0241-0","https://doi.org/10.1186/s40537-019-0241-0","In the era of accelerating growth of genomic data, feature-selection techniques are believed to become a game changer that can help substantially reduce the complexity of the data, thus making it easier to analyze and translate it into useful information. It is expected that within the next decade, researchers will head towards analyzing the genomes of all living creatures making genomics the main generator of data. Feature selection techniques are believed to become a game changer that can help substantially reduce the complexity of genomic data, thus making it easier to analyze it and translating it into useful information. With the absence of a thorough investigation of the field, it is almost impossible for researchers to get an idea of how their work relates to existing studies as well as how it contributes to the research community. In this paper, we present a systematic and structured literature review of the feature-selection techniques used in studies related to big genomic data analytics.","2019-08-27","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:38","79","","1","6","","J Big Data","Feature selection methods and genomic big data","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\JKAGURVS\Tadist et al. - 2019 - Feature selection methods and genomic big data a .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S8HPXSNM","journalArticle","2019","Ismail, Ahmed; Truong, Hong-Linh; Kastner, Wolfgang","Manufacturing process data analysis pipelines: a requirements analysis and survey","Journal of Big Data","","2196-1115","10.1186/s40537-018-0162-3","https://doi.org/10.1186/s40537-018-0162-3","Smart manufacturing is strongly correlated with the digitization of all manufacturing activities. This increases the amount of data available to drive productivity and profit through data-driven decision making programs. The goal of this article is to assist data engineers in designing big data analysis pipelines for manufacturing process data. Thus, this paper characterizes the requirements for process data analysis pipelines and surveys existing platforms from academic literature. The results demonstrate a stronger focus on the storage and analysis phases of pipelines than on the ingestion, communication, and visualization stages. Results also show a tendency towards custom tools for ingestion and visualization, and relational data tools for storage and analysis. Tools for handling heterogeneous data are generally well-represented throughout the pipeline. Finally, batch processing tools are more widely adopted than real-time stream processing frameworks, and most pipelines opt for a common script-based data processing approach. Based on these results, recommendations are offered for each phase of the pipeline.","2019-01-07","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:38","1","","1","6","","J Big Data","Manufacturing process data analysis pipelines","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\BYSH885C\Ismail et al. - 2019 - Manufacturing process data analysis pipelines a r.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W5N6ZN4A","journalArticle","2019","Turhan, Anni-Yasmin","Bridging the Prototype Gap: On the Evolution of Ugly Ducklings","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-019-00604-2","https://doi.org/10.1007/s13218-019-00604-2","","2019-09-01","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:38","205-207","","3","33","","Künstl Intell","Bridging the Prototype Gap","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\YBLSJIE2\Turhan - 2019 - Bridging the Prototype Gap On the Evolution of Ug.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JPD745JM","journalArticle","2019","Legg, Phil; Smith, Jim; Downing, Alexander","Visual analytics for collaborative human-machine confidence in human-centric active learning tasks","Human-centric Computing and Information Sciences","","2192-1962","10.1186/s13673-019-0167-8","https://doi.org/10.1186/s13673-019-0167-8","Active machine learning is a human-centric paradigm that leverages a small labelled dataset to build an initial weak classifier, that can then be improved over time through human-machine collaboration. As new unlabelled samples are observed, the machine can either provide a prediction, or query a human ‘oracle’ when the machine is not confident in its prediction. Of course, just as the machine may lack confidence, the same can also be true of a human ‘oracle’: humans are not all-knowing, untiring oracles. A human’s ability to provide an accurate and confident response will often vary between queries, according to the duration of the current interaction, their level of engagement with the system, and the difficulty of the labelling task. This poses an important question of how uncertainty can be expressed and accounted for in a human-machine collaboration. In short, how can we facilitate a mutually-transparent collaboration between two uncertain actors—a person and a machine—that leads to an improved outcome? In this work, we demonstrate the benefit of human-machine collaboration within the process of active learning, where limited data samples are available or where labelling costs are high. To achieve this, we developed a visual analytics tool for active learning that promotes transparency, inspection, understanding and trust, of the learning process through human-machine collaboration. Fundamental to the notion of confidence, both parties can report their level of confidence during active learning tasks using the tool, such that this can be used to inform learning. Human confidence of labels can be accounted for by the machine, the machine can query for samples based on confidence measures, and the machine can report confidence of current predictions to the human, to further the trust and transparency between the collaborative parties. In particular, we find that this can improve the robustness of the classifier when incorrect sample labels are provided, due to unconfidence or fatigue. Reported confidences can also better inform human-machine sample selection in collaborative sampling. Our experimentation compares the impact of different selection strategies for acquiring samples: machine-driven, human-driven, and collaborative selection. We demonstrate how a collaborative approach can improve trust in the model robustness, achieving high accuracy and low user correction, with only limited data sample selections.","2019-02-14","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","5","","1","9","","Hum. Cent. Comput. Inf. Sci.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\6JRNMUN6\Legg et al. - 2019 - Visual analytics for collaborative human-machine c.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UDDVUG6K","journalArticle","2018","Floridi, Luciano; Cowls, Josh; Beltrametti, Monica; Chatila, Raja; Chazerand, Patrice; Dignum, Virginia; Luetge, Christoph; Madelin, Robert; Pagallo, Ugo; Rossi, Francesca; Schafer, Burkhard; Valcke, Peggy; Vayena, Effy","AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations","Minds and Machines","","1572-8641","10.1007/s11023-018-9482-5","https://doi.org/10.1007/s11023-018-9482-5","This article reports the findings of AI4People, an Atomium—EISMD initiative designed to lay the foundations for a “Good AI Society”. We introduce the core opportunities and risks of AI for society; present a synthesis of five ethical principles that should undergird its development and adoption; and offer 20 concrete recommendations—to assess, to develop, to incentivise, and to support good AI—which in some cases may be undertaken directly by national or supranational policy makers, while in others may be led by other stakeholders. If adopted, these recommendations would serve as a firm foundation for the establishment of a Good AI Society.","2018-12-01","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","689-707","","4","28","","Minds & Machines","AI4People—An Ethical Framework for a Good AI Society","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\76RTKP3T\Floridi et al. - 2018 - AI4People—An Ethical Framework for a Good AI Socie.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EUDM8RY7","journalArticle","2019","Schweizer, Paul","Triviality Arguments Reconsidered","Minds and Machines","","1572-8641","10.1007/s11023-019-09501-x","https://doi.org/10.1007/s11023-019-09501-x","Opponents of the computational theory of mind (CTM) have held that the theory is devoid of explanatory content, since whatever computational procedures are said to account for our cognitive attributes will also be realized by a host of other ‘deviant’ physical systems, such as buckets of water and possibly even stones. Such ‘triviality’ claims rely on a simple mapping account (SMA) of physical implementation. Hence defenders of CTM traditionally attempt to block the trivialization critique by advocating additional constraints on the implementation relation. However, instead of attempting to ‘save’ CTM by constraining the account of physical implementation, I argue that the general form of the triviality argument is invalid. I provide a counterexample scenario, and show that SMA is in fact consistent with empirically rich and theoretically plausible versions of CTM. This move requires rejection of the computational sufficiency thesis, which I argue is scientifically unjustified in any case. By shifting the ‘burden of explanatory force’ away from the concept of physical implementation, and instead placing it on salient aspects of the target phenomenon to be explained, it’s possible to retain a maximally liberal and unfettered view of physical implementation, and at the same time defuse the triviality arguments that have motivated defenders of CTM to impose various theory-laden constraints on SMA.","2019-06-01","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","287-308","","2","29","","Minds & Machines","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\625ZY674\Schweizer - 2019 - Triviality Arguments Reconsidered.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGZS8IVN","journalArticle","2019","Ionescu, Tudor B.","Simulation, Epistemic Opacity, and ‘Envirotechnical Ignorance’ in Nuclear Crisis","Minds and Machines","","1572-8641","10.1007/s11023-018-9488-z","https://doi.org/10.1007/s11023-018-9488-z","The Fukushima nuclear accident from 2011 provided an occasion for the public display of radiation maps (or dose projections) generated using decision-support systems for nuclear emergency management. Such systems rely on computer models for simulating the atmospheric dispersion of radioactive materials and estimating potential doses in the event of a radioactive release from a nuclear reactor. In Germany, as in Japan, such systems are part of the national emergency response apparatus and, in case of accidents, they can be used by emergency task forces for planning radioprotection countermeasures. In this context, the paper addresses the epistemology of dose projections by critically analyzing some of the sources of epistemic opacity and non-knowledge (or ignorance) affecting them, and the different methods and practices used by German radioprotection experts to improve their trustworthiness and reliability. It will be argued that dose projections are part of an entire radioprotection regime or assemblage built around the belief that the effects of nuclear accidents can be effectively mitigated thanks to the simulation technologies underlying different protocols and practices of nuclear preparedness. And, as the Fukushima experience showed, some of these expectations will not be met in real emergencies due to the inherent uncertainties entailed by the use of dose projections when planning protective countermeasures.","2019-03-01","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","61-86","","1","29","","Minds & Machines","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\T89G2RDY\Ionescu - 2019 - Simulation, Epistemic Opacity, and ‘Envirotechnica.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"835AP4SW","journalArticle","2019","Shih, Shun-Yao; Sun, Fan-Keng; Lee, Hung-yi","Temporal pattern attention for multivariate time series forecasting","Machine Learning","","1573-0565","10.1007/s10994-019-05815-0","https://doi.org/10.1007/s10994-019-05815-0","Forecasting of multivariate time series data, for instance the prediction of electricity consumption, solar power production, and polyphonic piano pieces, has numerous valuable applications. However, complex and non-linear interdependencies between time steps and series complicate this task. To obtain accurate prediction, it is crucial to model long-term dependency in time series data, which can be achieved by recurrent neural networks (RNNs) with an attention mechanism. The typical attention mechanism reviews the information at each previous time step and selects relevant information to help generate the outputs; however, it fails to capture temporal patterns across multiple time steps. In this paper, we propose using a set of filters to extract time-invariant temporal patterns, similar to transforming time series data into its “frequency domain”. Then we propose a novel attention mechanism to select relevant time series, and use its frequency domain information for multivariate forecasting. We apply the proposed model on several real-world tasks and achieve state-of-the-art performance in almost all of cases. Our source code is available at https://github.com/gantheory/TPA-LSTM.","2019-09-01","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","1421-1441","","8","108","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\58BW7LVF\Shih et al. - 2019 - Temporal pattern attention for multivariate time s.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZHW9NNIG","journalArticle","2019","Wong, Kelvin Kian Loong; Liu, Zhihua; Zou, Quan","Multi-objective optimization and data analysis in informationization","Computing","","1436-5057","10.1007/s00607-019-00718-3","https://doi.org/10.1007/s00607-019-00718-3","","2019-06-01","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","495-498","","6","101","","Computing","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\G4LLI2A9\Wong et al. - 2019 - Multi-objective optimization and data analysis in .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VWKY859A","journalArticle","2019","Carsten, Oliver; Martens, Marieke H.","How can humans understand their automated cars? HMI principles, problems and solutions","Cognition, Technology & Work","","1435-5566","10.1007/s10111-018-0484-0","https://doi.org/10.1007/s10111-018-0484-0","As long as vehicles do not provide full automation, the design and function of the Human Machine Interface (HMI) is crucial for ensuring that the human “driver” and the vehicle-based automated systems collaborate in a safe manner. When the driver is decoupled from active control, the design of the HMI becomes even more critical. Without mutual understanding, the two agents (human and vehicle) will fail to accurately comprehend each other’s intentions and actions. This paper proposes a set of design principles for in-vehicle HMI and reviews some current HMI designs in the light of those principles. We argue that in many respects, the current designs fall short of best practice and have the potential to confuse the driver. This can lead to a mismatch between the operation of the automation in the light of the current external situation and the driver’s awareness of how well the automation is currently handling that situation. A model to illustrate how the various principles are interrelated is proposed. Finally, recommendations are made on how, building on each principle, HMI design solutions can be adopted to address these challenges.","2019-02-01","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","3-20","","1","21","","Cogn Tech Work","How can humans understand their automated cars?","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\AATNCHVD\Carsten and Martens - 2019 - How can humans understand their automated cars HM.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SMQHD25Y","journalArticle","2019","Manogaran, Gunasekaran; Chilamkurti, Naveen; Hsu, Ching-Hsien","A A EDITORIAL Emerging intelligent algorithms: challenges and applications","Neural Computing and Applications","","1433-3058","10.1007/s00521-018-3930-2","https://doi.org/10.1007/s00521-018-3930-2","","2019-05-01","2021-06-07 21:10:40","2021-06-17 14:02:26","2021-06-07 21:10:39","1259-1262","","5","31","","Neural Comput & Applic","Emerging intelligent algorithms","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\RFN2JVYD\Manogaran et al. - 2019 - Emerging intelligent algorithms challenges and ap.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MAJU4YMF","journalArticle","2019","Khanam, Shirin Akther; Liu, Fei; Chen, Yi-Ping Phoebe","Comprehensive structured knowledge base system construction with natural language presentation","Human-centric Computing and Information Sciences","","2192-1962","10.1186/s13673-019-0184-7","https://doi.org/10.1186/s13673-019-0184-7","Constructing an ontology-based machine-readable knowledge base system from different sources with minimum human intervention, also known as ontology-based machine-readable knowledge base construction (OMRKBC), has been a long-term outstanding problem. One of the issues is how to build a large-scale OMRKBC process with appropriate structural information. To address this issue, we propose Natural Language Independent Knowledge Representation (NLIKR), a method which regards each word as a concept which should be defined by its relations with other concepts. Using NLIKR, we propose a framework for the OMRKBC process to automatically develop a comprehensive ontology-based machine-readable knowledge base system (OMRKBS) using well-built structural information. Firstly, as part of this framework, we propose formulas to discover concepts and their relations in the OMRKBS. Secondly, the challenges in obtaining rich structured information are resolved through the development of algorithms and rules. Finally, rich structured information is built in the OMRKBS. OMRKBC allows the efficient search of words and supports word queries with a specific attribute. We conduct experiments and analyze the results of relational information extraction, with the results showing that OMRKBS had an accuracy of 84% which was higher than the other knowledge base systems, namely ConceptNet, DBpedia and WordNet.","2019-06-10","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","23","","1","9","","Hum. Cent. Comput. Inf. Sci.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\YWVQ28SV\Khanam et al. - 2019 - Comprehensive structured knowledge base system con.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9S7NSLZG","journalArticle","2019","Ding, Nan; Xu, Shiming; Song, Zhenya; Zhang, Baoquan; Li, Jingmei; Zheng, Zhigao","Using hardware counter-based performance model to diagnose scaling issues of HPC applications","Neural Computing and Applications","","1433-3058","10.1007/s00521-018-3496-z","https://doi.org/10.1007/s00521-018-3496-z","Performance diagnosing for HPC applications can be extremely difficult due to their complicated performance behaviors. One hand, developers used to identify the potential performance bottlenecks by conducting detailed instrumentation, which may introduce significant performance overheads or even performance deviations. On the other hand, developers can only conduct small numbers of application runs for profiling the performance with the limitations on both computing resources and time duration. Meanwhile, the performance bottlenecks of HPC applications may vary with the degree of parallelism. To address these challenges, our paper proposes a systematic performance diagnosing method focusing on building an accurate and interpretable performance model with performance counters. Our method is able to diagnose the HPC application scaling issues by predicting its runtime and performance behaviors in different functions. After applying this modeling method on three real-world HPC applications, HOMME, CICE and OpenFoam, our evaluations show that our diagnosing method based on the performance model has the ability to diagnose the potential scaling issues, which is typically missed by the traditional performance diagnosing method and achieves about 10% prediction errors in a scale of 4096 MPI ranks on two problem sizes.","2019-05-01","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","1563-1575","","5","31","","Neural Comput & Applic","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\TYDN3CZV\Ding et al. - 2019 - Using hardware counter-based performance model to .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4N9NCUE8","journalArticle","2019","Holzinger, Andreas; Plass, Markus; Kickmeier-Rust, Michael; Holzinger, Katharina; Crişan, Gloria Cerasela; Pintea, Camelia-M.; Palade, Vasile","Interactive machine learning: experimental evidence for the human in the algorithmic loop","Applied Intelligence","","1573-7497","10.1007/s10489-018-1361-5","https://doi.org/10.1007/s10489-018-1361-5","Recent advances in automatic machine learning (aML) allow solving problems without any human intervention. However, sometimes a human-in-the-loop can be beneficial in solving computationally hard problems. In this paper we provide new experimental insights on how we can improve computational intelligence by complementing it with human intelligence in an interactive machine learning approach (iML). For this purpose, we used the Ant Colony Optimization (ACO) framework, because this fosters multi-agent approaches with human agents in the loop. We propose unification between the human intelligence and interaction skills and the computational power of an artificial system. The ACO framework is used on a case study solving the Traveling Salesman Problem, because of its many practical implications, e.g. in the medical domain. We used ACO due to the fact that it is one of the best algorithms used in many applied intelligence problems. For the evaluation we used gamification, i.e. we implemented a snake-like game called Traveling Snakesman with the MAX–MIN Ant System (MMAS) in the background. We extended the MMAS–Algorithm in a way, that the human can directly interact and influence the ants. This is done by “traveling” with the snake across the graph. Each time the human travels over an ant, the current pheromone value of the edge is multiplied by 5. This manipulation has an impact on the ant’s behavior (the probability that this edge is taken by the ant increases). The results show that the humans performing one tour through the graphs have a significant impact on the shortest path found by the MMAS. Consequently, our experiment demonstrates that in our case human intelligence can positively influence machine intelligence. To the best of our knowledge this is the first study of this kind.","2019-07-01","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","2401-2414","","7","49","","Appl Intell","Interactive machine learning","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\NL4RR4DP\Holzinger et al. - 2019 - Interactive machine learning experimental evidenc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7WTBJJCI","journalArticle","2019","Ozaki, Tomonobu; Goebel, Randy; Inoue, Katsumi","From Fifth Generation Computing to Skill Science","New Generation Computing","","1882-7055","10.1007/s00354-019-00058-y","https://doi.org/10.1007/s00354-019-00058-y","Professor Koichi Furukawa, an eminent computer scientist and former Editor-in-Chief of the New Generation Computing journal, passed away on January 31, 2017. His passing was a surprise, and we were all shocked and saddened by the news. To remember the deceased, this article reviews the great career and contributions of Professor Koichi Furukawa, focusing on his research activities on the foundation and application of logic programming. Professor Furukawa had both a deep understanding and broad impact on logic programming, and he was always gentle but persistent in articulating its value across a broad spectrum of computer science and artificial intelligence research. This article introduces his research along with its insightful and unique philosophical framework.","2019-04-01","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","141-158","","2","37","","New Gener. Comput.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\KRCCRIKM\Ozaki et al. - 2019 - From Fifth Generation Computing to Skill Science.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S7LLDT6W","journalArticle","2019","Altuncu, M. Tarik; Mayer, Erik; Yaliraki, Sophia N.; Barahona, Mauricio","From free text to clusters of content in health records: an unsupervised graph partitioning approach","Applied Network Science","","2364-8228","10.1007/s41109-018-0109-9","https://doi.org/10.1007/s41109-018-0109-9","Electronic healthcare records contain large volumes of unstructured data in different forms. Free text constitutes a large portion of such data, yet this source of richly detailed information often remains under-used in practice because of a lack of suitable methodologies to extract interpretable content in a timely manner. Here we apply network-theoretical tools to the analysis of free text in Hospital Patient Incident reports in the English National Health Service, to find clusters of reports in an unsupervised manner and at different levels of resolution based directly on the free text descriptions contained within them. To do so, we combine recently developed deep neural network text-embedding methodologies based on paragraph vectors with multi-scale Markov Stability community detection applied to a similarity graph of documents obtained from sparsified text vector similarities. We showcase the approach with the analysis of incident reports submitted in Imperial College Healthcare NHS Trust, London. The multiscale community structure reveals levels of meaning with different resolution in the topics of the dataset, as shown by relevant descriptive terms extracted from the groups of records, as well as by comparing a posteriori against hand-coded categories assigned by healthcare personnel. Our content communities exhibit good correspondence with well-defined hand-coded categories, yet our results also provide further medical detail in certain areas as well as revealing complementary descriptors of incidents beyond the external classification. We also discuss how the method can be used to monitor reports over time and across different healthcare providers, and to detect emerging trends that fall outside of pre-existing categories.","2019-01-24","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","2","","1","4","","Appl Netw Sci","From free text to clusters of content in health records","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\BHY3EFU3\Altuncu et al. - 2019 - From free text to clusters of content in health re.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G64N3H9S","journalArticle","2019","Hariri, Reihaneh H.; Fredericks, Erik M.; Bowers, Kate M.","Uncertainty in big data analytics: survey, opportunities, and challenges","Journal of Big Data","","2196-1115","10.1186/s40537-019-0206-3","https://doi.org/10.1186/s40537-019-0206-3","Big data analytics has gained wide attention from both academia and industry as the demand for understanding trends in massive datasets increases. Recent developments in sensor networks, cyber-physical systems, and the ubiquity of the Internet of Things (IoT) have increased the collection of data (including health care, social media, smart cities, agriculture, finance, education, and more) to an enormous scale. However, the data collected from sensors, social media, financial records, etc. is inherently uncertain due to noise, incompleteness, and inconsistency. The analysis of such massive amounts of data requires advanced analytical techniques for efficiently reviewing and/or predicting future courses of action with high precision and advanced decision-making strategies. As the amount, variety, and speed of data increases, so too does the uncertainty inherent within, leading to a lack of confidence in the resulting analytics process and decisions made thereof. In comparison to traditional data techniques and platforms, artificial intelligence techniques (including machine learning, natural language processing, and computational intelligence) provide more accurate, faster, and scalable results in big data analytics. Previous research and surveys conducted on big data analytics tend to focus on one or two techniques or specific application domains. However, little work has been done in the field of uncertainty when applied to big data analytics as well as in the artificial intelligence techniques applied to the datasets. This article reviews previous work in big data analytics and presents a discussion of open challenges and future directions for recognizing and mitigating uncertainty in this domain.","2019-06-04","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","44","","1","6","","J Big Data","Uncertainty in big data analytics","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\7DUFLZZJ\Hariri et al. - 2019 - Uncertainty in big data analytics survey, opportu.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KRQKQJDB","journalArticle","2019","Dumbacher, Brian; Morris, Darcy Steeg; Hogue, Carma","Using electronic transaction data to add geographic granularity to official estimates of retail sales","Journal of Big Data","","2196-1115","10.1186/s40537-019-0242-z","https://doi.org/10.1186/s40537-019-0242-z","Economists are interested in more granular, more frequent data to aid in their understanding of the U.S. economy. The most frequent economic data currently available from the U.S. Census Bureau come from monthly economic indicators such as the Monthly Retail Trade Survey, which produces national estimates of retail sales. On the other hand, the most granular data (in terms of geographic and industry detail) come from the Economic Census, which is conducted every five years. The Census Bureau is researching whether organic, third-party Big Data sources, in conjunction with survey data, allow for the production of retail sales estimates that are both monthly and subnational.","2019-08-29","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:39","80","","1","6","","J Big Data","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\57Z4ZJUN\Dumbacher et al. - 2019 - Using electronic transaction data to add geographi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LNTRQJJC","journalArticle","2019","Krempl, G.; Lang, D.; Hofer, V.","Temporal density extrapolation using a dynamic basis approach","Data Mining and Knowledge Discovery","","1573-756X","10.1007/s10618-019-00636-0","https://doi.org/10.1007/s10618-019-00636-0","Density estimation is a versatile technique underlying many data mining tasks and techniques, ranging from exploration and presentation of static data, to probabilistic classification, or identifying changes or irregularities in streaming data. With the pervasiveness of embedded systems and digitisation, this latter type of streaming and evolving data becomes more important. Nevertheless, research in density estimation has so far focused on stationary data, leaving the task of of extrapolating and predicting density at time points outside a training window an open problem. For this task, temporal density extrapolation (TDX) is proposed. This novel method models and predicts gradual monotonous changes in a distribution. It is based on the expansion of basis functions, whose weights are modelled as functions of compositional data over time by using an isometric log-ratio transformation. Extrapolated density estimates are then obtained by extrapolating the weights to the requested time point, and querying the density from the basis functions with back-transformed weights. Our approach aims for broad applicability by neither being restricted to a specific parametric distribution, nor relying on cluster structure in the data. It requires only two additional extrapolation-specific parameters, for which reasonable defaults exist. Experimental evaluation on various data streams, synthetic as well as from the real-world domains of credit scoring and environmental health, shows that the model manages to capture monotonous drift patterns accurately and better than existing methods. Thereby, it requires not more than 1.5 times the run time of a corresponding static density estimation approach.","2019-09-01","2021-06-07 21:10:40","2021-06-07 21:10:40","2021-06-07 21:10:40","1323-1356","","5","33","","Data Min Knowl Disc","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\X3BEILEL\Krempl et al. - 2019 - Temporal density extrapolation using a dynamic bas.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9Q43SY52","journalArticle","2018","Matta, John; Zhao, Junya; Ercal, Gunes; Obafemi-Ajayi, Tayo","Applications of node-based resilience graph theoretic framework to clustering autism spectrum disorders phenotypes","Applied Network Science","","2364-8228","10.1007/s41109-018-0093-0","https://doi.org/10.1007/s41109-018-0093-0","With the growing ubiquity of data in network form, clustering in the context of a network, represented as a graph, has become increasingly important. Clustering is a very useful data exploratory machine learning tool that allows us to make better sense of heterogeneous data by grouping data with similar attributes based on some criteria. This paper investigates the application of a novel graph theoretic clustering method, Node-Based Resilience clustering (NBR-Clust), to address the heterogeneity of Autism Spectrum Disorder (ASD) and identify meaningful subgroups. The hypothesis is that analysis of these subgroups would reveal relevant biomarkers that would provide a better understanding of ASD phenotypic heterogeneity useful for further ASD studies. We address appropriate graph constructions suited for representing the ASD phenotype data. The sample population is drawn from a very large rigorous dataset: Simons Simplex Collection (SSC). Analysis of the results performed using graph quality measures, internal cluster validation measures, and clinical analysis outcome demonstrate the potential usefulness of resilience measure clustering for biomedical datasets. We also conduct feature extraction analysis to characterize relevant biomarkers that delineate the resulting subgroups. The optimal results obtained favored predominantly a 5-cluster configuration.","2018-08-29","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:03","38","","1","3","","Appl Netw Sci","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\8RLFWFVA\Matta et al. - 2018 - Applications of node-based resilience graph theore.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3WQMT58Q","journalArticle","2017","Patelli, Alina; Lewis, Peter R.; Ekart, Aniko; Wang, Hai; Nabney, Ian; Bennett, David; Lucas, Ralph; Cole, Alex","An architecture for the autonomic curation of crowdsourced knowledge","Cluster Computing","","1573-7543","10.1007/s10586-017-0908-2","https://doi.org/10.1007/s10586-017-0908-2","Human knowledge curators are intrinsically better than their digital counterparts at providing relevant answers to queries. That is mainly due to the fact that an experienced biological brain will account for relevant community expertise as well as exploit the underlying connections between knowledge pieces when offering suggestions pertinent to a specific question, whereas most automated database managers will not. We address this problem by proposing an architecture for the autonomic curation of crowdsourced knowledge, that is underpinned by semantic technologies. The architecture is instantiated in the career data domain, thus yielding Aviator, a collaborative platform capable of producing complete, intuitive and relevant answers to career related queries, in a time effective manner. In addition to providing numeric and use case based evidence to support these research claims, this extended work also contains a detailed architectural analysis of Aviator to outline its suitability for automatically curating knowledge to a high standard of quality.","2017-09-01","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:03","2031-2046","","3","20","","Cluster Comput","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\7LG3WB95\Patelli et al. - 2017 - An architecture for the autonomic curation of crow.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R632GDJN","journalArticle","2017","Bertsimas, Dimitris; Dunn, Jack","A A Optimal classification trees","Machine Learning","","1573-0565","10.1007/s10994-017-5633-9","https://doi.org/10.1007/s10994-017-5633-9","State-of-the-art decision tree methods apply heuristics recursively to create each split in isolation, which may not capture well the underlying characteristics of the dataset. The optimal decision tree problem attempts to resolve this by creating the entire decision tree at once to achieve global optimality. In the last 25 years, algorithmic advances in integer optimization coupled with hardware improvements have resulted in an astonishing 800 billion factor speedup in mixed-integer optimization (MIO). Motivated by this speedup, we present optimal classification trees, a novel formulation of the decision tree problem using modern MIO techniques that yields the optimal decision tree for axes-aligned splits. We also show the richness of this MIO formulation by adapting it to give optimal classification trees with hyperplanes that generates optimal decision trees with multivariate splits. Synthetic tests demonstrate that these methods recover the true decision tree more closely than heuristics, refuting the notion that optimal methods overfit the training data. We comprehensively benchmark these methods on a sample of 53 datasets from the UCI machine learning repository. We establish that these MIO methods are practically solvable on real-world datasets with sizes in the 1000s, and give average absolute improvements in out-of-sample accuracy over CART of 1–2 and 3–5% for the univariate and multivariate cases, respectively. Furthermore, we identify that optimal classification trees are likely to outperform CART by 1.2–1.3% in situations where the CART accuracy is high and we have sufficient training data, while the multivariate version outperforms CART by 4–7% when the CART accuracy or dimension of the dataset is low.","2017-07-01","2021-06-07 21:11:05","2021-06-15 09:30:20","2021-06-07 21:11:04","1039-1082","","7","106","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\35CTBLQX\Bertsimas and Dunn - 2017 - Optimal classification trees.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XYZQ2VNL","journalArticle","2018","An, Chuankai; O’Malley, A. James; Rockmore, Daniel N.","Referral paths in the U.S. physician network","Applied Network Science","","2364-8228","10.1007/s41109-018-0081-4","https://doi.org/10.1007/s41109-018-0081-4","In this paper, we analyze the millions of referral paths of patients’ interactions with the healthcare system for each year in the 2006-2011 time period and relate them to U.S. cardiovascular treatment records. For a patient, a “referral path” records the chronological sequence of physicians encountered by a patient (subject to certain constraints on the times between encounters). It provides a basic unit of analysis in a broader referral network that encodes the flow of patients and information between physicians in a healthcare system. We consider referral networks defined over a range of interactions as well as the characteristics of referral paths, producing a characterization of the various networks as well as the physicians they comprise. We further relate these metrics and findings to outcomes in the specific area of cardiovascular care. In particular, we match a referral path to occurrences of Acute Myocardial Infarction (AMI) and use the summary measures of the referral path to predict the treatment a patient receives and medical outcomes following treatment. Some referral path features are more significant with respect to their ability to boost a tree-based predictive model, and have stronger correlations with numerical treatment outcome variables. The patterns of referral paths and the derived informative features illustrate the potential for using network science to optimize patient referrals in healthcare systems for improved treatment outcomes and more efficient utilization of medical resources.","2018-07-31","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:04","20","","1","3","","Appl Netw Sci","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\39FCA7MA\An et al. - 2018 - Referral paths in the U.S. physician network.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJSXUGH2","journalArticle","2018","Pan, Yun-he","A A EDITORIAL 2018 special issue on artificial intelligence 2.0: theories and applications","Frontiers of Information Technology & Electronic Engineering","","2095-9230","10.1631/FITEE.1810000","https://doi.org/10.1631/FITEE.1810000","","2018-01-01","2021-06-07 21:11:05","2021-06-15 09:25:46","2021-06-07 21:11:04","1-2","","1","19","","Frontiers Inf Technol Electronic Eng","2018 special issue on artificial intelligence 2.0","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\WGVDVS3U\Pan - 2018 - 2018 special issue on artificial intelligence 2.0.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LUN93FKH","journalArticle","2018","Huisman, Sjoerd M. H.; Mahfouz, Ahmed; Batmanghelich, Nematollah K.; Lelieveldt, Boudewijn P. F.; Reinders, Marcel J. T.; for the Alzheimer’s Disease Neuroimaging Initiative","A structural equation model for imaging genetics using spatial transcriptomics","Brain Informatics","","2198-4026","10.1186/s40708-018-0091-0","https://doi.org/10.1186/s40708-018-0091-0","Imaging genetics deals with relationships between genetic variation and imaging variables, often in a disease context. The complex relationships between brain volumes and genetic variants have been explored with both dimension reduction methods and model-based approaches. However, these models usually do not make use of the extensive knowledge of the spatio-anatomical patterns of gene activity. We present a method for integrating genetic markers (single nucleotide polymorphisms) and imaging features, which is based on a causal model and, at the same time, uses the power of dimension reduction. We use structural equation models to find latent variables that explain brain volume changes in a disease context, and which are in turn affected by genetic variants. We make use of publicly available spatial transcriptome data from the Allen Human Brain Atlas to specify the model structure, which reduces noise and improves interpretability. The model is tested in a simulation setting and applied on a case study of the Alzheimer’s Disease Neuroimaging Initiative.","2018-11-02","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:04","13","","2","5","","Brain Inf.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\B8Q4S3NA\Huisman et al. - 2018 - A structural equation model for imaging genetics u.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KGDLZRVJ","journalArticle","2018","Tandle, Avinash L.; Joshi, Manjusha S.; Dharmadhikari, Ambrish S.; Jaiswal, Suyog V.","Mental state and emotion detection from musically stimulated EEG","Brain Informatics","","2198-4026","10.1186/s40708-018-0092-z","https://doi.org/10.1186/s40708-018-0092-z","This literature survey attempts to clarify different approaches considered to study the impact of the musical stimulus on the human brain using EEG Modality. Glancing at the field through various aspects of such studies specifically an experimental protocol, the EEG machine, number of channels investigated, feature extracted, categories of emotions, the brain area, the brainwaves, statistical tests, machine learning algorithms used for classification and validation of the developed model. This article comments on how these different approaches have particular weaknesses and strengths. Ultimately, this review concludes a suitable method to study the impact of the musical stimulus on brain and implications of such kind of studies.","2018-11-29","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:04","14","","2","5","","Brain Inf.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\5KKHR8MD\Tandle et al. - 2018 - Mental state and emotion detection from musically .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NANLH2Q2","journalArticle","2018","Beykikhoshk, Adham; Arandjelović, Ognjen; Phung, Dinh; Venkatesh, Svetha","Discovering topic structures of a temporally evolving document corpus","Knowledge and Information Systems","","0219-3116","10.1007/s10115-017-1095-4","https://doi.org/10.1007/s10115-017-1095-4","In this paper we describe a novel framework for the discovery of the topical content of a data corpus, and the tracking of its complex structural changes across the temporal dimension. In contrast to previous work our model does not impose a prior on the rate at which documents are added to the corpus nor does it adopt the Markovian assumption which overly restricts the type of changes that the model can capture. Our key technical contribution is a framework based on (i) discretization of time into epochs, (ii) epoch-wise topic discovery using a hierarchical Dirichlet process-based model, and (iii) a temporal similarity graph which allows for the modelling of complex topic changes: emergence and disappearance, evolution, splitting, and merging. The power of the proposed framework is demonstrated on two medical literature corpora concerned with the autism spectrum disorder (ASD) and the metabolic syndrome (MetS)—both increasingly important research subjects with significant social and healthcare consequences. In addition to the collected ASD and metabolic syndrome literature corpora which we made freely available, our contribution also includes an extensive empirical analysis of the proposed framework. We describe a detailed and careful examination of the effects that our algorithms’s free parameters have on its output and discuss the significance of the findings both in the context of the practical application of our algorithm as well as in the context of the existing body of work on temporal topic analysis. Our quantitative analysis is followed by several qualitative case studies highly relevant to the current research on ASD and MetS, on which our algorithm is shown to capture well the actual developments in these fields.","2018-06-01","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:04","599-632","","3","55","","Knowl Inf Syst","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\B5V36LDU\Beykikhoshk et al. - 2018 - Discovering topic structures of a temporally evolv.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYM6V6Y7","journalArticle","2018","Venianaki, M.; Salvetti, O.; de Bree, E.; Maris, T.; Karantanas, A.; Kontopodis, E.; Nikiforaki, K.; Marias, K.","Pattern recognition and pharmacokinetic methods on DCE-MRI data for tumor hypoxia mapping in sarcoma","Multimedia Tools and Applications","","1573-7721","10.1007/s11042-017-5046-6","https://doi.org/10.1007/s11042-017-5046-6","The main purpose of this study is to analyze the intrinsic tumor physiologic characteristics in patients with sarcoma through model-free analysis of dynamic contrast enhanced MR imaging data (DCE-MRI). Clinical data were collected from three patients with two different types of histologically proven sarcomas who underwent conventional and advanced MRI examination prior to excision. An advanced matrix factorization algorithm has been applied to the data, resulting in the identification of the principal time-signal uptake curves of DCE-MRI data, which were used to characterize the physiology of the tumor area, described by three different perfusion patterns i.e. hypoxic, well-perfused and necrotic one. The performance of the algorithm was tested by applying different initialization approaches with subsequent comparison of their results. The algorithm was proven to be robust and led to the consistent segmentation of the tumor area in three regions of different perfusion, i.e. well-perfused, hypoxic and necrotic. Results from the model-free approach were compared with a widely used pharmacokinetic (PK) model revealing significant correlations.","2018-04-01","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:04","9417-9439","","8","77","","Multimed Tools Appl","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\Z8SHQ9UY\Venianaki et al. - 2018 - Pattern recognition and pharmacokinetic methods on.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMM9JQ7D","journalArticle","2018","Egedorf, Søren; Shaker, Hamid Reza; Martin, Rodney A.; Jørgensen, Bo Nørregaard","Adverse condition and critical event prediction in commercial buildings: Danish case study","Energy Informatics","","2520-8942","10.1186/s42162-018-0015-5","https://doi.org/10.1186/s42162-018-0015-5","Over the last two decades, there has been a growing realization that the actual energy performances of many buildings fail to meet the original intent of building design. Faults in systems and equipment, incorrectly configured control systems and inappropriate operating procedures increase the energy consumption about 20% and therefore compromise the building energy performance. To improve the energy performance of buildings and to prevent occupant discomfort, adverse condition and critical event prediction plays an important role. The Adverse Condition and Critical Event Prediction Toolbox (ACCEPT) is a generic framework to compare and contrast methods that enable prediction of an adverse event, with low false alarm and missed detection rates. In this paper, ACCEPT is used for fault detection and prediction in a real building at the University of Southern Denmark. To make fault detection and prediction possible, machine learning methods such as Kernel Density Estimation (KDE), and Principal Component Analysis (PCA) are used. A new PCA–based method is developed for artificial fault generation. While the proposed method finds applications in different areas, it has been used primarily for analysis purposes in this work. The results are evaluated, discussed and compared with results from Canonical Variate Analysis (CVA) with KDE. The results show that ACCEPT is more powerful than CVA with KDE which is known to be one of the best multivariate data-driven techniques in particular, under dynamically changing operational conditions.","2018-08-14","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:04","10","","1","1","","Energy Inform","Adverse condition and critical event prediction in commercial buildings","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\QN9YTA22\Egedorf et al. - 2018 - Adverse condition and critical event prediction in.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KY8KJIMK","journalArticle","2017","Wiese, Eliane S.; Koedinger, Kenneth R.","Designing Grounded Feedback: Criteria for Using Linked Representations to Support Learning of Abstract Symbols","International Journal of Artificial Intelligence in Education","","1560-4306","10.1007/s40593-016-0133-9","https://doi.org/10.1007/s40593-016-0133-9","This paper proposes grounded feedback as a way to provide implicit verification when students are working with a novel representation. In grounded feedback, students’ responses are in the target, to-be-learned representation, and those responses are reflected in a more-accessible linked representation that is intrinsic to the domain. By examining the accessible feedback representation, students can infer if their work with the novel representation is correct. This paper presents the criteria for grounded feedback, provides examples of systems that implement grounded feedback, contrasts grounded feedback with similar feedback types, and discusses the evidence for grounded feedback’s effectiveness. Controlled experiments with random assignment that compare grounded feedback to other approaches are limited in number and scope (i.e., comparisons to explicit verification with and without text hints, linked representations, and no feedback). The two experiments we found with full implementation of grounded feedback and a sample size larger than 20 found robust learning benefits of grounded feedback over explicit verification feedback. These results are promising and indicate that grounded feedback warrants further investigation.","2017-09-01","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:04","448-474","","3","27","","Int J Artif Intell Educ","Designing Grounded Feedback","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\GSTRLZDJ\Wiese and Koedinger - 2017 - Designing Grounded Feedback Criteria for Using Li.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3BF59Z6J","journalArticle","2018","Amador Diaz Lopez, Julio; Molina-Solana, Miguel; Kennedy, Mark T.","foo.castr: visualising the future AI workforce","Big Data Analytics","","2058-6345","10.1186/s41044-018-0034-z","https://doi.org/10.1186/s41044-018-0034-z","Organization of companies and their HR departments are becoming hugely affected by recent advancements in computational power and Artificial Intelligence, with this trend likely to dramatically rise in the next few years. This work presents foo.castr, a tool we are developing to visualise, communicate and facilitate the understanding of the impact of these advancements in the future of workforce. It builds upon the idea that particular tasks within job descriptions will be progressively taken by computers, forcing the shaping of human jobs. In its current version, foo.castr presents three different scenarios to help HR departments planning potential changes and disruptions brought by the adoption of Artificial Intelligence.","2018-11-01","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:04","9","","1","3","","Big Data Anal","foo.castr","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\DHQ23FTE\Amador Diaz Lopez et al. - 2018 - foo.castr visualising the future AI workforce.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LZF36F36","journalArticle","2017","Rahimi, Zahra; Litman, Diane; Correnti, Richard; Wang, Elaine; Matsumura, Lindsay Clare","Assessing Students’ Use of Evidence and Organization in Response-to-Text Writing: Using Natural Language Processing for Rubric-Based Automated Scoring","International Journal of Artificial Intelligence in Education","","1560-4306","10.1007/s40593-017-0143-2","https://doi.org/10.1007/s40593-017-0143-2","This paper presents an investigation of score prediction based on natural language processing for two targeted constructs within analytic text-based writing: 1) students’ effective use of evidence and, 2) their organization of ideas and evidence in support of their claim. With the long-term goal of producing feedback for students and teachers, we designed a task-dependent model, for each dimension, that aligns with the scoring rubric and makes use of the source material. We believe the model will be meaningful and easy to interpret given the writing task. We used two datasets of essays written by students in grades 5–6 and 6–8. Our experimental results show that our task-dependent model (consistent with the rubric) performs as well as if not outperforms competitive baselines. We also show the potential generalizability of the rubric-based model by performing cross-corpus experiments. Finally, we show that the predictive utility of different feature groups in our rubric-based modeling approach is related to how much each feature group covers a rubric’s criteria.","2017-12-01","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:04","694-728","","4","27","","Int J Artif Intell Educ","Assessing Students’ Use of Evidence and Organization in Response-to-Text Writing","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\CDN4EPPD\Rahimi et al. - 2017 - Assessing Students’ Use of Evidence and Organizati.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5RSS9DB9","journalArticle","2017","Li, Huang; Fang, Shiaofen; Contreras, Joey A.; West, John D.; Risacher, Shannon L.; Wang, Yang; Sporns, Olaf; Saykin, Andrew J.; Goñi, Joaquín; Shen, Li; for the Alzheimer’s Disease Neuroimaging Initiative","Brain explorer for connectomic analysis","Brain Informatics","","2198-4026","10.1007/s40708-017-0071-9","https://doi.org/10.1007/s40708-017-0071-9","Visualization plays a vital role in the analysis of multimodal neuroimaging data. A major challenge in neuroimaging visualization is how to integrate structural, functional, and connectivity data to form a comprehensive visual context for data exploration, quality control, and hypothesis discovery. We develop a new integrated visualization solution for brain imaging data by combining scientific and information visualization techniques within the context of the same anatomical structure. In this paper, new surface texture techniques are developed to map non-spatial attributes onto both 3D brain surfaces and a planar volume map which is generated by the proposed volume rendering technique, spherical volume rendering. Two types of non-spatial information are represented: (1) time series data from resting-state functional MRI measuring brain activation; (2) network properties derived from structural connectivity data for different groups of subjects, which may help guide the detection of differentiation features. Through visual exploration, this integrated solution can help identify brain regions with highly correlated functional activations as well as their activation patterns. Visual detection of differentiation features can also potentially discover image-based phenotypic biomarkers for brain diseases.","2017-12-01","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:05","253-269","","4","4","","Brain Inf.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\C45Z2IQZ\Li et al. - 2017 - Brain explorer for connectomic analysis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KBH4UAY6","journalArticle","2018","Marchant, Roman; Haan, Sebastian; Clancey, Garner; Cripps, Sally","Applying machine learning to criminology: semi-parametric spatial-demographic Bayesian regression","Security Informatics","","2190-8532","10.1186/s13388-018-0030-x","https://doi.org/10.1186/s13388-018-0030-x","This paper describes the use of machine learning techniques to implement a Bayesian approach to modelling the dependency between offence data and environmental factors such as demographic characteristics and spatial location. The main goal of this paper is to provide a fully probabilistic approach to modelling crime which reflects all uncertainties in the prediction of offences as well as the uncertainties surrounding model parameters.","2018-06-19","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:05","1","","1","7","","Secur Inform","Applying machine learning to criminology","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\A93DWWF4\Marchant et al. - 2018 - Applying machine learning to criminology semi-par.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XNE3Z3MN","journalArticle","2017","Master, Neal; Zhou, Zhengyuan; Miller, Daniel; Scheinker, David; Bambos, Nicholas; Glynn, Peter","Improving predictions of pediatric surgical durations with supervised learning","International Journal of Data Science and Analytics","","2364-4168","10.1007/s41060-017-0055-0","https://doi.org/10.1007/s41060-017-0055-0","Effective management of operating room resources relies on accurate predictions of surgical case durations. This prediction problem is known to be particularly difficult in pediatric hospitals due to the extreme variation in pediatric patient populations. We pursue two supervised learning approaches: (1) We directly predict the surgical case durations using features derived from electronic medical records and from hospital operational information. For this regression problem, we propose a novel metric for measuring accuracy of predictions which captures key issues relevant to hospital operations. We evaluate several prediction models; some are automated (they do not require input from surgeons) while others are semi-automated (they do require input from surgeons). We see that many of our automated methods generally outperform currently used algorithms and our semi-automated methods can outperform surgeons by a substantial margin. (2) We consider a classification problem in which each prediction provided by a surgeon is predicted to be correct, an overestimate, or an underestimate. This classification mechanism builds on the metric mentioned above and could potentially be useful for detecting human errors. Both supervised learning approaches give insights into the feature engineering process while creating the basis for decision support tools.","2017-08-01","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:05","35-52","","1","4","","Int J Data Sci Anal","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\Q76KE59M\Master et al. - 2017 - Improving predictions of pediatric surgical durati.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3MTZGMJJ","journalArticle","2018","Yan, Chunli; Lindgren, Helena; Nieves, Juan Carlos","A dialogue-based approach for dealing with uncertain and conflicting information in medical diagnosis","Autonomous Agents and Multi-Agent Systems","","1573-7454","10.1007/s10458-018-9396-x","https://doi.org/10.1007/s10458-018-9396-x","In this paper, we propose a multi-agent framework to deal with situations involving uncertain or inconsistent information located in a distributed environment which cannot be combined into a single knowledge base. To this end, we introduce an inquiry dialogue approach based on a combination of possibilistic logic and a formal argumentation-based theory, where possibilistic logic is used to capture uncertain information, and the argumentation-based approach is used to deal with inconsistent knowledge in a distributed environment. We also modify the framework of earlier work, so that the system is not only easier to implement but also more suitable for educational purposes. The suggested approach is implemented in a clinical decision-support system in the domain of dementia diagnosis. The approach allows the physician to suggest a hypothetical diagnosis in a patient case, which is verified through the dialogue if sufficient patient information is present. If not, the user is informed about the missing information and potential inconsistencies in the information as a way to provide support for continuing medical education. The approach is presented, discussed, and applied to one scenario. The results contribute to the theory and application of inquiry dialogues in situations where the data are uncertain and inconsistent.","2018-11-01","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:05","861-885","","6","32","","Auton Agent Multi-Agent Syst","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\BI8MA26G\Yan et al. - 2018 - A dialogue-based approach for dealing with uncerta.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3R8MTU5A","journalArticle","2017","Sriwanna, Kittakorn; Boongoen, Tossapon; Iam-On, Natthakan","Graph clustering-based discretization of splitting and merging methods (GraphS and GraphM)","Human-centric Computing and Information Sciences","","2192-1962","10.1186/s13673-017-0103-8","https://doi.org/10.1186/s13673-017-0103-8","Discretization plays a major role as a data preprocessing technique used in machine learning and data mining. Recent studies have focused on multivariate discretization that considers relations among attributes. The general goal of this method is to obtain the discrete data, which preserves most of the semantics exhibited by original continuous data. However, many techniques generate the final discrete data that may be less useful with natural groups of data not being maintained. This paper presents a novel graph clustering-based discretization algorithm that encodes different similarity measures into a graph representation of the examined data. The intuition allows more refined data-wise relations to be obtained and used with the effective graph clustering technique based on normalized association to discover nature graphs accurately. The goodness of this approach is empirically demonstrated over 30 standard datasets and 20 imbalanced datasets, compared with 11 well-known discretization algorithms using 4 classifiers. The results suggest the new approach is able to preserve the natural groups and usually achieve the efficiency in terms of classifier performance, and the desired number of intervals than the comparative methods.","2017-08-03","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:05","21","","1","7","","Hum. Cent. Comput. Inf. Sci.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\Q7GC5S8G\Sriwanna et al. - 2017 - Graph clustering-based discretization of splitting.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JI48TIKW","journalArticle","2018","Ślęzak, Dominik; Glick, Rick; Betliński, Paweł; Synak, Piotr","A new approximate query engine based on intelligent capture and fast transformations of granulated data summaries","Journal of Intelligent Information Systems","","1573-7675","10.1007/s10844-017-0471-6","https://doi.org/10.1007/s10844-017-0471-6","We outline the processes of intelligent creation and utilization of granulated data summaries in the engine aimed at fast approximate execution of analytical SQL statements. We discuss how to use the introduced engine for the purposes of ad-hoc data exploration over large and quickly increasing data collected in a heterogeneous or distributed fashion. We focus on mechanisms that transform input data summaries into result sets representing query outcomes. We also illustrate how our computational principles can be put together with other paradigms of scaling and harnessing data analytics.","2018-04-01","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:05","385-414","","2","50","","J Intell Inf Syst","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\IM2JMY43\Ślęzak et al. - 2018 - A new approximate query engine based on intelligen.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8CR6Z9UG","journalArticle","2018","Muñoz, Mario A.; Villanova, Laura; Baatar, Davaatseren; Smith-Miles, Kate","Instance spaces for machine learning classification","Machine Learning","","1573-0565","10.1007/s10994-017-5629-5","https://doi.org/10.1007/s10994-017-5629-5","This paper tackles the issue of objective performance evaluation of machine learning classifiers, and the impact of the choice of test instances. Given that statistical properties or features of a dataset affect the difficulty of an instance for particular classification algorithms, we examine the diversity and quality of the UCI repository of test instances used by most machine learning researchers. We show how an instance space can be visualized, with each classification dataset represented as a point in the space. The instance space is constructed to reveal pockets of hard and easy instances, and enables the strengths and weaknesses of individual classifiers to be identified. Finally, we propose a methodology to generate new test instances with the aim of enriching the diversity of the instance space, enabling potentially greater insights than can be afforded by the current UCI repository.","2018-01-01","2021-06-07 21:11:05","2021-06-07 21:11:05","2021-06-07 21:11:05","109-147","","1","107","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\AZATSZ9L\Muñoz et al. - 2018 - Instance spaces for machine learning classificatio.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IGDXISVF","journalArticle","2016","Gill, Karamjit S.","A A EDITORIAL Artificial super intelligence: beyond rhetoric","AI & SOCIETY","","1435-5655","10.1007/s00146-016-0651-x","https://doi.org/10.1007/s00146-016-0651-x","","2016-05-01","2021-06-07 21:11:38","2021-06-15 09:25:57","2021-06-07 21:11:35","137-143","","2","31","","AI & Soc","Artificial super intelligence","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\AZ53X3NL\Gill - 2016 - Artificial super intelligence beyond rhetoric.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ULVYL9H4","journalArticle","2016","Luo, Gang","Automatically explaining machine learning prediction results: a demonstration on type 2 diabetes risk prediction","Health Information Science and Systems","","2047-2501","10.1186/s13755-016-0015-4","https://doi.org/10.1186/s13755-016-0015-4","Predictive modeling is a key component of solutions to many healthcare problems. Among all predictive modeling approaches, machine learning methods often achieve the highest prediction accuracy, but suffer from a long-standing open problem precluding their widespread use in healthcare. Most machine learning models give no explanation for their prediction results, whereas interpretability is essential for a predictive model to be adopted in typical healthcare settings.","2016-03-08","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:35","2","","1","4","","Health Inf Sci Syst","Automatically explaining machine learning prediction results","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\4UY29PHD\Luo - 2016 - Automatically explaining machine learning predicti.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRL7I4XC","journalArticle","2016","Dyagilev, Kirill; Saria, Suchi","Learning (predictive) risk scores in the presence of censoring due to interventions","Machine Learning","","1573-0565","10.1007/s10994-015-5527-7","https://doi.org/10.1007/s10994-015-5527-7","A large and diverse set of measurements are regularly collected during a patient’s hospital stay to monitor their health status. Tools for integrating these measurements into severity scores, that accurately track changes in illness severity, can improve clinicians’ ability to provide timely interventions. Existing approaches for creating such scores either (1) rely on experts to fully specify the severity score, (2) infer a score using detailed models of disease progression, or (3) train a predictive score, using supervised learning, by regressing against a surrogate marker of severity such as the presence of downstream adverse events. The first approach does not extend to diseases where an accurate score cannot be elicited from experts. The second assumes that the progression of disease can be accurately modeled, limiting its application to populations with simple, well-understood disease dynamics. The third approach, also most commonly used, often produces scores that suffer from bias due to treatment-related censoring (Paxton et al. in AMIA annual symposium proceedings, American Medical Informatics Association, p 1109, 2013). Specifically, since the downstream outcomes used for their training are observed only noisily and are influenced by treatment administration patterns, these scores do not generalize well when treatment administration patterns change. We propose a novel ranking based framework for disease severity score learning (DSSL). DSSL exploits the following key observation: while it is challenging for experts to quantify the disease severity at any given time, it is often easy to compare the disease severity at two different times. Extending existing ranking algorithms, DSSL learns a function that maps a vector of patient’s measurements to a scalar severity score subject to two constraints. First, the resulting score should be consistent with the expert’s ranking of the disease severity state. Second, changes in score between consecutive periods should be smooth. We apply DSSL to the problem of learning a sepsis severity score using a large, real-world electronic health record dataset. The learned scores significantly outperform state-of-the-art clinical scores in ranking patient states by severity and in early detection of downstream adverse events. We also show that the learned disease severity trajectories are consistent with clinical expectations of disease evolution. Further, we simulate datasets containing different treatment administration patterns and show that DSSL shows better generalization performance to changes in treatment patterns compared to the above approaches.","2016-03-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:35","323-348","","3","102","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\TY9LHKK8\Dyagilev and Saria - 2016 - Learning (predictive) risk scores in the presence .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S4RHJ8V4","journalArticle","2016","Rummel, Nikol; Walker, Erin; Aleven, Vincent","Different Futures of Adaptive Collaborative Learning Support","International Journal of Artificial Intelligence in Education","","1560-4306","10.1007/s40593-016-0102-3","https://doi.org/10.1007/s40593-016-0102-3","In this position paper we contrast a Dystopian view of the future of adaptive collaborative learning support (ACLS) with a Utopian scenario that – due to better-designed technology, grounded in research – avoids the pitfalls of the Dystopian version and paints a positive picture of the practice of computer-supported collaborative learning 25 years from now. We discuss research that we see as important in working towards a Utopian future in the next 25 years. In particular, we see a need to work towards a comprehensive instructional framework building on educational theory. This framework will allow us to provide nuanced and flexible (i.e. intelligent) ACLS to collaborative learners – the type of support we sketch in our Utopian scenario.","2016-06-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","784-795","","2","26","","Int J Artif Intell Educ","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\D4VGF87F\Rummel et al. - 2016 - Different Futures of Adaptive Collaborative Learni.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7YR9GFEJ","journalArticle","2017","Costa, Pedro; Campilho, Aurélio","Convolutional bag of words for diabetic retinopathy detection from eye fundus images","IPSJ Transactions on Computer Vision and Applications","","1882-6695","10.1186/s41074-017-0023-6","https://doi.org/10.1186/s41074-017-0023-6","This paper describes a methodology for diabetic retinopathy detection from eye fundus images using a generalization of the bag-of-visual-words (BoVW) method. We formulate the BoVW as two neural networks that can be trained jointly. Unlike the BoVW, our model is able to learn how to perform feature extraction, feature encoding, and classification guided by the classification error. The model achieves 0.97 area under the curve (AUC) on the DR2 dataset while the standard BoVW approach achieves 0.94 AUC. Also, it performs at the same level of the state-of-the-art on the Messidor dataset with 0.90 AUC.","2017-03-24","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","10","","1","9","","IPSJ T Comput Vis Appl","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\JVNMH2NY\Costa and Campilho - 2017 - Convolutional bag of words for diabetic retinopath.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y9L7XG7C","journalArticle","2016","Krawczyk, Bartosz","Learning from imbalanced data: open challenges and future directions","Progress in Artificial Intelligence","","2192-6360","10.1007/s13748-016-0094-0","https://doi.org/10.1007/s13748-016-0094-0","Despite more than two decades of continuous development learning from imbalanced data is still a focus of intense research. Starting as a problem of skewed distributions of binary tasks, this topic evolved way beyond this conception. With the expansion of machine learning and data mining, combined with the arrival of big data era, we have gained a deeper insight into the nature of imbalanced learning, while at the same time facing new emerging challenges. Data-level and algorithm-level methods are constantly being improved and hybrid approaches gain increasing popularity. Recent trends focus on analyzing not only the disproportion between classes, but also other difficulties embedded in the nature of data. New real-life problems motivate researchers to focus on computationally efficient, adaptive and real-time methods. This paper aims at discussing open issues and challenges that need to be addressed to further develop the field of imbalanced learning. Seven vital areas of research in this topic are identified, covering the full spectrum of learning from imbalanced data: classification, regression, clustering, data streams, big data analytics and applications, e.g., in social media and computer vision. This paper provides a discussion and suggestions concerning lines of future research for each of them.","2016-11-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","221-232","","4","5","","Prog Artif Intell","Learning from imbalanced data","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\IQYHUV4C\Krawczyk - 2016 - Learning from imbalanced data open challenges and.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2N3S57RQ","journalArticle","2016","Xiao, Cao; Bledsoe, Jesse; Wang, Shouyi; Chaovalitwongse, Wanpracha Art; Mehta, Sonya; Semrud-Clikeman, Margaret; Grabowski, Thomas","An integrated feature ranking and selection framework for ADHD characterization","Brain Informatics","","2198-4026","10.1007/s40708-016-0047-1","https://doi.org/10.1007/s40708-016-0047-1","Today, diagnosis of attention deficit hyperactivity disorder (ADHD) still primarily relies on a series of subjective evaluations that highly rely on a doctor’s experiences and intuitions from diagnostic interviews and observed behavior measures. An accurate and objective diagnosis of ADHD is still a challenge and leaves much to be desired. Many children and adults are inappropriately labeled with ADHD conditions, whereas many are left undiagnosed and untreated. Recent advances in neuroimaging studies have enabled us to search for both structural (e.g., cortical thickness, brain volume) and functional (functional connectivity) abnormalities that can potentially be used as new biomarkers of ADHD. However, structural and functional characteristics of neuroimaging data, especially magnetic resonance imaging (MRI), usually generate a large number of features. With a limited sample size, traditional machine learning techniques can be problematic to discover the true characteristic features of ADHD due to the significant issues of overfitting, computational burden, and interpretability of the model. There is an urgent need of efficient approaches to identify meaningful discriminative variables from a higher dimensional feature space when sample size is small compared with the number of features. To tackle this problem, this paper proposes a novel integrated feature ranking and selection framework that utilizes normalized brain cortical thickness features extracted from MRI data to discriminate ADHD subjects against healthy controls. The proposed framework combines information theoretic criteria and the least absolute shrinkage and selection operator (Lasso) method into a two-step feature selection process which is capable of selecting a sparse model while preserving the most informative features. The experimental results showed that the proposed framework generated the highest/comparable ADHD prediction accuracy compared with the state-of-the-art feature selection approaches with minimum number of features in the final model. The selected regions of interest in our model were consistent with recent brain–behavior studies of ADHD development, and thus confirmed the validity of the selected features by the proposed approach.","2016-09-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","145-155","","3","3","","Brain Inf.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\4MXAUAIT\Xiao et al. - 2016 - An integrated feature ranking and selection framew.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3AY2DXG6","journalArticle","2016","Gamberger, Dragan; Ženko, Bernard; Mitelpunkt, Alexis; Shachar, Netta; Lavrač, Nada","Clusters of male and female Alzheimer’s disease patients in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database","Brain Informatics","","2198-4026","10.1007/s40708-016-0035-5","https://doi.org/10.1007/s40708-016-0035-5","This paper presents homogeneous clusters of patients, identified in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) data population of 317 females and 342 males, described by a total of 243 biological and clinical descriptors. Clustering was performed with a novel methodology, which supports identification of patient subpopulations that are homogeneous regarding both clinical and biological descriptors. Properties of the constructed clusters clearly demonstrate the differences between female and male Alzheimer’s disease patient groups. The major difference is the existence of two male subpopulations with unexpected values of intracerebral and whole brain volumes.","2016-09-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","169-179","","3","3","","Brain Inf.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\L57TLI74\Gamberger et al. - 2016 - Clusters of male and female Alzheimer’s disease pa.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8R7ZLBNX","journalArticle","2017","Baechle, Christopher; Agarwal, Ankur; Zhu, Xingquan","Big data driven co-occurring evidence discovery in chronic obstructive pulmonary disease patients","Journal of Big Data","","2196-1115","10.1186/s40537-017-0067-6","https://doi.org/10.1186/s40537-017-0067-6","Chronic Obstructive Pulmonary Disease (COPD) is a chronic lung disease that affects airflow to the lungs. Discovering the co-occurrence of COPD with other diseases, symptoms, and medications is invaluable to medical staff. Building co-occurrence indexes and finding causal relationships with COPD can be difficult because often times disease prevalence within a population influences results. A method which can better separate occurrence within COPD patients from population prevalence would be desirable. Large hospital systems may potentially have tens of millions of patient records spanning decades of collection and a big data approach that is scalable is desirable. The presented method, Co-Occurring Evidence Discovery (COED), presents a methodology and framework to address these issues.","2017-04-04","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","9","","1","4","","J Big Data","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\68F7N7JR\Baechle et al. - 2017 - Big data driven co-occurring evidence discovery in.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XT4MBWIJ","journalArticle","2016","van Altena, Allard J.; Moerland, Perry D.; Zwinderman, Aeilko H.; Olabarriaga, Sílvia D.","Understanding big data themes from scientific biomedical literature through topic modeling","Journal of Big Data","","2196-1115","10.1186/s40537-016-0057-0","https://doi.org/10.1186/s40537-016-0057-0","Nowadays, big data is a key component in (bio)medical research. However, the meaning of the term is subject to a wide array of opinions, without a formal definition. This hampers communication and leads to missed opportunities. For example, in the (bio)medical field we have observed many different interpretations, some of which have a negative connotation, impeding exploitation of big data approaches. In this paper we pursue a better understanding of the term big data through a data-driven systematic approach using text analysis of scientific (bio)medical literature. We attempt to find how existing big data definitions are expressed within the chosen application domain. We build upon findings of previous qualitative research by De Mauro et al. (Lib Rev 65: 122–135, 14), which analysed fifteen definitions and identified four key big data themes (i.e., information, methods, technology, and impact). We have revisited these and other definitions of big data, and consolidated them into eight additional themes, resulting in a total of twelve themes. The corpus was composed of paper abstracts extracted from (bio)medical literature databases, searching for ‘big data’. After text pre-processing and parameter selection, topic modelling was applied with 25 topics. The resulting top-20 words per topic were annotated with the twelve big data themes by seven observers. The analysis of these annotations show that the themes proposed by De Mauro et al. are strongly expressed in the corpus. Furthermore, several of the most popular big data V’s (i.e., volume, velocity, and value) also have a relatively high presence. Other V’s introduced more recently (e.g. variability) were however hardly found in the 25 topics. These findings show that the current understanding of big data within the (bio)medical domain is in agreement with more general definitions of the term.","2016-11-15","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","23","","1","3","","J Big Data","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\2LQGL2GW\van Altena et al. - 2016 - Understanding big data themes from scientific biom.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62LGMGBN","journalArticle","2016","Guo, Xuan; Yu, Qi; Li, Rui; Alm, Cecilia Ovesdotter; Calvelli, Cara; Shi, Pengcheng; Haake, Anne","Intelligent medical image grouping through interactive learning","International Journal of Data Science and Analytics","","2364-4168","10.1007/s41060-016-0021-2","https://doi.org/10.1007/s41060-016-0021-2","Image grouping in knowledge-rich domains is challenging, since domain knowledge and human expertise are key to transform image pixels into meaningful content. Manually marking and annotating images is not only labor-intensive but also ineffective. Furthermore, most traditional machine learning approaches cannot bridge this gap for the absence of experts’ input. We thus present an interactive machine learning paradigm that allows experts to become an integral part of the learning process. This paradigm is designed for automatically computing and quantifying interpretable grouping of dermatological images. In this way, the computational evolution of an image grouping model, its visualization, and expert interactions form a loop to improve image grouping. In our paradigm, dermatologists encode their domain knowledge about the medical images by grouping a small subset of images via a carefully designed interface. Our learning algorithm automatically incorporates these manually specified connections as constraints for reorganizing the whole image dataset. Performance evaluation shows that this paradigm effectively improves image grouping based on expert knowledge.","2016-12-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","95-105","","3","2","","Int J Data Sci Anal","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\U3LGVIUC\Guo et al. - 2016 - Intelligent medical image grouping through interac.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AP9ITC52","journalArticle","2017","Zheng, Nan-ning; Liu, Zi-yi; Ren, Peng-ju; Ma, Yong-qiang; Chen, Shi-tao; Yu, Si-yu; Xue, Jian-ru; Chen, Ba-dong; Wang, Fei-yue","Hybrid-augmented intelligence: collaboration and cognition","Frontiers of Information Technology & Electronic Engineering","","2095-9230","10.1631/FITEE.1700053","https://doi.org/10.1631/FITEE.1700053","The long-term goal of artificial intelligence (AI) is to make machines learn and think like human beings. Due to the high levels of uncertainty and vulnerability in human life and the open-ended nature of problems that humans are facing, no matter how intelligent machines are, they are unable to completely replace humans. Therefore, it is necessary to introduce human cognitive capabilities or human-like cognitive models into AI systems to develop a new form of AI, that is, hybrid-augmented intelligence. This form of AI or machine intelligence is a feasible and important developing model. Hybrid-augmented intelligence can be divided into two basic models: one is human-in-the-loop augmented intelligence with human-computer collaboration, and the other is cognitive computing based augmented intelligence, in which a cognitive model is embedded in the machine learning system. This survey describes a basic framework for human-computer collaborative hybrid-augmented intelligence, and the basic elements of hybrid-augmented intelligence based on cognitive computing. These elements include intuitive reasoning, causal models, evolution of memory and knowledge, especially the role and basic principles of intuitive reasoning for complex problem solving, and the cognitive learning framework for visual scene understanding based on memory and reasoning. Several typical applications of hybrid-augmented intelligence in related fields are given.","2017-02-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","153-179","","2","18","","Frontiers Inf Technol Electronic Eng","Hybrid-augmented intelligence","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\6I8SWP8F\Zheng et al. - 2017 - Hybrid-augmented intelligence collaboration and c.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RI2DSRYW","journalArticle","2016","Fan, Miaolin; Chou, Chun-An","Exploring stability-based voxel selection methods in MVPA using cognitive neuroimaging data: a comprehensive study","Brain Informatics","","2198-4026","10.1007/s40708-016-0048-0","https://doi.org/10.1007/s40708-016-0048-0","Feature selection plays a key role in multi-voxel pattern analysis because functional magnetic resonance imaging data are typically noisy, sparse, and high-dimensional. Although the conventional evaluation criterion is the classification accuracy, selecting a stable feature set that is not sensitive to the variance in dataset may provide more scientific insights. In this study, we aim to investigate the stability of feature selection methods and test the stability-based feature selection scheme on two benchmark datasets. Top-k feature selection with a ranking score of mutual information and correlation, recursive feature elimination integrated with support vector machine, and L1 and L2-norm regularizations were adapted to a bootstrapped stability selection framework, and the selected algorithms were compared based on both accuracy and stability scores. The results indicate that regularization-based methods are generally more stable in StarPlus dataset, but in Haxby dataset they failed to perform as well as others.","2016-09-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","193-203","","3","3","","Brain Inf.","Exploring stability-based voxel selection methods in MVPA using cognitive neuroimaging data","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\JIH7XPY3\Fan and Chou - 2016 - Exploring stability-based voxel selection methods .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5JASZDSF","journalArticle","2016","Agarwal, Ankur; Baechle, Christopher; Behara, Ravi S.; Rao, Vinaya","Multi-method approach to wellness predictive modeling","Journal of Big Data","","2196-1115","10.1186/s40537-016-0049-0","https://doi.org/10.1186/s40537-016-0049-0","Patient wellness and preventative care are increasingly becoming a concern for many patients, employers, and healthcare professionals. The federal government has increased spending for wellness alongside new legislation which gives employers and insurance providers some new tools for encouraging preventative care. Not all preventative care and wellness programs have a net positive savings however. Our research attempts to create a patient wellness score which integrates many lifestyle components and a holistic patient prospective. Using a large comprehensive survey conducted by the Centers for Disease Control and Prevention, models are built combining both medical professional input and machine learning algorithms. Models are compared and 8 out of 9 models are shown to have a statistically significant (p = 0.05) increase in area under the receiver operating characteristic when using the hybrid approach when compared to expert-only models. Models are then aggregated and linearly transformed for patient-friendly output. The resulting predictive models provide patients and healthcare providers a comprehensive numerical assessment of a patient’s health, which may be used to track patient wellness so at to help maintain or improve their current condition.","2016-08-24","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","15","","1","3","","J Big Data","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\UMEPX4R3\Agarwal et al. - 2016 - Multi-method approach to wellness predictive model.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P8QLW5KA","journalArticle","2017","Fratamico, Lauren; Conati, Cristina; Kardan, Samad; Roll, Ido","Applying a Framework for Student Modeling in Exploratory Learning Environments: Comparing Data Representation Granularity to Handle Environment Complexity","International Journal of Artificial Intelligence in Education","","1560-4306","10.1007/s40593-016-0131-y","https://doi.org/10.1007/s40593-016-0131-y","Interactive simulations can facilitate inquiry learning. However, similarly to other Exploratory Learning Environments, students may not always learn effectively in these unstructured environments. Thus, providing adaptive support has great potential to help improve student learning with these rich activities. Providing adaptive support requires a student model that can both evaluate learning as well inform relevant feedback. Building such a model for interactive simulations is especially challenging because the exploratory nature of the interaction makes it hard to know a priori which behaviors are conducive to learning. To address this problem, in this paper we leverage the student modeling framework proposed in (Kardan and Conati, 2011) to specifically address the challenge of modeling students in interactive simulations. The framework has already been successfully applied to build a student model and to give adaptive interventions for an interactive simulation for constraint satisfaction. We seek to investigate the generality of the framework by building student models for a more complex simulation on electric circuits called Circuit Construction Kit (CCK). We evaluate alternative representations of logged interaction data with CCK, capturing different amounts of granularity and feature engineering. We then apply the student modeling framework proposed in (Kardan and Conati, 2011) to group students based on their interaction behaviors, map these behaviors into learning outcomes and leverage the resulting clusters to classify new learners. Data collected from 100 college students working with the CCK simulation indicates that the proposed framework is able to successfully classify students in groups of high and low learners and identify patterns of productive behaviors that are common across representations that can inform real-time feedback. In addition to presenting these results, we discuss trade-offs between levels of granularity and feature engineering in the tested interaction representations in terms of their ability to evaluate learning, classify students, and inform feedback.","2017-06-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:36","320-352","","2","27","","Int J Artif Intell Educ","Applying a Framework for Student Modeling in Exploratory Learning Environments","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\KX7AQT7N\Fratamico et al. - 2017 - Applying a Framework for Student Modeling in Explo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X8VSURBS","journalArticle","2016","Khan, Suleiman A.; Leppäaho, Eemeli; Kaski, Samuel","Bayesian multi-tensor factorization","Machine Learning","","1573-0565","10.1007/s10994-016-5563-y","https://doi.org/10.1007/s10994-016-5563-y","We introduce Bayesian multi-tensor factorization, a model that is the first Bayesian formulation for joint factorization of multiple matrices and tensors. The research problem generalizes the joint matrix–tensor factorization problem to arbitrary sets of tensors of any depth, including matrices, can be interpreted as unsupervised multi-view learning from multiple data tensors, and can be generalized to relax the usual trilinear tensor factorization assumptions. The result is a factorization of the set of tensors into factors shared by any subsets of the tensors, and factors private to individual tensors. We demonstrate the performance against existing baselines in multiple tensor factorization tasks in structural toxicogenomics and functional neuroimaging.","2016-11-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:37","233-253","","2","105","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\AXUK4D9J\Khan et al. - 2016 - Bayesian multi-tensor factorization.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RIRPLXS4","journalArticle","2016","Souillard-Mandar, William; Davis, Randall; Rudin, Cynthia; Au, Rhoda; Libon, David J.; Swenson, Rodney; Price, Catherine C.; Lamar, Melissa; Penney, Dana L.","Learning classification models of cognitive conditions from subtle behaviors in the digital Clock Drawing Test","Machine Learning","","1573-0565","10.1007/s10994-015-5529-5","https://doi.org/10.1007/s10994-015-5529-5","The Clock Drawing Test—a simple pencil and paper test—has been used for more than 50 years as a screening tool to differentiate normal individuals from those with cognitive impairment, and has proven useful in helping to diagnose cognitive dysfunction associated with neurological disorders such as Alzheimer’s disease, Parkinson’s disease, and other dementias and conditions. We have been administering the test using a digitizing ballpoint pen that reports its position with considerable spatial and temporal precision, making available far more detailed data about the subject’s performance. Using pen stroke data from these drawings categorized by our software, we designed and computed a large collection of features, then explored the tradeoffs in performance and interpretability in classifiers built using a number of different subsets of these features and a variety of different machine learning techniques. We used traditional machine learning methods to build prediction models that achieve high accuracy. We operationalized widely used manual scoring systems so that we could use them as benchmarks for our models. We worked with clinicians to define guidelines for model interpretability, and constructed sparse linear models and rule lists designed to be as easy to use as scoring systems currently used by clinicians, but more accurate. While our models will require additional testing for validation, they offer the possibility of substantial improvement in detecting cognitive impairment earlier than currently possible, a development with considerable potential impact in practice.","2016-03-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:37","393-441","","3","102","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\7TVRKJYD\Souillard-Mandar et al. - 2016 - Learning classification models of cognitive condit.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WVBM9TVK","journalArticle","2016","Ustun, Berk; Rudin, Cynthia","Supersparse linear integer models for optimized medical scoring systems","Machine Learning","","1573-0565","10.1007/s10994-015-5528-6","https://doi.org/10.1007/s10994-015-5528-6","Scoring systems are linear classification models that only require users to add, subtract and multiply a few small numbers in order to make a prediction. These models are in widespread use by the medical community, but are difficult to learn from data because they need to be accurate and sparse, have coprime integer coefficients, and satisfy multiple operational constraints. We present a new method for creating data-driven scoring systems called a Supersparse Linear Integer Model (SLIM). SLIM scoring systems are built by using an integer programming problem that directly encodes measures of accuracy (the 0–1 loss) and sparsity (the $$\ell _0$$-seminorm) while restricting coefficients to coprime integers. SLIM can seamlessly incorporate a wide range of operational constraints related to accuracy and sparsity, and can produce acceptable models without parameter tuning because of the direct control provided over these quantities. We provide bounds on the testing and training accuracy of SLIM scoring systems, and present a new data reduction technique that can improve scalability by eliminating a portion of the training data beforehand. Our paper includes results from a collaboration with the Massachusetts General Hospital Sleep Laboratory, where SLIM is being used to create a highly tailored scoring system for sleep apnea screening.","2016-03-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:37","349-391","","3","102","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\UPPQ4X8F\Ustun and Rudin - 2016 - Supersparse linear integer models for optimized me.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S44ZJL6A","journalArticle","2016","Foss, Alex; Markatou, Marianthi; Ray, Bonnie; Heching, Aliza","A semiparametric method for clustering mixed data","Machine Learning","","1573-0565","10.1007/s10994-016-5575-7","https://doi.org/10.1007/s10994-016-5575-7","Despite the existence of a large number of clustering algorithms, clustering remains a challenging problem. As large datasets become increasingly common in a number of different domains, it is often the case that clustering algorithms must be applied to heterogeneous sets of variables, creating an acute need for robust and scalable clustering methods for mixed continuous and categorical scale data. We show that current clustering methods for mixed-type data are generally unable to equitably balance the contribution of continuous and categorical variables without strong parametric assumptions. We develop KAMILA (KAy-means for MIxed LArge data), a clustering method that addresses this fundamental problem directly. We study theoretical aspects of our method and demonstrate its effectiveness in a series of Monte Carlo simulation studies and a set of real-world applications.","2016-12-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:38","419-458","","3","105","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\QGAM6HI5\Foss et al. - 2016 - A semiparametric method for clustering mixed data.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WC92S6UP","journalArticle","2016","Loza Mencía, Eneldo; Janssen, Frederik","Learning rules for multi-label classification: a stacking and a separate-and-conquer approach","Machine Learning","","1573-0565","10.1007/s10994-016-5552-1","https://doi.org/10.1007/s10994-016-5552-1","Dependencies between the labels are commonly regarded as the crucial issue in multi-label classification. Rules provide a natural way for symbolically describing such relationships. For instance, rules with label tests in the body allow for representing directed dependencies like implications, subsumptions, or exclusions. Moreover, rules naturally allow to jointly capture both local and global label dependencies. In this paper, we introduce two approaches for learning such label-dependent rules. Our first solution is a bootstrapped stacking approach which can be built on top of a conventional rule learning algorithm. For this, we learn for each label a separate ruleset, but we include the remaining labels as additional attributes in the training instances. The second approach goes one step further by adapting the commonly used separate-and-conquer algorithm for learning multi-label rules. The main idea is to re-include the covered examples with the predicted labels so that this information can be used for learning subsequent rules. Both approaches allow for making label dependencies explicit in the rules. In addition, the usage of standard rule learning techniques targeted at producing accurate predictions ensures that the found rules are useful for actual classification. Our experiments show (a) that the discovered dependencies contribute to the understanding and improve the analysis of multi-label datasets, and (b) that the found multi-label rules are crucial for the predictive performance as our proposed approaches beat the baseline using conventional rules.","2016-10-01","2021-06-07 21:11:38","2021-06-07 21:11:38","2021-06-07 21:11:38","77-126","","1","105","","Mach Learn","Learning rules for multi-label classification","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\RBVC8C62\Loza Mencía and Janssen - 2016 - Learning rules for multi-label classification a s.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NKCFKKVN","journalArticle","2015","Vogt, Julia E.; Kloft, Marius; Stark, Stefan; Raman, Sudhir S.; Prabhakaran, Sandhya; Roth, Volker; Rätsch, Gunnar","Probabilistic clustering of time-evolving distance data","Machine Learning","","1573-0565","10.1007/s10994-015-5516-x","https://doi.org/10.1007/s10994-015-5516-x","We present a novel probabilistic clustering model for objects that are represented via pairwise distances and observed at different time points. The proposed method utilizes the information given by adjacent time points to find the underlying cluster structure and obtain a smooth cluster evolution. This approach allows the number of objects and clusters to differ at every time point, and no identification on the identities of the objects is needed. Further, the model does not require the number of clusters being specified in advance—they are instead determined automatically using a Dirichlet process prior. We validate our model on synthetic data showing that the proposed method is more accurate than state-of-the-art clustering methods. Finally, we use our dynamic clustering model to analyze and illustrate the evolution of brain cancer patients over time.","2015-09-01","2021-06-07 21:12:04","2021-06-07 21:12:04","2021-06-07 21:12:02","635-654","","2","100","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\YMFXWIU4\Vogt et al. - 2015 - Probabilistic clustering of time-evolving distance.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQ5HDBDQ","journalArticle","2016","Matusevich, David Sergio; Cabrera, Wellington; Ordonez, Carlos","Accelerating a Gibbs sampler for variable selection on genomics data with summarization and variable pre-selection combining an array DBMS and R","Machine Learning","","1573-0565","10.1007/s10994-015-5534-8","https://doi.org/10.1007/s10994-015-5534-8","Variable selection in high dimensional data is a challenging problem due to the exponential number of variable combinations, and Markov Chain Monte Carlo (MCMC) methods represent the state of the art to solve it. With genomics data this problem becomes even more difficult because there are generally more dimensions (variables) than points (records) leading to slow convergence and numerically unstable solutions. On the other hand, despite many alternative prototypes and languages, R remains a popular system to compute machine learning models. Unfortunately, R can be particularly slow with heavy matrix computations and the high number of iterations required by MCMC methods. Moreover, making R scale to large matrices, possibly beyond RAM, requires careful system integration. Recently, array DBMSs have opened the possibility of manipulating matrices of unlimited size. With such motivation in mind, we present algorithmic optimizations to accelerate the computation of variable selection in linear regression with the Gibbs sampler, a fundamental MCMC method. Such optimizations have the potential to accelerate other models. We study how to leverage the speed and scalability of the array DBMS to exploit our optimizations in R. We present a comprehensive experimental evaluation to assess time efficiency and model quality with a cancer data set containing RNA and miRNA variables to predict survival time. We show our optimized algorithm combining DBMS and R processing is significantly faster than R alone. We show our system allows fast joint analysis of RNA and miRNA variables, instead of analyzing them separately. Finally, we confirm our algorithm finds medically significant variables already identified in the biomedical literature. Our optimized MCMC method for the array DBMS can be easily called from R, leaving the final model within R runtime in RAM for further interpretation.","2016-03-01","2021-06-07 21:12:04","2021-06-07 21:12:04","2021-06-07 21:12:02","483-504","","3","102","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\CMK8QX8K\Matusevich et al. - 2016 - Accelerating a Gibbs sampler for variable selectio.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RH9WRG47","journalArticle","2016","Wiens, Jenna; Wallace, Byron C.","A A Editorial: special issue on machine learning for health and medicine","Machine Learning","","1573-0565","10.1007/s10994-015-5533-9","https://doi.org/10.1007/s10994-015-5533-9","","2016-03-01","2021-06-07 21:12:04","2021-06-15 09:26:18","2021-06-07 21:12:02","305-307","","3","102","","Mach Learn","Editorial","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\B2WBL987\Wiens and Wallace - 2016 - Editorial special issue on machine learning for h.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"REJXFFVS","journalArticle","2015","Cao, Bokai; Kong, Xiangnan; Yu, Philip S.","A review of heterogeneous data mining for brain disorder identification","Brain Informatics","","2198-4026","10.1007/s40708-015-0021-3","https://doi.org/10.1007/s40708-015-0021-3","With rapid advances in neuroimaging techniques, the research on brain disorder identification has become an emerging area in the data mining community. Brain disorder data poses many unique challenges for data mining research. For example, the raw data generated by neuroimaging experiments is in tensor representations, with typical characteristics of high dimensionality, structural complexity, and nonlinear separability. Furthermore, brain connectivity networks can be constructed from the tensor data, embedding subtle interactions between brain regions. Other clinical measures are usually available reflecting the disease status from different perspectives. It is expected that integrating complementary information in the tensor data and the brain network data, and incorporating other clinical parameters will be potentially transformative for investigating disease mechanisms and for informing therapeutic interventions. Many research efforts have been devoted to this area. They have achieved great success in various applications, such as tensor-based modeling, subgraph pattern mining, and multi-view feature analysis. In this paper, we review some recent data mining methods that are used for analyzing brain disorders.","2015-12-01","2021-06-07 21:12:04","2021-06-07 21:12:04","2021-06-07 21:12:03","253-264","","4","2","","Brain Inf.","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\QP8AG6VF\Cao et al. - 2015 - A review of heterogeneous data mining for brain di.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F889L8JK","journalArticle","2015","Tomczak, Jakub M.; Zięba, Maciej","Probabilistic combination of classification rules and its application to medical diagnosis","Machine Learning","","1573-0565","10.1007/s10994-015-5508-x","https://doi.org/10.1007/s10994-015-5508-x","Application of machine learning to medical diagnosis entails facing two major issues, namely, a necessity of learning comprehensible models and a need of coping with imbalanced data phenomenon. The first one corresponds to a problem of implementing interpretable models, e.g., classification rules or decision trees. The second issue represents a situation in which the number of examples from one class (e.g., healthy patients) is significantly higher than the number of examples from the other class (e.g., ill patients). Learning algorithms which are prone to the imbalance data return biased models towards the majority class. In this paper, we propose a probabilistic combination of soft rules, which can be seen as a probabilistic version of the classification rules, by introducing new latent random variable called conjunctive feature. The conjunctive features represent conjunctions of values of attribute variables (features) and we assume that for given conjunctive feature the object and its label (class) become independent random variables. In order to deal with the between class imbalance problem, we present a new estimator which incorporates the knowledge about data imbalanceness into hyperparameters of initial probability of objects with fixed class labels. Additionally, we propose a method for aggregating sufficient statistics needed to estimate probabilities in a graph-based structure to speed up computations. At the end, we carry out two experiments: (1) using benchmark datasets, (2) using medical datasets. The results are discussed and the conclusions are drawn.","2015-10-01","2021-06-07 21:12:04","2021-06-07 21:12:04","2021-06-07 21:12:03","105-135","","1","101","","Mach Learn","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\43KTAMDM\Tomczak and Zięba - 2015 - Probabilistic combination of classification rules .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GRHAG8N9","journalArticle","2015","Khot, Tushar; Natarajan, Sriraam; Kersting, Kristian; Shavlik, Jude","Gradient-based boosting for statistical relational learning: the Markov logic network and missing data cases","Machine Learning","","1573-0565","10.1007/s10994-015-5481-4","https://doi.org/10.1007/s10994-015-5481-4","Recent years have seen a surge of interest in Statistical Relational Learning (SRL) models that combine logic with probabilities. One prominent and highly expressive SRL model is Markov Logic Networks (MLNs), but the expressivity comes at the cost of learning complexity. Most of the current methods for learning MLN structure follow a two-step approach where first they search through the space of possible clauses (i.e. structures) and then learn weights via gradient descent for these clauses. We present a functional-gradient boosting algorithm to learn both the weights (in closed form) and the structure of the MLN simultaneously. Moreover most of the learning approaches for SRL apply the closed-world assumption, i.e., whatever is not observed is assumed to be false in the world. We attempt to open this assumption. We extend our algorithm for MLN structure learning to handle missing data by using an EM-based approach and show this algorithm can also be used to learn Relational Dependency Networks and relational policies. Our results in many domains demonstrate that our approach can effectively learn MLNs even in the presence of missing data.","2015-07-01","2021-06-07 21:12:04","2021-06-07 21:12:04","2021-06-07 21:12:03","75-100","","1","100","","Mach Learn","Gradient-based boosting for statistical relational learning","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\VYBAMKSD\Khot et al. - 2015 - Gradient-based boosting for statistical relational.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CTAX6YVE","journalArticle","2015","Burrows, Steven; Gurevych, Iryna; Stein, Benno","The Eras and Trends of Automatic Short Answer Grading","International Journal of Artificial Intelligence in Education","","1560-4306","10.1007/s40593-014-0026-8","https://doi.org/10.1007/s40593-014-0026-8","Automatic short answer grading (ASAG) is the task of assessing short natural language responses to objective questions using computational methods. The active research in this field has increased enormously of late with over 80 papers fitting a definition of ASAG. However, the past efforts have generally been ad-hoc and non-comparable until recently, hence the need for a unified view of the whole field. The goal of this paper is to address this aim with a comprehensive review of ASAG research and systems according to history and components. Our historical analysis identifies 35 ASAG systems within 5 temporal themes that mark advancement in methodology or evaluation. In contrast, our component analysis reviews 6 common dimensions from preprocessing to effectiveness. A key conclusion is that an era of evaluation is the newest trend in ASAG research, which is paving the way for the consolidation of the field.","2015-03-01","2021-06-07 21:12:04","2021-06-07 21:12:04","2021-06-07 21:12:03","60-117","","1","25","","Int J Artif Intell Educ","","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\ZJXDM2Z8\Burrows et al. - 2015 - The Eras and Trends of Automatic Short Answer Grad.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9EXKGRXN","journalArticle","2015","Luo, Gang","MLBCD: a machine learning tool for big clinical data","Health Information Science and Systems","","2047-2501","10.1186/s13755-015-0011-0","https://doi.org/10.1186/s13755-015-0011-0","Predictive modeling is fundamental for extracting value from large clinical data sets, or “big clinical data,” advancing clinical research, and improving healthcare. Machine learning is a powerful approach to predictive modeling. Two factors make machine learning challenging for healthcare researchers. First, before training a machine learning model, the values of one or more model parameters called hyper-parameters must typically be specified. Due to their inexperience with machine learning, it is hard for healthcare researchers to choose an appropriate algorithm and hyper-parameter values. Second, many clinical data are stored in a special format. These data must be iteratively transformed into the relational table format before conducting predictive modeling. This transformation is time-consuming and requires computing expertise.","2015-09-28","2021-06-07 21:12:04","2021-06-07 21:12:04","2021-06-07 21:12:04","3","","1","3","","Health Inf Sci Syst","MLBCD","","","","","","","en","","","","","Springer Link","","","","C:\Users\Asus\Zotero\storage\T2PVUQJ9\Luo - 2015 - MLBCD a machine learning tool for big clinical da.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""